<html>
<head>
<title>What happens when you train a neural net on video-game slang.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当你训练一个关于电子游戏俚语的神经网络时会发生什么。</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/training-a-word-prediction-model-on-videogame-forum-comments-d00dc16d06b0?source=collection_archive---------4-----------------------#2017-04-09">https://medium.com/hackernoon/training-a-word-prediction-model-on-videogame-forum-comments-d00dc16d06b0?source=collection_archive---------4-----------------------#2017-04-09</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/77f172bbe9110b1bab573ad5ed378a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgf74mE5_wLB9OaVbg8OVQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">‘Middle Lane’ — Dota2— Image <a class="ae jg" href="http://dota2.gamepedia.com/Mid_Lane_%28Loading_Screen%29" rel="noopener ugc nofollow" target="_blank">credit</a></figcaption></figure><p id="b420" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">一个被设计用来做简单单词预测的神经网络<a class="ae jg" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">在从在线论坛收集的帖子上接受训练时会有怎样的表现？如果我们选择一个特定主题的来源(例如:一个特定的视频游戏)，模型将如何处理特定主题的单词(例如:角色和拼写名称)和俚语？是来自非正式英语(俚语、拼写错误、混合语法等)的“噪音”。)重要到可以忽略使用这种类型的数据进行</a><a class="ae jg" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>练习？在这个练习中，我们为在线游戏“DoTA2”搜集讨论板，并看看结果与使用经典的精选数据集相比如何。</p><p id="8d75" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh kf kg kh ki b">This is a personal project I worked on to learn more about deep learning. If you just want to see the code, check it out on <a class="ae jg" href="https://github.com/daendinam/dota-nn" rel="noopener ugc nofollow" target="_blank">Github</a>. The slides from <a class="ae jg" href="https://www.coursera.org/course/neuralnets" rel="noopener ugc nofollow" target="_blank"><strong class="jj hv">this course</strong></a> were very helpful in learning the core concepts.</code></p><h1 id="68e1" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">模型</h1><p id="b833" class="pw-post-body-paragraph jh ji hu jj b jk lh jm jn jo li jq jr js lj ju jv jw lk jy jz ka ll kc kd ke hn dt translated">神经网络取自多伦多大学的一个神经网络课程作业，我当时是由Geoffrey Hinton教授的(课程的最新版本可以在这里找到<a class="ae jg" href="https://www.cs.toronto.edu/~rgrosse/csc321" rel="noopener ugc nofollow" target="_blank"><strong class="jj hv"/></a>)。</p><blockquote class="lm"><p id="bbb9" class="ln lo hu bd lp lq lr ls lt lu lv ke ek translated"><em class="lw">“它接收3个连续单词作为输入，其目的是预测下一个单词(目标单词)的分布。我们使用交叉熵准则来训练模型，这相当于最大化它分配给训练集中的目标的概率。希望它也能学会对以前从未见过的序列做出明智的预测。”— G .辛顿</em></p></blockquote><figure class="ly lz ma mb mc iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lx"><img src="../Images/b1bb8da75761ee1288df97dfc896491f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrIctUzkkEq1u3EJXwWAlQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="http://www.cs.toronto.edu/~guerzhoy/321/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="5766" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">收集数据</h1><p id="f2e6" class="pw-post-body-paragraph jh ji hu jj b jk lh jm jn jo li jq jr js lj ju jv jw lk jy jz ka ll kc kd ke hn dt translated"><em class="md">选择的数据来源是两个讨论热门多人游戏《DoTA2》的活跃论坛:<br/></em><a class="ae jg" href="https://www.reddit.com/r/dota2" rel="noopener ugc nofollow" target="_blank"><strong class="jj hv"><em class="md">reddit.com/r/dota2</em></strong></a><em class="md">和</em><a class="ae jg" href="https://www.nadota.com/" rel="noopener ugc nofollow" target="_blank"><strong class="jj hv"><em class="md">nadota.com</em></strong></a><em class="md">。</em></p><p id="3ddb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">python框架“Scrapy”用于从两个网站收集个人评论和帖子。对于这两个网站，scraper从尽可能多的线程收集了所有评论。对于nadota来说，整个Dota聊天’论坛都被刮了(大约500页的内容)。Reddit的内容要多得多，所以经过近3天的搜索，不到2年的内容就被删除了(超过1000万条个人评论)。如果我不限制刮刀的速度以免影响网站的服务器，这可能会快一点，但两年已经足够了。</p><h1 id="691b" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">消毒</h1><p id="b6fe" class="pw-post-body-paragraph jh ji hu jj b jk lh jm jn jo li jq jr js lj ju jv jw lk jy jz ka ll kc kd ke hn dt translated">论坛帖子的问题是语法和标点符号没有被强制执行(甚至很少被鼓励)，从一个词的评论到巨型ASCII艺术杰作，任何东西都可以在收集的数据中。为了最大限度地减少噪音，刮出的数据被净化，看起来尽可能“简洁”。特殊字符和键盘垃圾被丢弃，过多的标点符号被缩短为1个字符，句号被附加到没有它们的句子中，等等。因为目标是看看在没有严格完善的训练数据的情况下学习有多容易，所以净化仅限于不改变数据意义的生活质量变化。</p><p id="2716" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">消毒后，每个评论被分成单独的句子，然后再分成三元模型(3个单词的集合)，作为我们模型的输入。三元模型的顺序是随机的，以避免训练中的偏差。为了将数据降低到一个具有合理词汇量的工作集，任何包含出现次数不超过设定阈值的单词的句子都将被丢弃。</p><p id="949b" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="md">你可以通过</em><a class="ae jg" href="https://github.com/daendinam/dota-nn/blob/master/scraped-data/sanitize.py" rel="noopener ugc nofollow" target="_blank"><strong class="jj hv"><em class="md">sanitize . py</em></strong></a><em class="md">中的代码查看更多细节。</em></p><p id="4a91" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">下面是数据在分割成三元模型并序列化之前的预览:</p><pre class="me mf mg mh fq mi ki mj mk aw ml dt"><span id="7b2f" class="mm kk hu ki b fv mn mo l mp mq">It 's been a pub thing for a long time .<br/>I have done this , a lot .<br/>Glad Im not the only one .<br/>I have done this a lot too you are not alone .<br/>I must ask , which match was this ?<br/>By spamming meepo .</span></pre><p id="e655" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="md">感谢nadota.com ch0p允许刮。</em></p><h1 id="a73e" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">结果</h1><h2 id="694b" class="mm kk hu bd kl mr ms mt kp mu mv mw kt js mx my kx jw mz na lb ka nb nc lf nd dt translated"><strong class="ak"> <em class="lw"> 1。</em></strong><em class="lw">NADota.com</em></h2><blockquote class="ne nf ng"><p id="bcaa" class="jh ji md jj b jk jl jm jn jo jp jq jr nh jt ju jv ni jx jy jz nj kb kc kd ke hn dt translated"><strong class="jj hv">总评论刮掉:</strong> ~49万<br/> <strong class="jj hv">单词出现阈值:</strong> 500 <br/> <strong class="jj hv">总词汇量:</strong> 946 <br/> <strong class="jj hv">总训练输入:</strong>16.99万(50%) <br/> <strong class="jj hv">总验证和训练集规模:</strong>8.49万(25%，25%) <br/> <strong class="jj hv">训练设置</strong>:1</p></blockquote><p id="1ed5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">因为这个数据集与Reddit相比相当小，所以它主要用作概念验证，以便有信心迁移到更大的数据集。</p><p id="0824" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">判断性能的一种方法是给模型一个输入，然后看看输出有多合理。当我们给模型一个输入，继续取最大概率输出。尽管在当前输入之前它没有任何上下文，我们仍然生成看起来可信的句子:</p><pre class="me mf mg mh fq mi ki mj mk aw ml dt"><span id="eef3" class="mm kk hu ki b fv mn mo l mp mq">"I think that" -&gt; " 's a good player."<br/>"You are definitely" -&gt; "the best."<br/>"He plays a" -&gt; "game."<br/>"Why does nadota" -&gt; "play dota two./?"<br/>"Do you even" -&gt; "know what he did./?"<br/>"You should pick" -&gt; "me."<br/>"I like when" -&gt; "I m not sure if it was a good player."</span></pre><p id="10a4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了更直观地了解我们的模型是如何学习的，我们可以看看使用一种叫做t-SNE的算法绘制的分布式表示空间的二维图。该算法试图将16维空间中靠近的点映射为2维空间中靠近的点。你可以在这里 阅读关于t-SNE如何运作<a class="ae jg" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="jj hv">。</strong></a></p><p id="6dd3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">完整剧情如下。<br/> <em class="md">(注:词汇包括显性语言。)</em></p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nk"><img src="../Images/8cf78582b8c751f75dda1ee0c2d278d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KirkNX1g_Xc6O_TKnPYaNg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="https://daendinam.github.io/projects/images/nadota-freq500-(16-128)-tsne.png" rel="noopener ugc nofollow" target="_blank">Click for full resolution</a></figcaption></figure><p id="25c8" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们可以看到一些例子，表明我们的模型已经学习了语法和语义特征。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff nl"><img src="../Images/b86a88cabc1c692d7932affb1cfdf113.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*bWRxRgf0p3lUKaLBeGZDaA.png"/></div></figure><p id="94a1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在这里，我们看到嵌入将语义不同但语法相似的单词紧密地组合在一起，用于分隔独立的分句。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/ff20efa2ab64280bd9349c1225d49ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/0*lCU2mJMckldyy_U7.png"/></div></figure><p id="aeed" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">证据学习能够将替换拼法和俚语与它们的词根联系起来。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff nn"><img src="../Images/ea8365c8f42964c64ce3a30f9c2a5a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/0*6kr8OwVaF7feuOFv.png"/></div></figure><p id="7d12" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">一串语义相关的单词。</p><p id="a9e3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">所以我们有很好的证据证明训练数据不需要是完美的英语。接下来，我们来看看更大的数据集的结果。</p><h2 id="a792" class="mm kk hu bd kl mr ms mt kp mu mv mw kt js mx my kx jw mz na lb ka nb nc lf nd dt translated"><strong class="ak"> 2。</strong> Reddit</h2><blockquote class="ne nf ng"><p id="7dcc" class="jh ji md jj b jk jl jm jn jo jp jq jr nh jt ju jv ni jx jy jz nj kb kc kd ke hn dt translated"><strong class="jj hv">总评论刮:</strong>~ 1000万<br/> <strong class="jj hv">单词出现阈值:</strong> 4000 <br/> <strong class="jj hv">总词汇量:</strong>1780<br/><strong class="jj hv">总训练输入:</strong>837.14万(80%) <br/> <strong class="jj hv">总验证和训练集规模:</strong>104.64万(10%，10%) <br/> <strong class="jj hv">训练设置:】</strong></p></blockquote><p id="c9e7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这个数据集要大得多，所以希望我们能看到有趣的结果。</p><p id="b9f8" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">给模型一个前所未见的输入会产生相当合理的输出:</p><pre class="me mf mg mh fq mi ki mj mk aw ml dt"><span id="6604" class="mm kk hu ki b fv mn mo l mp mq">&gt;&gt;&gt; model.predict_next_word('correctly','learning','to')<br/>correctly learning to do Prob: 0.06963<br/>correctly learning to play Prob: 0.05980<br/>correctly learning to be Prob: 0.05595<br/>correctly learning to get Prob: 0.03048<br/>correctly learning to see Prob: 0.02387<br/>correctly learning to go Prob: 0.02322<br/>correctly learning to watch Prob: 0.02066<br/>correctly learning to win Prob: 0.01970<br/>correctly learning to pick Prob: 0.01947<br/>correctly learning to make Prob: 0.01940</span></pre><p id="bd85" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">该模型不仅正确地预测了一个最符合语法的动词，而且它的最佳选择对于学习环境都非常适用。</p><p id="bfc4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">从SNE霸王龙的图中我们能学到什么？</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nk"><img src="../Images/38d5736e98850704a9620dbf8f6882d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WaN-cWUzlO2sTKsZizw0zA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="https://daendinam.github.io/projects/images/reddit-full.png" rel="noopener ugc nofollow" target="_blank">Click for full resolution</a></figcaption></figure><p id="c4a9" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们可以再次看到该模型将语法和语义相似的单词紧密联系起来的好例子:</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff no"><img src="../Images/fb5c7e6a10c1de07b390c3ef7bf9d0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*eXziOYRRTNs7QcBf.png"/></div></figure><p id="5824" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">最后一个非常有趣，当你注意到模型将副词聚集在一起，以及聚集那些意思相似的副词——图中的副词都与时间有一些关系。</p><p id="9d40" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">针对论坛(DoTA2)主题的词汇呢？</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff np"><img src="../Images/180540623c7f862fd9b19cce75b6e220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e97QxJsTpJTal7gU7rs4cQ.png"/></div></div></figure><p id="ffca" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">从上面我们可以看到，该模型有一个区域包含许多DoTA2特有的代词，特别是有两个不同的集群将英雄(游戏角色)姓名与真实的玩家和组织名称分开。也就是说，绿色泡泡中的几乎所有东西都是指游戏中的虚构角色，而蓝色泡泡中的所有东西都是指真实的人或组织(即团队、品牌)。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff no"><img src="../Images/def3b1dbf1b70ad7b05a56046725f331.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*o_k8MM-O9HxGXs0X.png"/></div></figure><p id="ba59" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在另一个区域，我们看到了一串单词，它们都与游戏环境的特定属性相关。</p><p id="7821" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="md">(例如:</em> <strong class="jj hv"> <em class="md">速度:</em> </strong> <em class="md">你的角色或者法术移动的速度有多快。<br/> </em> <strong class="jj hv"> <em class="md">护甲:</em> </strong> <em class="md">用于计算人物抗伤害的统计。<br/></em><strong class="jj hv"><em class="md">【Regen:</em></strong><em class="md">再生的简称，一种使角色随时间再生生命值的属性。)</em></p><p id="07c3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这让我们了解到该模型能够很好地适应训练数据的上下文，包括特定于方言或社区的独特词汇和词义。</p><h2 id="c672" class="mm kk hu bd kl mr ms mt kp mu mv mw kt js mx my kx jw mz na lb ka nb nc lf nd dt translated">这是什么意思？</h2><p id="9c2d" class="pw-post-body-paragraph jh ji hu jj b jk lh jm jn jo li jq jr js lj ju jv jw lk jy jz ka ll kc kd ke hn dt translated">使用在网上找到的数据可以是一种合理的方法来开发一个特定环境的学习模型。人们可以通过一种非常有趣的方式来洞察语言在不同社区之间的进化和变形。一个有趣的练习可能是用不同年份的数据集训练两个聊天机器人，看看俚语和情绪的差异是否明显。</p><p id="2675" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">非常酷的是，我们已经证明了从不完整的数据(拼写错误、垃圾邮件等)中学习是可能的。)，并且仍然提出一个对语法有合理把握的模型，所有这些都不需要手动挖掘数据来清理。如果你熟悉深度学习，这可能对你来说并不令人震惊，但作为一个刚接触这个主题的人来说，这是一次非常有趣的学习经历！</p><h1 id="186e" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">博学的</h1><p id="630a" class="pw-post-body-paragraph jh ji hu jj b jk lh jm jn jo li jq jr js lj ju jv jw lk jy jz ka ll kc kd ke hn dt translated">抓取和处理大量数据会比预期花费更多的时间！在未来的日子里，我希望我能更多地关注时间安排(以及问题出现时的重新安排)。</p><p id="9ce1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">关于扩展的一些想法:在游戏世界中还有哪些其他形式的可用数据？多人游戏的比赛统计数据可以用来可靠地预测玩家技能吗？游戏内聊天记录呢？</p><p id="5860" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="md">所有代码都是</em> <a class="ae jg" href="https://github.com/daendinam/dota-nn" rel="noopener ugc nofollow" target="_blank"> <em class="md">在Github </em> </a> <em class="md">上可用并免费使用。请尽可能说明。</em></p><div class="me mf mg mh fq ab cb"><figure class="nq iv nr ns nt nu nv paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="nq iv nr ns nt nu nv paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="nq iv nr ns nt nu nv paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="ne nf ng"><p id="f922" class="jh ji md jj b jk jl jm jn jo jp jq jr nh jt ju jv ni jx jy jz nj kb kc kd ke hn dt translated"><a class="ae jg" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae jg" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae jg" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae jg" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jh ji md jj b jk jl jm jn jo jp jq jr nh jt ju jv ni jx jy jz nj kb kc kd ke hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jg" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jg" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nw"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="me mf mg mh fq iv"><div class="bz el l di"><div class="nx ny l"/></div></figure></div></div>    
</body>
</html>