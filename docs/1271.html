<html>
<head>
<title>How I deployed my spark document classification(Logistic Regression) model/s as a standalone app for real-time prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何将我的spark文档分类(逻辑回归)模型部署为独立的实时预测应用程序</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/how-i-deployed-my-spark-document-classification-logistic-regression-model-s-as-a-standalone-app-64b05b44e102?source=collection_archive---------2-----------------------#2016-10-04">https://medium.com/hackernoon/how-i-deployed-my-spark-document-classification-logistic-regression-model-s-as-a-standalone-app-64b05b44e102?source=collection_archive---------2-----------------------#2016-10-04</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="940d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">TLDR —使用管道保存从训练集生成的TF-IDF模型，以及用于预测的SVM模型。所以本质上保存两个模型，一个用于特征提取和输入转换，另一个用于预测。</p><p id="f47c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当你开发一个文本分类模型时，最大的挑战之一是，如果你的计划是离线训练，在某些情况下只部署模型进行预测，那么你得到的训练模型不足以进行预测。特别是在我们使用<a class="ae jp" href="http://en.wikipedia.org/wiki/Feature_hashing" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">【哈希技巧】</strong> </a>从训练集中提取特征，并使用<strong class="it hv">【逆文档频率】</strong>将特征/术语对文档的重要性归一化的情况下，文档中最频繁出现的术语实际上对整个语料库的重要性较低。根据spark网站，这通常被标记为“<a class="ae jp" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">术语频率-逆文档频率(TF-IDF) </a>是一种广泛用于文本挖掘的特征矢量化方法，以反映术语对语料库中文档的重要性”</p><p id="229f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果我们使用TF-IDF通过spark进行特征矢量化，我们通常会这样实现它</p><pre class="jq jr js jt fq ju jv jw jx aw jy dt"><span id="8269" class="jz ka hu jv b fv kb kc l kd ke"><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.mllib.feature.</strong>{<strong class="jv hv">HashingTF</strong>, <strong class="jv hv">IDF</strong>}<br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.mllib.linalg.Vector</strong><br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.rdd.RDD</strong><br/><br/><em class="kf">// Load documents (one per line).</em><br/><strong class="jv hv">val</strong> documents<strong class="jv hv">:</strong> RDD[Seq[String]] <strong class="jv hv">=</strong> sc.textFile("data/mllib/kmeans_data.txt")<br/>  .map(<strong class="jv hv">_</strong>.split(" ").toSeq)<br/><br/><strong class="jv hv">val</strong> hashingTF <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">HashingTF</strong>()<br/><strong class="jv hv">val</strong> tf<strong class="jv hv">:</strong> RDD[Vector] <strong class="jv hv">=</strong> hashingTF.transform(documents)<br/><br/><em class="kf">// While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:</em><br/><em class="kf">// First to compute the IDF vector and second to scale the term frequencies by IDF.</em><br/>tf.cache()<br/><strong class="jv hv">val</strong> idf <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">IDF</strong>().fit(tf)<br/><strong class="jv hv">val</strong> tfidf<strong class="jv hv">:</strong> RDD[Vector] <strong class="jv hv">=</strong> idf.transform(tf)</span></pre><p id="fbca" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，正如你所看到的，仅仅训练好的模型不足以进行独立的预测，因为我们必须从输入文档中提取特征，并归一化它们的词频，这一切都取决于训练集，我们不想将训练集包括在实时预测中(耗时，并增加应用程序的内存消耗)。</p><p id="a5d1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，从spark 1.3开始，引入了管道，我们可以使用管道自动化我们的提取、转换和预测工作流。从spark 1.6开始，我们能够保存包含所有工作流的管道模型。因此，如果我们想使用管道离线训练模型并预测某个地方，它们就是goto解决方案。因此，如果我们想使用逻辑回归来训练和预测，这就是我们可以做的(摘自<a class="ae jp" href="http://spark.apache.org/docs/latest/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank">http://spark.apache.org/docs/latest/ml-pipeline.html</a>)。</p><pre class="jq jr js jt fq ju jv jw jx aw jy dt"><span id="a40b" class="jz ka hu jv b fv kb kc l kd ke"><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.ml.</strong>{<strong class="jv hv">Pipeline</strong>, <strong class="jv hv">PipelineModel</strong>}<br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.ml.classification.LogisticRegression</strong><br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.ml.feature.</strong>{<strong class="jv hv">HashingTF</strong>, <strong class="jv hv">Tokenizer</strong>}<br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.ml.linalg.Vector</strong><br/><strong class="jv hv">import</strong> <strong class="jv hv">org.apache.spark.sql.Row</strong><br/><br/><em class="kf">// Prepare training documents from a list of (id, text, label) tuples.</em><br/><strong class="jv hv">val</strong> training <strong class="jv hv">=</strong> spark.createDataFrame(<strong class="jv hv">Seq</strong>(<br/>  (0L, "a b c d e spark", 1.0),<br/>  (1L, "b d", 0.0),<br/>  (2L, "spark f g h", 1.0),<br/>  (3L, "hadoop mapreduce", 0.0)<br/>)).toDF("id", "text", "label")<br/><br/><em class="kf">// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.</em><br/><strong class="jv hv">val</strong> tokenizer <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">Tokenizer</strong>()<br/>  .setInputCol("text")<br/>  .setOutputCol("words")<br/><strong class="jv hv">val</strong> hashingTF <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">HashingTF</strong>()<br/>  .setNumFeatures(1000)<br/>  .setInputCol(tokenizer.getOutputCol)<br/>  .setOutputCol("features")<br/><strong class="jv hv">val</strong> lr <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">LogisticRegression</strong>()<br/>  .setMaxIter(10)<br/>  .setRegParam(0.01)<br/><strong class="jv hv">val</strong> pipeline <strong class="jv hv">=</strong> <strong class="jv hv">new</strong> <strong class="jv hv">Pipeline</strong>()<br/>  .setStages(<strong class="jv hv">Array</strong>(tokenizer, hashingTF, lr))<br/><br/><em class="kf">// Fit the pipeline to training documents.</em><br/><strong class="jv hv">val</strong> model <strong class="jv hv">=</strong> pipeline.fit(training)<br/><br/><em class="kf">// Now we can optionally save the fitted pipeline to disk</em><br/>model.write.overwrite().save("/tmp/spark-logistic-regression-model")<br/><br/><em class="kf">// We can also save this unfit pipeline to disk</em><br/>pipeline.write.overwrite().save("/tmp/unfit-lr-model")<br/><br/><em class="kf">// And load it back in during production</em><br/><strong class="jv hv">val</strong> sameModel <strong class="jv hv">=</strong> <strong class="jv hv">PipelineModel</strong>.load("/tmp/spark-logistic-regression-model")<br/><br/><em class="kf">// Prepare test documents, which are unlabeled (id, text) tuples.</em><br/><strong class="jv hv">val</strong> test <strong class="jv hv">=</strong> spark.createDataFrame(<strong class="jv hv">Seq</strong>(<br/>  (4L, "spark i j k"),<br/>  (5L, "l m n"),<br/>  (6L, "mapreduce spark"),<br/>  (7L, "apache hadoop")<br/>)).toDF("id", "text")<br/><br/><em class="kf">// Make predictions on test documents.</em><br/>model.transform(test)<br/>  .select("id", "text", "probability", "prediction")<br/>  .collect()<br/>  .foreach { <strong class="jv hv">case</strong> <strong class="jv hv">Row</strong>(id<strong class="jv hv">:</strong> Long, text<strong class="jv hv">:</strong> String, prob<strong class="jv hv">:</strong> Vector, prediction<strong class="jv hv">:</strong> Double) <strong class="jv hv">=&gt;</strong><br/>    println(s"($id, $text) --&gt; prob=$prob, prediction=$prediction")<br/>  }</span></pre><p id="9c64" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这是完美的解决方案，但是并不是所有的逻辑回归算法都被支持，只有<strong class="it hv">逻辑回归</strong>和<strong class="it hv">朴素贝叶斯</strong>被支持。</p><p id="ddc9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以，如果我们想用SVM的或者T2的，我们就没那么幸运了。为了使这些算法库支持流水线，它们必须实现一种称为“fit”的方法。更准确地说，管道基于转换器和评估器的概念工作，无论我们在管道工作流中放入什么，都必须是其中之一。我们的算法模型是估值器，因为它们训练或拟合数据。fit()方法接受dataframe并返回“pipelineModel”。SVM不支持这种方法。</p><p id="6391" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我试图使现有的SVM成为一个估计器，但没有太大的成功，因为似乎完全缺乏关于如何创建我们自己的估计器和转换器的文档。我很乐意听到任何能够做到这一点的人的意见。</p><p id="a9c4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，经过一周的深思熟虑，我想到了一个办法，那就是使用管道。听起来令人困惑——我知道。我来细说一下。</p><p id="c59d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果我们将管道升级设置为仅生成IDF模型，会怎么样？。这将输出一个管道模型，该模型可以与经过训练的SVM模型一起保存。</p><p id="e0cc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我没有只保存一个SVM模型用于预测，而是使用管道生成一个提取和转换模型，包括“标记、提取和转换”阶段，以生成一个<strong class="it hv"> pipelineModel </strong>并保存它。</p><p id="7e3a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下面是我用来保存SVM和管道模型的最终代码片段。</p><pre class="jq jr js jt fq ju jv jw jx aw jy dt"><span id="aad1" class="jz ka hu jv b fv kb kc l kd ke">import org.apache.spark.ml.{Pipeline, PipelineModel}<br/>import org.apache.spark.ml.feature.{HashingTF, Tokenizer}<br/>val tokenizer = new Tokenizer()<br/> .setInputCol(“text”)<br/> .setOutputCol(“words”)<br/>val hashingTF = new HashingTF()<br/> .setInputCol(tokenizer.getOutputCol)<br/> .setOutputCol(“rawFeatures”)<br/>val idf = new IDF().setInputCol(“rawFeatures”).setOutputCol(“features”)</span><span id="2ff1" class="jz ka hu jv b fv kg kc l kd ke">val pipeline = new Pipeline()<br/> .setStages(Array(tokenizer, hashingTF, idf))<br/>val pipelineModel = pipeline.fit(training_df)<br/>pipelineModel.save(“somewhere”) # Saving the pipeline model<br/>val t = pipelineModel.transform(training_df).select(“features”, “label”).map( row =&gt; LabeledPoint(<br/> row.getAs[Double](“label”), <br/> row.getAs[org.apache.spark.mllib.linalg.Vector](“features”)<br/>))</span><span id="50b5" class="jz ka hu jv b fv kg kc l kd ke">val svm_model = new SVMWithSGD().run(t)<br/>svm_model.clearThreshold()<br/>svm_model.save(sc,”somewhere”) // Saving SVM model for prediction</span></pre><p id="c121" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">对于没有训练数据的独立预测，加载两个模型，将输入文本转换为提取特征的数据帧，并将转换后的输入DF传递给SVM模型。</p><p id="bfdd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">注意</strong> :-前两个代码片段引用自<a class="ae jp" href="http://spark.apache.org/docs/latest/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank">http://spark.apache.org/docs/latest/ml-pipeline.html</a></p><p id="a6f1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">PS:-如果有的话，我想听听替代的解决方案和更正。谢谢</p><blockquote class="kh ki kj"><p id="daed" class="ir is kf it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated"><a class="ae jp" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是阿妹家庭的一员。我们现在<a class="ae jp" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae jp" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="dca4" class="ir is kf it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">要了解更多信息，请阅读我们的“关于”页面、<a class="ae jp" href="http://bit.ly/HackernoonFB" rel="noopener ugc nofollow" target="_blank">在脸书</a>上给我们点赞/发消息，或者简单地发送<a class="ae jp" href="https://goo.gl/k7XYbx" rel="noopener ugc nofollow" target="_blank"> tweet/DM @HackerNoon。</a></p><p id="708a" class="ir is kf it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jp" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jp" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote></div></div>    
</body>
</html>