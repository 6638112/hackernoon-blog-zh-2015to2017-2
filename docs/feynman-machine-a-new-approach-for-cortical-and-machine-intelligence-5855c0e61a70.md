# 费曼机器:大脑皮层和机器智能的新途径

> 原文：<https://medium.com/hackernoon/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70>

[深度学习](https://hackernoon.com/tagged/deep-learning)可能不会带来你一直期待的人工智能革命。这一季的炒作仅仅是 80 年代末的重演，只是规模大得多吗？也许甲壳虫乐队时代的技术再次迎来了冬天，因为它是基于二战的神经科学和维多利亚时代的统计力学。幸运的是，如果一种新方法可以将计算神经科学和应用数学的最新知识与今天的 GPU 的能力相融合，那么深度学习的新能量、热情和投资不必浪费。[费曼机器](https://arxiv.org/abs/1609.03971)既是对大脑实际工作方式的准确描述，也是机器智能的蓝图。结合耦合、通信、混沌动力系统的应用数学和神经科学的最新发现，我们在一年前成立了 [Ogma](https://ogma.ai) ，将[理论](https://arxiv.org/abs/1512.05245)转化为[工作软件](https://github.com/ogmacorp/OgmaNeo)，为新的人工智能技术奠定基础。

我不会过多地谈论为什么我认为今天的深度学习热潮不是它被大肆宣传的灵丹妙药。DL 自己的许多长期被忽视和新近被称赞的领导人已经在这样做了(其中最突出的是 [Yoshua Bengio](https://www.quora.com/Is-the-current-hype-about-Deep-Learning-justified?redirected_qid=6578691) 、 [Yann LeCun](https://www.quora.com/What-are-the-limits-of-deep-learning-2/answer/Yann-LeCun) 和 [Geoff Hinton](https://www.youtube.com/watch?v=VIRCybGgHts) )，尽管他们最近受到了高调的任命和天文数字的金融承销。在不同程度上，他们现在正超越最近数字图书馆的成功应用，寻找可能提供下一步发展的新想法。

DL 的主要弱点(在我看来)是:依赖尽可能简单的模型神经元(LeCun 称之为“卡通化”)；使用 19 世纪统计力学和统计学的思想，这是能量函数和对数似然方法的基础；这些技术的结合，如反向投影和随机梯度下降，导致应用范围非常有限(离线，主要是批量，监督学习)，需要非常有才华的从业者(又名“[随机研究生下降](https://twitter.com/kaydeeb0y/status/626469795391778816)”)，大量昂贵的标记训练数据和计算能力。虽然对于那些可以吸引或购买人才并部署无限资源来收集和处理数据的大公司来说，DL 是伟大的，但对我们大多数人来说，它既不容易获得也没有用。

另一方面，费曼机器利用从整个研究领域中学习，这在 20 世纪 60 年代和 70 年代之前根本不存在。第一次，它们被结合在一个模型中，该模型描述了新大脑皮层如何处理高速、流动的感觉数据并产生行为和认知，这也是智能机器的一种新架构。我将简要描述这些结果，以及它们是如何在自然和人工费曼机器中使用的。

首先要学习的是应用数学的一个分支，它研究非线性动力系统(NDSs)的特性，也就是通常所说的混沌理论。科学中的大多数现象(包括自然现象和人类现象)只能被精确地建模为 NDSs，但这在 20 世纪 60 年代早期计算机广泛普及之前并不实用。爱德华·洛伦茨在 1963 年发表的具有里程碑意义的论文被广泛认为是对一类完全未知的行为:确定性混沌的第一次详细描述。虽然使用传统的数学工具几乎不可能处理，但涉及 NDSs 的系统具有如此丰富的结构，并且在本质上如此普遍，以至于它们的研究已经主导了应用数学超过 40 年。

1978 年，[帕卡德*等人*](http://www.csee.wvu.edu/~xinl/library/papers/physics/packard1980.pdf) 发现了来自混沌系统的信号的一些意想不到的东西:它们包含了重建混沌系统的整个时间行为和预测其未来所需的所有信息，而*却没有任何关于驱动系统的实际机制的知识*。1981 年，芙罗莉丝·塔肯斯根据这个惊人的事实证明了他著名的定理。来自 George Sugihara 实验室的以下视频简要描述了该定理如何工作，以及如何使用重建来检查和分析 NDSs(这里使用 Lorenz 系统进行说明):

视频中描述的因果关系标准后来被称为 Sugihara 因果关系，它比传统上用于统计和[机器学习](https://hackernoon.com/tagged/machine-learning)的格兰杰因果关系更强大，因为它使用了时间序列信号中的时间结构。George Sugihara 是在渔业可持续性和基因网络等领域应用这些强大方法的领军人物之一。

费曼机器的中心思想是，大脑的各个区域形成了一个 NDSs 网络，它们通过相互发送神经信号的时间序列来进行交流和合作，而认知是从耦合的动力系统的因果相互作用中产生的。应用数学告诉我们这是可能的，但是我们现在需要依靠神经科学来解释这在真实的大脑中是如何发生的。

1986 年，future Palm 创始人杰夫·霍金斯(Jeff Hawkins)写了一篇论文提案，其中他描述了一个不断预测自身未来进化的新大脑皮层模型。由于当时对理论神经科学缺乏兴趣，直到 2000 年代初，霍金斯才能够回到这个想法，并投入全部时间来领导它的发展。他和桑德拉·布莱克斯利一起写了关于智力的[](https://en.wikipedia.org/wiki/On_Intelligence)*，成立了理论神经科学的[雷德伍德中心](http://redwood.berkeley.edu/)(现在是加州大学伯克利分校的一部分)，并共同创立了 [Numenta](http://numenta.com/) ，在那里他的理论，现在被称为分级时间记忆，继续得到研究和发展。霍金斯(以及努门塔的同事苏布泰·阿默德和·崔)将他的所有理论建立在硬神经科学的基础上，除非有强有力的证据证明它存在于大脑中，否则什么也不包括。2013 年，Numenta 开源了他们的 NuPIC HTM 软件，我成为了全球社区的一员，与 Hawkins 一起探索 HTM 作为皮层模型和一种有前途的机器学习技术。我 2015 年的[论文](https://arxiv.org/abs/1509.08255)是第一份关于 HTM 如何运作的深度数学描述。*

*那篇论文是概述我在 2015 年的发现的两篇论文中的第一篇，我的发现是耦合 NDS 的机制是哺乳动物大脑计算能力的秘密来源。当我看到圣达菲学院的 Melanie Mitchell 的这个演讲并在复杂性探索 MOOC 网站上学习她的课程时，我有了这个想法。我太老了，在大学里没有学过这些，所以这是我第一次有机会了解复杂系统的广阔世界，以及最近开发的探索和利用它们的方法的力量。*

*我的第二篇论文——[Synapses 的交响乐:Neocortex 作为一个使用分级时间记忆的通用动力系统建模器](https://arxiv.org/abs/1512.05245) —概述了大脑如何作为一种新型的计算机器，使用 Takens 定理处理流数据，紧急自我组织认知，并智能地行为。HTM 被用来证明新皮层各层神经元之间的直接联系和网络 NDSs 的紧急处理。因为 Numenta 已经有了可以在小范围内模拟大脑皮层计算的工作软件，所以我提出，只要建立正确的这种模块网络，智能机器就可以实现。*

*我们现在知道(h/t George Sugihara)在真正的灵长类动物新大脑皮层中有[强有力的证据](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004537)证明费曼机器过程。有趣的是，这项分析使用了 Takens 定理和 Sugihara 的方法，找到了跨越一百多个大脑区域的因果网络(Tajima 因这篇论文获得了[这个奖](http://www.theassc.org/past_recipients))。就在几周前，同一个团队[在这种分析和 Tononi-Koch 信息整合理论之间建立了联系](https://arxiv.org/abs/1701.05157)。*

*大约在 2015 年末的同一时间，Eric Laukien 正在开发[高性能 GPU 技术](https://github.com/222464/NeoRL)，大致基于我们在 HTM 社区中讨论的想法。2016 年 1 月，埃里克、我们的朋友理查德·克劳德和我(在埃里克的父亲马克的支持下)成立了 [Ogma](https://ogma.ai/) 来开发一种新技术，将理论与埃里克在 GPU 驱动的机器学习方面的经验相融合。奥格马是以奥格马·麦克·埃拉特汉的名字命名的，奥格米奥斯是盖尔人的半神人，也是爱尔兰第一个书写系统奥格汉姆的发明者。*

*去年九月，我们发表了第一篇关于我们现在称之为*费曼机器*的论文[，并发布了我们](https://arxiv.org/abs/1609.03971) [OgmaNeo 软件套件](https://github.com/ogmacorp)的第一个版本。*

*正如该理论所预测的，大脑区域只是各种自适应学习 NDS 的一个例子，当这些 NDS 适当地连接在一起时，它们将自组织并紧急合作，形成费曼机器。没有必要模仿真实神经元的所有细节(如人脑项目模型中所见)，甚至也没有必要模仿 HTM 中神经元、列和层的更简单的抽象。同样，传统的深度学习给了我们一些并行建模和实现高性能、自适应数据转换的想法，但我们也可以避免它的许多限制，因为费曼机器中的每个模块都是半独立的预测学习器，并且学习是在线的、本地的和无监督的。*

*费曼机器是为了纪念我们的英雄理查德·p·费曼而命名的，原因有很多。从洛斯阿拉莫斯到 20 世纪 80 年代参与早期大规模并行算法，他都是计算领域的先驱，是约翰·冯·诺依曼的同事，后者现在是现代数字计算机架构的同义词，尽管冯·诺依曼的设计灵感来自于他对皮层神经元的理解。我们希望，如果费曼能活着看到神经科学和应用数学的最新进展，他会理解这些想法的简单性，甚至可能已经自己想出了它们。我们最近发现，他的姐姐琼(Joan)自己也是一位杰出的物理学家，她的一篇开创性论文就是基于塔肯斯定理。我们可能有理由称之为费曼-费曼机器，因为理查德可能有这种洞察力，如果琼有机会解释 NDSs 的力量。*

*无论如何，我们的论文在发表的那一周被列为《麻省理工技术评论》[最佳物理学论文 arXiv](https://www.technologyreview.com/s/602455/the-best-of-the-physics-arxiv-week-ending-september-24-2016/) 之一，我们将在 3 月份由麻省理工学院的[大脑、思维和机器中心](http://cbmm.mit.edu/)组织的即将举行的[神经启发计算元素](https://www.src.org/calendar/e006125)研讨会和 [AAAI 智能科学春季研讨会](http://cbmm.mit.edu/knowledge-transfer/workshops-conferences-symposia/science-intelligence-computational-principles)上展示海报(如果你也参加，请顺便来看看我们的海报)。*

*我将在下一篇文章中解释费曼机器的细节，但同时这里有一个最近在 NDSs 的“Hello World”上运行的演示，Lorenz 系统:*

*“嘈杂的”洛伦兹吸引子(基于最近的论文)在这里被使用，因为它比普通的 NDS 更具挑战性。关于这个演示，有几个要点可能不太明显。首先，左上角的步数是 FM 随机初始化后看到的数据点数，所以这是在线学习，完全从零开始，没有预训练。第二，FM 看到的信号只是高度嘈杂的观察结果，而不是大多数视频中显示的平滑 Takens 轨迹，所以相似之处是因为 FM 在嘈杂的数据中找到了真实的信号。第三，这代表了在我的 Macbook Pro 上以每秒 60 步的速度运行的整个 8 层网络，其中包括绘制所有的 3D 图形。*

*请继续关注我的第二篇文章，它将描述费曼机器的内部工作原理。*