<html>
<head>
<title>DL03: Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DL03:梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/dl03-gradient-descent-719aff91c7d6?source=collection_archive---------12-----------------------#2017-10-20">https://medium.com/hackernoon/dl03-gradient-descent-719aff91c7d6?source=collection_archive---------12-----------------------#2017-10-20</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="1dcb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">之前的两个帖子可以在这里找到:<br/> <a class="ae jp" href="https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864" rel="noopener ugc nofollow" target="_blank"> DL01:神经网络理论</a><br/>T5】DL02:从零开始写一个神经网络(代码)</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/4a434b690182e66d818860332632381c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5y-MqiYKmizFhdI-x8peaw.jpeg"/></div></div></figure><p id="30c0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">休息时间到了，程序员们！让我们钻研一下数学，好吗？*插入书呆子的微笑*</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kc"><img src="../Images/3ebf07091cd2b850e4456663d2f7e8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*VW4luAF5NCxhOykO.gif"/></div></figure><p id="5b64" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了理解梯度下降，让我们来看看线性回归。线性回归是一种技术，其中给定一些数据点，我们试图拟合一条穿过这些点的线，然后通过外推该线来进行预测。挑战在于找到最适合该系列的产品。为了简单起见，我们假设输出(<em class="kd"> y </em>)只依赖于一个输入变量(<em class="kd"> x </em>)，即给我们的数据点的形式为<em class="kd"> (x，y) </em>。</p><p id="eff8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们想通过这些数据点拟合一条线。这条线显然将是<code class="eh ke kf kg kh b">y = mx + b</code>的形式，我们想要找到<em class="kd"> m </em>和<em class="kd"> b的最佳值</em>这条线被称为我们的“假设”。</p><p id="fe57" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，<em class="kd"> m </em>和<em class="kd"> b </em>被随机初始化。因此，一开始我们得到一条随机的线。我们的目标是更新这些值，使结果行产生最小的误差。我们将使用均方误差作为<strong class="it hv">成本函数(J) </strong>，它计算实际值输出和假设预测之间的误差。</p><p id="dc69" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">J(m，b) = (1/n) * <strong class="it hv"> ∑ </strong> (yᵢ - (mxᵢ + b))</p><p id="b8ab" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">其中<em class="kd"> n </em>表示数据点的总数，<em class="kd"> i </em>表示<em class="kd"> iᵗʰ </em>数据点(<em class="kd"> i </em>取1到n的值)。</p><p id="bf45" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这个误差函数典型的是<strong class="it hv">凸</strong>。简单地说，我们可以说这个误差函数通常只有一个最小值(全局最小值)。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ki"><img src="../Images/aa071de8a887df843617e874e28c4492.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*X09H19Ylqmw_Yjw9.png"/></div><figcaption class="kj kk fg fe ff kl km bd b be z ek">A convex function</figcaption></figure><p id="aca2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">横轴代表函数依赖的变量，纵轴代表输出。在我们的例子中，水平轴由<em class="kd"> m </em>和<em class="kd"> b </em>表示，而垂直轴由<em class="kd">y</em>表示</p><p id="8795" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当我们从随机值<em class="kd"> m </em>和<em class="kd"> b </em>开始，我们相应地得到一些值<em class="kd"> y </em>。该图最低点的误差最小，因此我们的目标是沿着斜坡向下移动，最终到达最底部的点。问题是，怎么做？</p><p id="627a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在继续之前，我想让你回忆一下你的高中数学。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kc"><img src="../Images/37080fe4f61bed5c4b86c590c6483d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*P4Y5Qm3NKUxwEo6U.jpg"/></div></figure><p id="a143" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">图上任意一点的切线斜率等于该图相对于输入变量的导数。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kn"><img src="../Images/a9e70f4a2c5d61772f9362f41ff82ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*V-1bZWjkjZm6K4vV.png"/></div><figcaption class="kj kk fg fe ff kl km bd b be z ek">Source: <a class="ae jp" href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/</a></figcaption></figure><p id="565e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在，如你所见，图中最底部点的切线斜率为0，即J在最底部点的偏导数为0。为了到达最底部的点，我们必须在斜坡的方向上移动，也就是说，我们将更新<em class="kd"> m </em>和<em class="kd"> b </em>的值，以便我们最终到达最佳值，其中误差函数最小。</p><p id="ef5d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">更新方程式如下:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ko"><img src="../Images/6c735d06be40fd34cfb09d703d20c686.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/1*juys82bHN0QASTd0vkQbSA.gif"/></div></figure><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kp"><img src="../Images/1958e4c607aee2ad215101ef49c14916.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/1*2qRBbz4EpXLWLwxWsT9JEw.gif"/></div></figure><p id="b202" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里，α被称为“学习率”，它决定了在梯度方向上要走多大的步长。如果α的值非常小，达到最佳值是有保证的，但是要花很多时间才能收敛。如果α非常大，那么<em class="kd"> m </em>和<em class="kd"> b </em>的值可能会超过最佳值，然后误差将开始增大而不是减小。因此，学习率在凸优化中起着重要的作用。</p><p id="b538" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">梯度下降的直观表现如下所示:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kq"><img src="../Images/e532c93b6618d183a883f4bd0fb8b519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D7zG46WrdKx54pbU.gif"/></div></div><figcaption class="kj kk fg fe ff kl km bd b be z ek">Source: <a class="ae jp" href="https://alykhantejani.github.io/images/gradient_descent_line_graph.gif" rel="noopener ugc nofollow" target="_blank">https://alykhantejani.github.io/images/gradient_descent_line_graph.gif</a></figcaption></figure><h1 id="6616" class="kr ks hu bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo dt translated">它是如何直观地工作的？</h1><p id="f862" class="pw-post-body-paragraph ir is hu it b iu lp iw ix iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo hn dt translated">为了简单起见，考虑一个误差函数<em class="kd"> J </em>，它只依赖于一个变量(或权重)<em class="kd"> w </em>。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lu"><img src="../Images/87722830364c503a93d5782ce3fe6f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yzBz_qja1v8_dIse.png"/></div></div></figure><p id="3888" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">考虑如图所示的情况，即初始重量超过最佳值。在这里，最初的斜率是大而正的。因此，在更新等式中，w减少了。随着w不断减小，注意梯度也减小，因此更新变得越来越小，最终，它收敛到最小值。</p><p id="3d3d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在考虑初始权重小于最优值的情况。这里，斜率是负的。因此，在更新等式中，w的值在每次更新后都会增加。最初，斜率很大，因此更新也很大。随着w的增加，斜率变得越来越小，因此更新变得越来越小，最终收敛到最小值。</p><p id="0353" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，在任何情况下，权重将最终收敛到一个值，使得成本函数的导数为0(假设α的值足够小)。</p><p id="96ad" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在这篇文章中，我讨论了线性回归的梯度下降。然而，这也是神经网络背后的基本思想。根据该算法更新神经网络中的所有权重。梯度下降用于<a class="ae jp" href="https://hackernoon.com/dl04-backpropagation-bbcfbf2528d6" rel="noopener ugc nofollow" target="_blank">反向传播算法</a>中的优化，这将在<a class="ae jp" href="https://hackernoon.com/dl04-backpropagation-bbcfbf2528d6" rel="noopener ugc nofollow" target="_blank">下一篇文章</a>中介绍。</p><blockquote class="lv lw lx"><p id="b03e" class="ir is kd it b iu iv iw ix iy iz ja jb ly jd je jf lz jh ji jj ma jl jm jn jo hn dt translated"><strong class="it hv">梯度下降101: </strong> <br/>有时候正确方向上的最小一步，到头来却是你人生的最大一步。</p></blockquote></div></div>    
</body>
</html>