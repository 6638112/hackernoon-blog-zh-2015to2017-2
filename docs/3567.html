<html>
<head>
<title>Learning to Learn by Gradient Descent by Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过梯度下降来学习</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2?source=collection_archive---------3-----------------------#2017-04-09">https://medium.com/hackernoon/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2?source=collection_archive---------3-----------------------#2017-04-09</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="005a" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">TensorFlow中尽可能简单</h2></div><p id="ecaf" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">当我第一次看到DeepMind的论文“<a class="ae kf" href="https://arxiv.org/abs/1606.04474" rel="noopener ugc nofollow" target="_blank">通过梯度下降学习通过梯度下降学习</a>”时，我的反应是“哇，这到底是怎么回事？”。不幸的是，我第一次通读这篇论文并不那么有启发性，而且看着<a class="ae kf" href="https://github.com/deepmind/learning-to-learn" rel="noopener ugc nofollow" target="_blank">代码</a>令人望而生畏。</p><p id="889d" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">谢天谢地，我被激起了足够的好奇心，强迫自己重新深入阅读这篇论文，它实际上最终被证明是惊人的简单。就我个人而言，当我试图理解某事时，真正帮助我的是创建问题的最简单的非平凡版本，然后从那里扩大规模。这是我能创造的最简单的想法，也是<em class="kg">没有数学方程式！</em>我希望你会觉得它很有启发性。</p><p id="5072" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我建议先略读这篇论文，但这应该是可以理解的。</p><p id="f257" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">本文带有语法高亮的<a class="ae kf" href="https://hackernoon.com/tagged/ipython" rel="noopener ugc nofollow" target="_blank"> IPython </a> <a class="ae kf" href="https://hackernoon.com/tagged/notebook" rel="noopener ugc nofollow" target="_blank">笔记本</a>版本可以在<a class="ae kf" href="https://github.com/LlionJ/blog/blob/master/blog/Meta-Learning.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。代码在TF v1.0.0-rc1上运行。</p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><p id="99b9" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">所以让我们开始吧。是时候通过阅读我的文章来学习如何通过梯度下降来学习了！</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="1012" class="kx ky hu kt b fv kz la l lb lc">import tensorflow as tf</span></pre><p id="ff2b" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">首先，我们需要元学习优化器解决一个问题。让我们从论文中取一个最简单的实验；求多维二次函数的最小值。我们将随机缩放抛物线，从随机位置开始，解总是在(0，0)。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="ba7c" class="kx ky hu kt b fv kz la l lb lc">DIMS = 10  # Dimensions of the parabola</span><span id="10e3" class="kx ky hu kt b fv ld la l lb lc">scale = tf.random_uniform([DIMS], 0.5, 1.5)</span><span id="3a35" class="kx ky hu kt b fv ld la l lb lc"># This represents the network/function we are trying to optimize,<br/># the `optimizee' as it's called in the paper.<br/># Actually, it's more accurate to think of this as the error<br/># landscape.<br/>def f(x):<br/>    x = scale*x<br/>    return tf.reduce_sum(x*x)</span></pre><p id="1d86" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们在这里不能轻易使用TensorFlow的内置优化器，因为该技术要求我们<em class="kg">在计算图</em>中展开训练循环，稍后我们会看到。因此，让我们定义几个简单的手工优化器来测试我们学习的优化器。正如本文所讨论的，优化器是一个函数<em class="kg"> g </em>,它在给定的步长获取参数的梯度，并返回您应该在参数空间中为该参数采取的步长。这里是普通梯度下降:(一些优化器需要跟踪状态，这里我只是传递参数)</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="b533" class="kx ky hu kt b fv kz la l lb lc">def g_sgd(gradients, state, learning_rate=0.1):<br/>    return -learning_rate*gradients, state</span></pre><p id="5f7c" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了获得更强的基线，让我们使用RMSProp:</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="7c98" class="kx ky hu kt b fv kz la l lb lc">def g_rms(gradients, state, learning_rate=0.1, decay_rate=0.99):<br/>    if state is None:<br/>        state = tf.zeros(DIMS)<br/>    state = decay_rate*state + (1-decay_rate)*tf.pow(gradients, 2)<br/>    update = -learning_rate*gradients / (tf.sqrt(state)+1e-5)<br/>    return update, state</span></pre><p id="1c5f" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">很好，现在让我们展开所有的训练步骤，这里的<em class="kg"> learn </em>是一个函数，它采用这些优化器中的一个，并将其应用于若干步骤的循环中，并收集函数<em class="kg"> f </em>在每一点的值，我们可以认为这是我们的损失。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="64c0" class="kx ky hu kt b fv kz la l lb lc">TRAINING_STEPS = 20  # This is 100 in the paper</span><span id="6d9d" class="kx ky hu kt b fv ld la l lb lc">initial_pos = tf.random_uniform([DIMS], -1., 1.)</span><span id="7df6" class="kx ky hu kt b fv ld la l lb lc">def learn(optimizer):<br/>    losses = []<br/>    x = initial_pos<br/>    state = None<br/>    for _ in range(TRAINING_STEPS):<br/>        loss = f(x)<br/>        losses.append(loss)<br/>        grads, = tf.gradients(loss, x)<br/>      <br/>        update, state = optimizer(grads, state)<br/>        x += update<br/>    return losses</span></pre><p id="21f6" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">好了，现在我们来测试一下。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="aef8" class="kx ky hu kt b fv kz la l lb lc">sgd_losses = learn(g_sgd)<br/>rms_losses = learn(g_rms)</span></pre><p id="878b" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">看看损失是什么样的。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="3d7d" class="kx ky hu kt b fv kz la l lb lc">sess = tf.InteractiveSession()<br/>sess.run(tf.global_variables_initializer())</span><span id="4536" class="kx ky hu kt b fv ld la l lb lc">import matplotlib<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import numpy as np</span><span id="0b51" class="kx ky hu kt b fv ld la l lb lc">x = np.arange(TRAINING_STEPS)<br/>for _ in range(3): <br/>    sgd_l, rms_l = sess.run([sgd_losses, rms_losses])<br/>    p1, = plt.plot(x, sgd_l, label='SGD')<br/>    p2, = plt.plot(x, rms_l, label='RMS')<br/>    plt.legend(handles=[p1, p2])<br/>    plt.title('Losses')<br/>    plt.show()</span></pre><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff le"><img src="../Images/fa5f56fffb92df1b9c1df96190c8ed68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*wlxKOOLQfKrrIDcvVRrk7Q.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff li"><img src="../Images/48a9ddbc761e038eb67004206b7cd8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*-F1Im3igGT_jP5rhhvWMfg.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff li"><img src="../Images/36f2f135fcf8ed8b3ede8de2dbda01dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*A_OhBAucKt1aRsyLsnEppQ.png"/></div></figure><p id="b27f" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">RMS Prop在这里的表现优于普通梯度下降。请注意，到目前为止还没有发生什么不寻常的事情，我只是手工滚动了我自己的优化器，并将整个训练展开到一个单独的计算图中，这通常是不推荐的，因为你会很快耗尽内存！</p><h2 id="899b" class="kx ky hu bd lj lk ll lm ln lo lp lq lr js ls lt lu jw lv lw lx ka ly lz ma mb dt translated">元学习</h2><p id="56da" class="pw-post-body-paragraph jj jk hu jl b jm mc iv jo jp md iy jr js me ju jv jw mf jy jz ka mg kc kd ke hn dt translated">是时候整合我们的元学习优化器了，我们将使用与论文中相同的架构:一个有2层和20个隐藏单元的LSTM。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="0075" class="kx ky hu kt b fv kz la l lb lc">LAYERS = 2<br/>STATE_SIZE = 20</span><span id="cb5e" class="kx ky hu kt b fv ld la l lb lc">cell = tf.contrib.rnn.MultiRNNCell(<br/>    [tf.contrib.rnn.LSTMCell(STATE_SIZE) for _ in range(LAYERS)])<br/>cell = tf.contrib.rnn.InputProjectionWrapper(cell, STATE_SIZE)<br/>cell = tf.contrib.rnn.OutputProjectionWrapper(cell, 1)<br/>cell = tf.make_template('cell', cell)</span><span id="959f" class="kx ky hu kt b fv ld la l lb lc">def g_rnn(gradients, state):<br/>    # Make a `batch' of single gradients to create a <br/>    # "coordinate-wise" RNN as the paper describes. <br/>    gradients = tf.expand_dims(gradients, axis=1)<br/> <br/>    if state is None:<br/>        state = [[tf.zeros([DIMS, STATE_SIZE])] * 2] * LAYERS<br/>    update, state = cell(gradients, state)<br/>    # Squeeze to make it a single batch again.<br/>    return tf.squeeze(update, axis=[1]), state</span></pre><p id="222a" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这就是，这就是我们的元学习者。我们可以以完全相同的方式使用它:</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="c06d" class="kx ky hu kt b fv kz la l lb lc">rnn_losses = learn(g_rnn)<br/>sum_losses = tf.reduce_sum(rnn_losses)</span></pre><blockquote class="mh mi mj"><p id="917f" class="jj jk kg jl b jm jn iv jo jp jq iy jr mk jt ju jv ml jx jy jz mm kb kc kd ke hn dt translated">神奇的是，我们希望<em class="hu">的sum_losses </em>越低，因为损失越低，优化器就越好，对吗？因为整个训练循环都在图中，所以我们可以使用时间反向传播(BPTT)和元优化器来最小化这个值！</p></blockquote><p id="0f79" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这是要点:<em class="kg">sum _ loss是可微的</em>，梯度流过我们定义的图形很好！TensorFlow能够计算出LSTM中参数相对于损失总和的梯度。让我们优化一下:</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="11ab" class="kx ky hu kt b fv kz la l lb lc">def optimize(loss):<br/>    optimizer = tf.train.AdamOptimizer(0.0001)<br/>    gradients, v = zip(*optimizer.compute_gradients(loss))<br/>    gradients, _ = tf.clip_by_global_norm(gradients, 1.)<br/>    return optimizer.apply_gradients(zip(gradients, v))</span><span id="09f9" class="kx ky hu kt b fv ld la l lb lc">apply_update = optimize(sum_losses)</span></pre><p id="8c8c" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我发现梯度裁剪在这里非常关键，因为在训练开始时，进入元学习者的值可能非常大。</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="fb29" class="kx ky hu kt b fv kz la l lb lc">sess.run(tf.global_variables_initializer())</span><span id="24df" class="kx ky hu kt b fv ld la l lb lc">ave = 0<br/>for i in range(3000):<br/>    err, _ = sess.run([sum_losses, apply_update])<br/>    ave += err<br/>    if i % 1000 == 0:<br/>        print(ave / 1000 if i!=0 else ave)<br/>        ave = 0<br/>print(ave / 1000)</span><span id="2be9" class="kx ky hu kt b fv ld la l lb lc">&gt; 223.577606201<br/>&gt; 15.9170453466<br/>&gt; 4.06150362206<br/>&gt; 3.94412120444</span></pre><p id="7a3a" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">看看它是如何做到的:</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="5fc0" class="kx ky hu kt b fv kz la l lb lc">for _ in range(3): <br/>    sgd_l, rms_l, rnn_l = sess.run(<br/>        [sgd_losses, rms_losses, rnn_losses])<br/>    p1, = plt.plot(x, sgd_l, label='SGD')<br/>    p2, = plt.plot(x, rms_l, label='RMS')<br/>    p3, = plt.plot(x, rnn_l, label='RNN')<br/>    plt.legend(handles=[p1, p2, p3])<br/>    plt.title('Losses')<br/>    plt.show()</span></pre><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff li"><img src="../Images/c9c8058f984332c3746b61a05d882309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*k8fB4Z5-cVrBNnkxt7UAeA.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff li"><img src="../Images/965b5be52a78fe49bc4efcd258965106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ZSoxtaIQa24h4ZaaX0FK_g.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff le"><img src="../Images/ed1260d179c9f79dd2b4e5b66a397cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*AWwEXN3ymWvD9jMHZavxgw.png"/></div></figure><p id="f5a3" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">成功！看起来在这个问题上它甚至比RMS做得更好。实际上，这些图表稍有误导，对数标度显示的东西略有不同:</p><pre class="ko kp kq kr fq ks kt ku kv aw kw dt"><span id="6f06" class="kx ky hu kt b fv kz la l lb lc">for _ in range(3): <br/>    sgd_l, rms_l, rnn_l = sess.run(<br/>        [sgd_losses, rms_losses, rnn_losses])<br/>    p1, = plt.semilogy(x, sgd_l, label='SGD')<br/>    p2, = plt.semilogy(x, rms_l, label='RMS')<br/>    p3, = plt.semilogy(x, rnn_l, label='RNN')<br/>    plt.legend(handles=[p1, p2, p3])<br/>    plt.title('Losses')<br/>    plt.show()</span></pre><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/72d288a261b95ad9cc22ca67126423f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*DRfhGLwSEaywl55sxrYUKg.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/af874a0b7180641530e2931d5b0ff671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*iJTXBnEWmuWog5qGQJ57Ng.png"/></div></figure><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/49289e4d3263c51cc432e93fc88e142f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*6Fk3_X2eHMBmldWCZHIlzQ.png"/></div></figure><p id="f9b3" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我认为，正如论文中所讨论的那样，这是因为输入LSTM的值的大小可能会有很大的变化，当这种情况发生时，神经网络通常不会很好地执行。这里的梯度变得如此之小，以至于它无法计算出合理的更新。对于更大的实验，论文使用了一个解决方案；改为输入对数梯度和方向。详见论文。</p><p id="6aa0" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">希望，现在你明白了如何通过梯度下降来学习，你可以看到它的局限性。好像扩展性不是很好。我认为论文中的实验规模很小就很能说明问题。即使是我们的玩具问题也需要4000步才能收敛，我们必须完全训练一个网络，只是为了元学习者的一步优化。我们将不得不多次优化一个大问题，这将花费很长时间。此外，在图中展开整个训练循环对于更大的问题是不可行的，尽管在论文中他们仅将BPTT展开到20步。论文中还有证据表明，RNN优化器可以从较小的问题推广到较大的问题。</p><p id="c54f" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了简单起见，我省略了许多细节，所以阅读这篇文章肯定是值得的。我希望元学习变得越来越重要，为了获得更多的灵感，我建议观看这些NIPS演示。</p><div class="ko kp kq kr fq ab cb"><figure class="mo lf mp mq mr ms mt paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mo lf mp mq mr ms mt paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mo lf mp mq mr ms mt paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="mh mi mj"><p id="f922" class="jj jk kg jl b jm jn iv jo jp jq iy jr mk jt ju jv ml jx jy jz mm kb kc kd ke hn dt translated"><a class="ae kf" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae kf" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae kf" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae kf" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jj jk kg jl b jm jn iv jo jp jq iy jr mk jt ju jv ml jx jy jz mm kb kc kd ke hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae kf" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae kf" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="ko kp kq kr fq lf fe ff paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="fe ff mu"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="ko kp kq kr fq lf"><div class="bz el l di"><div class="mz na l"/></div></figure></div></div>    
</body>
</html>