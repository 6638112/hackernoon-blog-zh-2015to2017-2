<html>
<head>
<title>One simple trick to train Keras model faster with Batch Normalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的技巧来训练Keras模型更快与批处理规范化</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/one-simple-trick-to-train-keras-model-faster-with-batch-normalization-baa7787bf923?source=collection_archive---------17-----------------------#2017-12-10">https://medium.com/hackernoon/one-simple-trick-to-train-keras-model-faster-with-batch-normalization-baa7787bf923?source=collection_archive---------17-----------------------#2017-12-10</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/2e88f00239882fe1d14cea3642169a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*X4kvt3x7ZlZonVkK.jpg"/></div></figure><p id="47d9" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">【www.dlology.com】最初发表于<a class="ae jx" href="https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/" rel="noopener ugc nofollow" target="_blank"><em class="jw"/></a><em class="jw">。</em></p><p id="7b68" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><strong class="ja hv">免责声明</strong>:批量归一化确实是一种帮助训练更快的优化，所以你不应该把它当成是让你的网络变得更好的一种方式。</p><h1 id="a3c0" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">什么是批量正常化？</h1><p id="0530" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">论文中首先介绍:<a class="ae jx" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">通过减少内部协变量移位来加速深度网络训练</a>。</p><p id="52fb" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">随着数据流经深度网络，权重和参数会调整这些值，有时会使数据再次变得过大或过小——作者将这一问题称为“内部协变量移位”。通过标准化每个小批量的数据，这个问题在很大程度上得以避免。批次标准化通过均值和方差参考对每个批次进行标准化。</p><h1 id="7673" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">批量标准化的好处</h1><ul class=""><li id="b85c" class="lb lc hu ja b jb kw jf kx jj ld jn le jr lf jv lg lh li lj dt translated">网络训练越快，收敛越快，</li><li id="352c" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">允许更高的学习率。梯度下降通常需要较小的学习速率来使网络收敛。</li><li id="b6e3" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">使权重更容易初始化</li><li id="e8fa" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">使更多的激活功能可行。因为批量标准化调节进入每个激活函数的值，在深度网络中似乎不太适用的非线性实际上又变得可行了。</li><li id="c079" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">可能会产生更好的整体效果。</li><li id="f8df" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">批量标准化照常使用权重，但是<strong class="ja hv">而不是</strong>增加了偏差项。这是因为它的计算包括伽玛和贝塔变量，使得偏差项变得不必要。在Keras，你可以做<code class="eh lp lq lr ls b">Dense(64, use_bias=False)</code>或<code class="eh lp lq lr ls b">Conv2D(32, (3, 3), use_bias=False)</code></li><li id="0a0c" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">我们在调用激活函数之前添加了规范化。</li></ul><h1 id="3f87" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">启用了批量规范化的Keras模型</h1><p id="d015" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">一个正常的<strong class="ja hv">密集的</strong>全连接层看起来像这样</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="87a5" class="mb jz hu ls b fv mc md l me mf">model.add(layers.Dense(64, activation='relu'))</span></pre><p id="3dda" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">要使其批量标准化启用，我们必须告诉密集层不使用偏差，因为它不是必需的，它可以节省一些计算。同样，把激活层放在<code class="eh lp lq lr ls b">BatchNormalization()</code>层之后</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="a540" class="mb jz hu ls b fv mc md l me mf">model.add(layers.Dense(64, use_bias=False)) model.add(layers.BatchNormalization()) model.add(Activation("relu"))</span></pre><h1 id="02dd" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">Conv2D层</h1><p id="6973" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">正常的Keras <strong class="ja hv"> Conv2D </strong>层可以定义为</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="6ed0" class="mb jz hu ls b fv mc md l me mf">model.add(layers.Conv2D(64, (3, 3), activation='relu'))</span></pre><p id="c5bf" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">将其转到批量标准化的Conv2D层，我们添加了类似于上面的密集层的<code class="eh lp lq lr ls b">BatchNormalization()</code>层</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="4fb5" class="mb jz hu ls b fv mc md l me mf">model.add(layers.Conv2D(64, (3, 3), use_bias=False)) model.add(layers.BatchNormalization()) model.add(layers.Activation("relu"))</span></pre><h1 id="b8cd" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">比较培训绩效</h1><p id="c329" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">通常，模型需要足够复杂，以便训练可以从批量标准化中获得明显的好处。</p><p id="8dd7" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">出于演示的目的，我们选择MNIST手写数字数据集，因为</p><ul class=""><li id="6164" class="lb lc hu ja b jb jc jf jg jj mg jn mh jr mi jv lg lh li lj dt translated">数据集附带Keras，因此不需要额外下载</li><li id="ab3e" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">它的训练速度相对较快</li><li id="fc81" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">模型架构很容易理解</li></ul><p id="c47c" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">这是一个简单的模型结构，具有3个堆叠的Conv2D层，用于从手写数字图像中提取特征。将数据从3维扁平化为1维，然后进行两次密集分层，生成最终的分类结果。</p><p id="38ef" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">我们将对所有<strong class="ja hv">致密</strong>和<strong class="ja hv"> Conv2D </strong>层进行批量归一化，并将结果与原始模型进行比较。</p><figure class="lt lu lv lw fq iv fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/d2ce710239df9be323122dfe2b97c1c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/0*0Apu1AjLBag1LVDi.png"/></div></figure><p id="98cf" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">我们用不同的参数训练模型，并并排比较两个模型</p><ul class=""><li id="eec0" class="lb lc hu ja b jb jc jf jg jj mg jn mh jr mi jv lg lh li lj dt translated">学习率(高或低)</li><li id="03c5" class="lb lc hu ja b jb lk jf ll jj lm jn ln jr lo jv lg lh li lj dt translated">激活(relu或sigmoid)</li></ul><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="2305" class="mb jz hu ls b fv mc md l me mf">train_and_test(learning_rate=0.001, activation='sigmoid', epochs=3, steps_per_epoch=1875)</span></pre><figure class="lt lu lv lw fq iv fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/9b6a6d9beddc461eda27b9158b9940d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*W5QYDQOUvNV5Pfxl.png"/></div></figure><p id="bf10" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如我们所看到的，使用批次归一化的模型的验证准确度曲线略高于未使用批次归一化的原始模型。</p><p id="2710" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">让我们用10倍的学习率来训练这两个模型，</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="8878" class="mb jz hu ls b fv mc md l me mf">train_and_test(learning_rate=0.01, activation='sigmoid', epochs=3, steps_per_epoch=1875)</span></pre><figure class="lt lu lv lw fq iv fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/ba45cabeae5cf2c9779af40a7c3e807b.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/0*_vHM8lFsHcbGcjxf.png"/></div></figure><p id="68d8" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">没有批量归一化的原始模型根本无法以这种学习速率进行学习。</p><p id="8fbc" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">如果我们使用非线性<strong class="ja hv"> relu </strong>激活函数来代替相同的x10学习速率，</p><pre class="lt lu lv lw fq lx ls ly lz aw ma dt"><span id="cc66" class="mb jz hu ls b fv mc md l me mf">train_and_test(learning_rate=0.01, activation='relu', epochs=3, steps_per_epoch=1875)</span></pre><figure class="lt lu lv lw fq iv fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/c0669487cd5051d955479e75aab6860f.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*sTJcNj3x0Lkr2GjO.png"/></div></figure><p id="a590" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">最后，未经批量归一化的原始模型能够进行训练，而经过批量归一化的模型在训练过程中具有更高的验证精度。</p><h1 id="7ef6" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">摘要</h1><p id="7c58" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">这篇文章展示了对现有的Keras模型应用批处理规范化是多么容易，并展示了一些训练结果，比较了有和没有批处理规范化的两个模型。值得注意的是，批量标准化在相对较大的学习速率下工作良好。</p><p id="3463" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">最后一点，批量标准化处理训练和测试是不同的，但是它在Keras中是自动处理的，所以你不必担心。</p><p id="5fda" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在我的<a class="ae jx" href="https://github.com/Tony607/BatchNormalization_Keras" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上查看这篇文章的源代码。</p><h1 id="2c91" class="jy jz hu bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dt translated">进一步阅读</h1><p id="d8aa" class="pw-post-body-paragraph iy iz hu ja b jb kw jd je jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv hn dt translated">论文<a class="ae jx" href="https://arxiv.org/abs/1603.09025" rel="noopener ugc nofollow" target="_blank">递归批量归一化</a></p><p id="30d6" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><a class="ae jx" href="https://keras.io/layers/normalization/" rel="noopener ugc nofollow" target="_blank">批量标准化Keras文档</a></p><p id="6b68" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><a class="ae jx" href="https://github.com/udacity/deep-learning/tree/master/batch-norm" rel="noopener ugc nofollow" target="_blank">用于Tensorflow演示</a> —向您展示培训和测试的区别</p><p id="c805" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><a class="ae jx" href="https://r2rt.com/implementing-batch-normalization-in-tensorflow.html" rel="noopener ugc nofollow" target="_blank">在Tensorflow中实现批量归一化</a></p></div><div class="ab cl mn mo hc mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hn ho hp hq hr"><p id="120b" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="jw">像这样更实用的深度学习教程，查看</em><a class="ae jx" href="https://www.dlology.com/blog/how-to-leverage-tensorflows-tfrecord-to-train-keras-model/" rel="noopener ugc nofollow" target="_blank"><em class="jw">www.dlology.com</em></a><em class="jw">。</em></p></div></div>    
</body>
</html>