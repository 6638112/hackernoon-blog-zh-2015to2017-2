# 如何登录 Apache Spark，一种函数式方法

> 原文：<https://medium.com/hackernoon/how-to-log-in-apache-spark-a-functional-approach-e48ffbbd935b>

登录 Apache Spark 非常容易，因为 Spark 提供了对开箱即用的 *log* 对象的访问。只需要完成一些配置设置。在上一篇[](/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw)**中，我们已经看到了如何做到这一点，同时也展示了一些可能出现的问题。然而，当我们想要收集日志时，所提出的解决方案可能会导致一些问题，因为它们分布在整个集群中。即使我们利用 **Yarn** 日志聚合功能，也会有一些可能影响性能的争用，甚至更糟，在某些情况下，我们可能会以日志交错破坏日志本身的性质而告终，它们应该呈现时间排序属性。**

**为了解决这些问题，需要采取一种不同的方法，一种功能性的方法。**

## **单子作者**

**我不打算讨论关于单子的细节，或者在这种特殊情况下，单子作者，如果你有兴趣了解更多，看看这个链接( [***函子，应用，和单子***](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) ),它提供了关于这个主题的很多信息。**

**为了便于理解，我们假设 monad writer(***【writer】***)是一个容器，除了保存值的历史记录(log)之外，还保存计算的当前值(值的转换集)。**

**因为 ***writer*** 的单子属性，它允许我们做函数转换，我们很快就会看到一切是如何粘在一起的。**

## **简单的日志**

**下面的代码演示了一个简单的日志。**

**唯一需要注意的是，日志记录实际上发生在 Spark 驱动程序上，所以我们没有同步或争用问题。一旦我们开始分配计算，一切都开始变得复杂。**

**以下代码行不通(看 [***上一篇***](/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw) 就知道为什么了)**

**在 [***之前的文章***](/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw) 中也提出了一个解决方案，但是需要额外的工作来管理日志。**

**一旦我们开始登录集群的每个节点，我们需要转到每个节点，收集每个日志文件，以便理解日志中的内容。希望您正在使用某种工具来帮助您完成这项任务，例如 Splunk、Datalog 等，但是您仍然需要了解很多东西才能将这些日志输入到您的系统中。**

## **我们的数据集**

**我们的数据集是 Person 类的集合，它将被转换，同时保持对数据集操作的统一日志。**

**假设我们希望加载数据集，然后过滤每个年龄小于 20 岁的人，最后提取其姓名。这是一个非常愚蠢的例子，但它将演示日志是如何产生的。您可以替换这些计算，但是构建统一日志的想法将会保留。**

## **获取编写器**

**我们将使用[*type level/Cats*](http://typelevel.org/projects/)*库来导入 monad writer，为此我们将下面一行添加到我们的 ***build.sbt*** 文件中。***

## **玩弄我们的数据**

**现在，让我们定义将要使用的转换。**

**首先，让我们加载数据。**

**这里的 ***~ >*** 操作通过隐式转换定义如下。**

**如果你仔细观察，我们的加载操作不是返回一个 RDD，事实上，它返回的是跟踪日志的 monad writer。**

**让我们定义要应用于用户集合的过滤器。**

**同样，我们应用相同的函数( **~ >** )来跟踪这个转换。**

**最后，我们定义映射，它遵循我们刚刚看到的相同模式。**

## **把它放在一起**

**到目前为止，我们只定义了我们的转换，但是我们需要把它们粘在一起。Scala ***for*** 是一种非常方便的处理一元结构的方式。让我们看看怎么做。**

**请注意，**结果**是类型: ***作家【列表【字符串】，RDD【字符串】*** 。**

**调用 **result.run** 会给我们**log:List【String】**和最终计算用**rdd**:***RDD【String】***表示。**

**此时，我们可以使用 *Spark logger* 来写下由转换链生成的日志。请注意，该操作将在 Spark master 上执行，这意味着将创建一个包含所有日志信息的日志文件。此外，我们正在消除日志写入期间潜在的争用问题。此外，我们没有锁定日志文件，这通过以串行方式创建和写入文件来避免性能问题。**

## **结论**

**我们已经通过使用 *Monad Writer* 改进了我们登录 Apache Spark 的方式。这种函数式方法允许我们将日志的创建与我们的计算一起分发，这是 Spark 非常熟悉的。但是，我们没有将日志写在每个 worker 节点上，而是将它们收集回主节点上进行记录。这种机制比我们以前的实现有一定的优势。现在，我们可以精确控制日志的写入方式和写入时间，通过消除工作节点上的 IO 操作来提高性能，通过以串行方式写入日志来消除同步问题，并且避免了在整个集群中记录日志的风险。**

> **[黑客中午](http://bit.ly/Hackernoon)是黑客如何开始他们的下午。我们是 [@AMI](http://bit.ly/atAMIatAMI) 家庭的一员。我们现在[接受投稿](http://bit.ly/hackernoonsubmission)并乐意[讨论广告&赞助](mailto:partners@amipublications.com)机会。**
> 
> **要了解更多信息，[请阅读我们的“关于”页面](https://goo.gl/4ofytp)，[在脸书上点赞/给我们发消息](http://bit.ly/HackernoonFB)，或者简单地说， [tweet/DM @HackerNoon。](https://goo.gl/k7XYbx)**
> 
> **如果你喜欢这个故事，我们推荐你阅读我们的[最新科技故事](http://bit.ly/hackernoonlatestt)和[趋势科技故事](https://hackernoon.com/trending)。直到下一次，不要把世界的现实想当然！**