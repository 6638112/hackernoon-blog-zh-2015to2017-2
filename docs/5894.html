<html>
<head>
<title>Making the Web More Accessible With AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">借助人工智能让网络更容易访问</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/making-the-web-more-accessible-with-ai-1fb2ed6ea2a4?source=collection_archive---------13-----------------------#2017-08-22">https://medium.com/hackernoon/making-the-web-more-accessible-with-ai-1fb2ed6ea2a4?source=collection_archive---------13-----------------------#2017-08-22</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/cf2fcfe68d9ea51dbb154a21670b700a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxCB95q9jaqKSqMw96FWqA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Person reading Braille stock image (<a class="ae jg" href="http://usabilitygeek.com/wp-content/uploads/2012/07/Software-For-Visually-Impaired-Blind-Users.jpg" rel="noopener ugc nofollow" target="_blank">src</a>)</figcaption></figure><p id="6c2c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><a class="ae jg" href="http://www.who.int/mediacentre/factsheets/fs282/en/" rel="noopener ugc nofollow" target="_blank">根据世界卫生组织</a>统计，全球约有2.85亿人存在视力障碍，仅在美国就有810万互联网用户存在视力障碍。</p><p id="f380" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">大多数非残疾人认为互联网是一个充满文本、图像、视频等的地方，但对视力障碍者来说却是完全不同的东西。屏幕阅读器是能够读取网页上的文本和元数据的工具，它非常有限，只能显示网页的一部分，即网站的文本。虽然一些开发人员会花时间浏览他们的网站，并为视觉障碍用户的图像添加描述性的标题，但绝大多数程序员并没有花时间去做这项公认的乏味任务。</p><p id="a8cf" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">所以，我决定借助<a class="ae jg" href="https://hackernoon.com/tagged/ai" rel="noopener ugc nofollow" target="_blank"> AI </a>的力量，做一个工具来帮助视力障碍者“看”互联网。它被称为自动替换文本，是一个chrome扩展，允许用户右键单击并获取图像中场景的描述，这是第一次这样做。</p><p id="27f5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">查看下面的视频，了解它是如何工作的,<a class="ae jg" href="http://abhinavsuri.com/aat" rel="noopener ugc nofollow" target="_blank">下载并试用</a>！</p><figure class="kf kg kh ki fq iv"><div class="bz el l di"><div class="kj kk l"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Demo of the chrome extension in action</figcaption></figure><h2 id="de71" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">我为什么制作自动替换文本:</h2><p id="6f83" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">我曾经是那些不花时间在我的页面上添加图片描述的开发者之一。对我来说，可访问性一直是我的第二个想法，直到我收到一封来自我的一个项目的用户的电子邮件。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ll"><img src="../Images/ac84a7b7482f8bbf09f9b995ce9d05df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYx_pi9vAI17mQ20D81ykw.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Email Text: “Hi Abhinav, I found your flask-base project and think it is definitely going to be a great fit for my next project. Thanks for working on it. I also just wanted to let you know that you should put some alt descriptions on your readme images. I’m legally blind and had a tough time making out what was in them :/ From REDACTED”</figcaption></figure><p id="1711" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在这一点上，我的开发过程将可访问性放在了列表的底部，基本上是事后的想法。然而，这封<a class="ae jg" href="https://hackernoon.com/tagged/email" rel="noopener ugc nofollow" target="_blank">电子邮件</a>对我来说是一个警钟。互联网上有许多人需要可访问性功能来理解网站、应用程序、项目等的初衷。</p><blockquote class="lm ln lo"><p id="6780" class="jh ji lp jj b jk jl jm jn jo jp jq jr lq jt ju jv lr jx jy jz ls kb kc kd ke hn dt translated">“网络上充斥着缺失、不正确或糟糕的替代文本的图像”——WebAIM(犹他州州立大学残疾人中心)</p></blockquote><h2 id="4e7a" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">人工智能拯救:</h2><p id="d006" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">有很多种方法为图像添加字幕；然而，大多数都有一些共同的缺点:</p><ol class=""><li id="deec" class="lt lu hu jj b jk jl jo jp js lv jw lw ka lx ke ly lz ma mb dt translated">他们没有反应，需要很长时间来回复字幕。</li><li id="b63c" class="lt lu hu jj b jk mc jo md js me jw mf ka mg ke ly lz ma mb dt translated">它们是半自动的(即依靠人工根据需要手动为图像添加字幕)。</li><li id="87cd" class="lt lu hu jj b jk mc jo md js me jw mf ka mg ke ly lz ma mb dt translated">它们的创建和维护成本很高。</li></ol><p id="e341" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">通过创建神经网络，所有这些问题都可以解决。我最近开始深入研究机器学习和人工智能，这时我遇到了Tensorflow，这是一个帮助机器学习的开源库。Tensorflow使开发人员能够构建健壮的模型，这些模型可用于完成从对象检测到图像识别的各种任务。</p><p id="0600" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">做了更多的研究，我看到了Vinyals等人的一篇论文，名为“<a class="ae jg" href="https://arxiv.org/abs/1609.06647" rel="noopener ugc nofollow" target="_blank">展示和讲述:从2015年MSCOCO图像字幕挑战赛</a>中吸取的教训”。这些研究人员创建了一个深度神经网络，以语义方式描述图像内容。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mh"><img src="../Images/f68a3a6a25b93b59868031cab167c232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSvmjcvUbpgB3izigcEi4w.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Examples of im2txt in action from the <a class="ae jg" href="https://github.com/tensorflow/models/tree/master/im2txt" rel="noopener ugc nofollow" target="_blank">im2txt Github Repository</a></figcaption></figure><h2 id="9ff2" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">im2txt的技术细节:</h2><p id="0e26" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">该模型的机制相当详细，但基本上是一个“编码器-解码器”方案。首先，图像通过一个名为Inception v3的深度卷积神经网络，这是一个图像分类器。接下来，编码后的图像通过LSTM，这是一种专门用于模拟序列/时间敏感信息的神经网络。然后，LSTM通过一组词汇，构建一个句子来描述图像。它是这样做的:从词汇表中取每个单词在句子中第一个出现的可能性，然后根据第一个单词的概率分布计算最可能的第二个单词的概率分布，依此类推，直到最可能的字符是“.”指示标题的结尾。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mi"><img src="../Images/444e1d9b94f0696dd1fc378eeaab249f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CW6YVV_zEriaGrxMzN4quA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Overview of the structure of the neural network (from the <a class="ae jg" href="https://github.com/tensorflow/models/tree/master/im2txt" rel="noopener ugc nofollow" target="_blank">im2txt Github repository</a>)</figcaption></figure><p id="d975" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">根据Github知识库，在Tesla k20m GPU上，这个神经网络的训练时间大约为1-2周(对于我所拥有的笔记本电脑上的标准CPU来说，时间可能要长得多)。幸运的是，tensorflow社区的一名成员提供了一个经过训练的模型供公众下载。</p><h2 id="9209" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">开箱即用的问题+ Lamdba:</h2><p id="60ff" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">在运行模型时，我设法让它与Bazel一起工作，Bazel是一个用于将tensorflow模型预打包到可运行脚本中的工具(除了其他目的之外)。然而，在命令行上运行时，我花了将近15秒的时间才从单个图像中得到一个结果！解决这个问题的唯一方法是将张量流图保存在内存中，但这需要保持应用程序全天候运行。我计划把这个模型放在AWS Elasticbeanstalk上，在那里计算时间是按小时分配的，让一个应用程序一直运行是不理想的(基本上导致了图像字幕软件的缺点中的第三种情况)。所以，我决定改用AWS Lamdba来托管一切。</p><p id="9092" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Lambda是一项以极低的成本提供无服务器计算的服务。此外，当它被积极使用时，它按秒计费。Lambda的工作方式很简单:一旦你的应用程序收到用户的请求，Lambda就会激活你的应用程序的图像，提供响应，并停用该图像。如果您有多个并发请求，它只是旋转更多的实例来适应负载。此外，只要一小时内有多个请求，它就会保持您的应用程序激活。这项服务非常适合我的用例。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mj"><img src="../Images/3e18e2b4d0263bfc9d08dd81f9fe323d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4EaQYos3s-67OkhhKzDkg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">AWS API Gateway + AWS = heart (<a class="ae jg" href="https://cdn-images-1.medium.com/max/700/1*SzOPXTf_YQNtFejG0e4HPg.png" rel="noopener">src</a>)</figcaption></figure><p id="d18e" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Lambda的问题是我必须为im2txt模型创建一个API。此外，Lamdba对可以作为函数加载的应用程序有内存限制。上传包含所有应用程序代码(包括依赖项)的zip文件时，最终文件不能超过250 MB。这个限制是一个问题，因为im2txt模型的大小超过了180 MB，并且它运行的依赖项超过了350 MB。我试图通过将一些部分上传到S3实例中，并在lambda实例激活时下载到我正在运行的实例中来解决这个问题；然而，lambda上的总存储大小限制是512 MB，我的应用程序已经超过了这个限制(总共大约是530 MB)。</p><p id="d13b" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了减小项目的最终规模，我重新配置了im2txt，以接受一个精简的模型，只包含经过训练的检查点，不包含无关的元数据。这一删除将我的模型大小减少到120 MB。然后我发现了<a class="ae jg" href="https://github.com/ryfeus/lambda-packs" rel="noopener ugc nofollow" target="_blank"> lambda-packs </a>，它包含了所有依赖项的最小化版本，尽管使用的是python和tensorflow的早期版本。在经历了降级任何python 3.6语法和tensorflow 1.2代码的痛苦过程后，我终于有了一个总共约480 MB的包，刚好低于512 MB的限制。</p><p id="e3b3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了保持快速响应，我创建了一个CloudWatch函数来保持Lambda实例“热”和应用程序活动。我添加了一些辅助函数来处理非JPG格式的图像，最终得到了一个可用的API。所有这些减少导致了https://github.com/abhisuri97/auto-alt-text-lambda-api&lt; 5 seconds in most cases!</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mk"><img src="../Images/7dc38edfccdbb10197607cd82c804cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5awvS8Z3k5V9qaxzMadQw.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Image with likely probabilities of what is in the image according to the API</figcaption></figure><p id="dbe4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Moreover, Lambda is incredibly cheap, at the current rate, I can analyze 60,952 images for free monthly and any additional image for $0.0001094 (meaning approx $6.67 for the next 60,952 images).</p><p id="1c05" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">More details about the API can be found at the repository: <a class="ae jg" href="https://github.com/abhisuri97/auto-alt-text-lambda-api" rel="noopener ugc nofollow" target="_blank">的极快响应时间</a></p><p id="8096" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">剩下的工作就是将它打包到chrome扩展中，以便于最终用户使用，这并不是一个太具挑战性的任务(因为它只涉及到对我的API端点的一个简单的AJAX请求)。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/4b641e448a95c90a1b0affa58e82835c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*SXf884JCTh_Ze-0XcXsxiw.gif"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Auto Alt Text Chrome Extension in action</figcaption></figure><h2 id="cc67" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">结果:</h2><p id="22e8" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">Im2txt在人物、风景等图像上表现出色。只要这些对象存在于上下文中的公共对象(COCO)数据集中。</p><figure class="kf kg kh ki fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mm"><img src="../Images/f2e6f74cbc4bf95ca6a41fbd045d8026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NE9GCZliWRPy9km6Kmaarw.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Categories of images in the COCO dataset</figcaption></figure><p id="128e" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这种模式在某种程度上限制了标题的范围；然而，它确实涵盖了脸书和Reddit等社交媒体网站上的大多数图片。</p><p id="3084" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">然而，由于COCO数据集不包含任何此类图片，因此它通常无法为包含文本的图像添加标题。我尝试使用宇宙魔方来完成这项任务；然而，结果太不准确，而且产生时间太长(超过10秒)。我目前正在Tensorflow中实现王等人的论文来帮助完成这个任务。</p><h2 id="e061" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">外卖:</h2><p id="9014" class="pw-post-body-paragraph jh ji hu jj b jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka lk kc kd ke hn dt translated">虽然每周都有关于人工智能奇迹的新故事，但重要的是退一步看看这些工具如何在研究环境之外使用，以及这些发现如何帮助世界各地的人们。总的来说，我喜欢用im2txt深入研究Tensorflow，并能够将我所学到的知识应用到现实世界的问题中。希望这个工具将是第一个帮助视障人士看到更好的互联网的工具。</p><h2 id="8ab7" class="kl km hu bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf dt translated">链接:</h2><ul class=""><li id="3eb6" class="lt lu hu jj b jk lg jo lh js mn jw mo ka mp ke mq lz ma mb dt translated">关注我:我主要在<a class="ae jg" rel="noopener" href="/@abhisuri97">我的媒体</a>上发布我正在做的事情。如果你喜欢这个帖子，我会很感激你的关注:)在接下来的几个月里，我会发布更多关于使用AI/tensorflow解决现实世界问题的“如何做”指南。在不久的将来，我也会发布一些JS解释者/教程。</li><li id="fda0" class="lt lu hu jj b jk mc jo md js me jw mf ka mg ke mq lz ma mb dt translated">链接到Chrome扩展<a class="ae jg" href="http://abhinavsuri.com/aat" rel="noopener ugc nofollow" target="_blank">下载页面</a></li><li id="bc9c" class="lt lu hu jj b jk mc jo md js me jw mf ka mg ke mq lz ma mb dt translated">链接到自动替换文本Lambda API <a class="ae jg" href="http://github.com/abhisuri97/auto-alt-text-lambda-api" rel="noopener ugc nofollow" target="_blank"> Github库</a></li></ul><figure class="kf kg kh ki fq iv"><div class="bz el l di"><div class="mr kk l"/></div></figure></div></div>    
</body>
</html>