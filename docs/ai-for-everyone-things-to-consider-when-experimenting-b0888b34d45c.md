# 面向所有人的人工智能:实验时需要考虑的事情

> 原文：<https://medium.com/hackernoon/ai-for-everyone-things-to-consider-when-experimenting-b0888b34d45c>

我最近开始合作教授 CS53SI——一系列围绕技术和社会公益的讨论。在校园的这个十字路口有很多炒作。

我们正处于人生中令人兴奋的时刻，我们拥有加速人工智能发展的硬件和软件，并看到日常技术产品中的研究成果。随着互联网上庞大的、不断增长的数据仓库的出现，像我这样的数据狂人可能会沉迷于应用机器学习技术来提取洞察力。

不幸的是，无节制的探索可能会导致一些恶劣的后果。几周前，我震惊地发现，我所在大学的一位教授提出了一种[算法，可以根据一个人的面部照片来判断他是否是同性恋](https://www.theguardian.com/technology/2017/sep/07/new-artificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph)。他声称，这种算法可以扩展到[检测政治观点和智商](http://www.businessinsider.com/stanford-professor-thinks-ai-will-soon-be-able-to-detect-your-politics-iq-sexuality-2017-9)。

这有什么可怕的？在数据集上训练模型时，需要考虑以下 4 点。

1.当这些算法被雇主用来扫描未来的员工时，会发生什么？或者如果这项技术坐在人们的后口袋里，让人们有机会通过用手机扫描某人的脸来“预测”某人是否是同性恋？这让我想起了《T4》中的 x 战警电影，在那里变种人发动战争来保护他们的同类。现实是，我们并不是生活在一个完美的世界——可悲的是，同性恋者和其他少数民族面临歧视。

2.机器学习是一种工具，主要识别数据集中现有的偏差和分隔符，并夸大这些差异以进行未来预测。如果输入数据不能代表总体数据，则输出预测将会不准确。在这种情况下，输入数据包括具有自我报告的性别的面部。唯一的训练点代表已经出柜或公开性取向的人。还有很多人没有公开表明他们的性取向，因为他们可能会面临不利的后果——偏见的输入。

3.当前的媒体夸大人工智能的研究成果，或者对事物进行全新的描述。进行这项研究的斯坦福教授谈到了他的研究带来的负面社会影响。但更多的人不理解人工智能的机制，新闻机构利用了公众对人工智能的恐惧。我们必须设计一个不威胁他人或危及他人身份的世界。

4.根据人们的长相或他们无法控制的因素对他们做出结论可能是不好的。事实上，[在 21 世纪，我们仍在与种族主义、性别歧视、同性恋恐惧症以及其他类似现象作斗争。如果人工智能让基于个人无法控制的因素的待遇差异永久化，我们将永远无法实现我们许多人正在为之奋斗的平等。](http://www.bbc.co.uk/newsbeat/article/41095623/pink-calls-for-ban-on-homophobia-racism-sexism---all-the-isms)

AI 的可能性和应用是无穷无尽的。他们有很大的潜力让 T4 为世界做好事。但是作为开发者、科学家、工程师和极客，我们必须预测和评估我们的工作对社会的影响。当我们设计未来的时候，我们不能让一部分人落在后面——我们应该致力于为每个人创造一个包容的世界。