<html>
<head>
<title>Installing Apache Hadoop in virtual-machine-based environments</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在基于虚拟机的环境中安装Apache Hadoop</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/installing-apache-hadoop-in-virtual-machine-based-environments-c6441a5dfb52?source=collection_archive---------1-----------------------#2015-09-07">https://medium.com/hackernoon/installing-apache-hadoop-in-virtual-machine-based-environments-c6441a5dfb52?source=collection_archive---------1-----------------------#2015-09-07</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/270b66b8ed59a82ff3312541458d8680.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*OMbjV9_0375Xvm-A.png"/></div></figure><p id="c6d4" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><a class="ae jw" href="https://hackernoon.com/tagged/big-data" rel="noopener ugc nofollow" target="_blank">大数据</a>不仅仅指机器学习、人工智能、统计学和<a class="ae jw" href="https://hackernoon.com/tagged/programming" rel="noopener ugc nofollow" target="_blank">编程</a>。它还指使事情发生的计算基础设施和软件框架。无论任何技术的积极和消极方面如何，Apache Hadoop都是大数据分析的艺术状态。它的几个特点之一是依靠现成的计算机，连接在一个网络环境中，配备非常少的硬件。对于Hadoop来说，不可靠、崩溃、重启和故障并不是真正的问题，前提是大量节点已经启动并运行，并且管理员已经保证通过复制实现大量冗余。</p><h1 id="1568" class="jx jy hu bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dt translated">安装Apache Hadoop。</h1><p id="9d0e" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">如果想法是最早启动Apache Hadoop，下载二进制会节省很多时间。一个好的开始是<a class="ae jw" href="https://hadoop.apache.org/#Download+Hadoop" rel="noopener ugc nofollow" target="_blank">官网</a>。将二进制文件解压缩到选择的目录中。我们正在<em class="la"> /opt/hadoop </em>中安装Hadoop</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="0201" class="lk jy hu lg b fv ll lm l ln lo">wget http://apache.belnet.be/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz sudo tar xvf hadoop-2.6.0.tar.gz -C /opt/ cd /opt sudo mv hadoop-2.6.0/ hadoop</span></pre><p id="990a" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">允许您的用户(在本例中为<em class="la"> frag </em>)访问<em class="la"> hadoop </em>目录</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="603c" class="lk jy hu lg b fv ll lm l ln lo">sudo chown -R frag hadoop</span></pre><p id="38ad" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">让你的Hadoop用户(如果有的话)知道东西在哪里。在这种情况下,<em class="la"> java </em>已经安装在JAVA_HOME env变量指示的路径下。在将更改应用到~/之前，请检查您的。bashrc</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="04ec" class="lk jy hu lg b fv ll lm l ln lo">export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export HADOOP_HOME=/usr/local/hadoop export PATH=$HADOOP_HOME/bin:$PATH</span></pre><p id="d7a2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">我在启动Hadoop时发现的一个问题是关于JAVA_HOME的。显然，还需要将JAVA_HOME添加到/opt/Hadoop/etc/Hadoop/Hadoop-env . sh，如下所示</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="b0c4" class="lk jy hu lg b fv ll lm l ln lo"># The java implementation to use. #export JAVA_HOME=${JAVA_HOME} export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</span></pre><p id="54e4" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">此时，从控制台启动的命令应该返回一些子命令的运行和使用说明。如果失败了，不要继续，因为已经有问题了。</p><h1 id="a986" class="jx jy hu bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dt translated">配置Hadoop</h1><p id="2ec3" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">现在，我们配置Hadoop将存储其数据文件、网络设置和其他一些配置选项的目录，这些配置选项在以后可能会有用。在我的例子中，<em class="la"> frag </em>是用户，我决定将本地文件系统存储在路径<em class="la"> /usr/local/hadoop/tmp </em>中，因此，我创建了这个目录</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="c80c" class="lk jy hu lg b fv ll lm l ln lo">sudo mkdir -p /usr/local/hadoop/tmp</span></pre><p id="c3e2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">并像以前一样设置权限</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="de65" class="lk jy hu lg b fv ll lm l ln lo">sudo chown frag /usr/local/hadoop/tmp/</span></pre><p id="1898" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">这样做与您的用户名和所有其他用户谁被允许访问也限制位的安全策略</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="ff4d" class="lk jy hu lg b fv ll lm l ln lo">sudo chmod 750 /usr/local/hadoop/tmp/</span></pre><p id="ef8a" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">现在最重要的部分来了，配置Hadoop平台的一些基础文件。</p><p id="6c17" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">核心网站. xml</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="e005" class="lk jy hu lg b fv ll lm l ln lo">cd /opt/hadoop/etc/hadoop/ <br/>sudo vi core-site.xml <br/>sudo cp mapred-site.xml.template mapred-site.xml <br/></span><span id="8d32" class="lk jy hu lg b fv lp lm l ln lo">&lt;configuration&gt; <br/>&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; <br/>&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; <br/>&lt;description&gt;The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem.<br/>&lt;/description&gt; <br/>&lt;/property&gt; <br/>&lt;/configuration&gt;</span></pre><p id="1d25" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">mapred-site.xml</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="dd97" class="lk jy hu lg b fv ll lm l ln lo">sudo vi mapred-site.xml </span><span id="586b" class="lk jy hu lg b fv lp lm l ln lo">&lt;configuration&gt; &lt;property&gt; <br/>&lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9001&lt;/value&gt; &lt;description&gt;The host and port that the MapReduce job tracker runs at. If "local", then jobs are run in-process as a single map and reduce task. <br/>&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;</span></pre><p id="0df6" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">hdfs-site.xml</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="0671" class="lk jy hu lg b fv ll lm l ln lo">sudo vi hdfs-site.xml </span><span id="0521" class="lk jy hu lg b fv lp lm l ln lo">&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt;Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. &lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;</span></pre><p id="0b6a" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">复制值决定了RDD数据块应该在节点间复制的次数。对于测试环境来说，2已经足够了。但是，在生产中，应该考虑更高的复制值，以提高整个集群的可靠性。</p><h2 id="51d4" class="lk jy hu bd jz lq lr ls kd lt lu lv kh jj lw lx kl jn ly lz kp jr ma mb kt mc dt translated">格式化HDFS文件系统</h2><p id="d7ee" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">通常，新的文件系统必须格式化。这也适用于HDFS。下面的命令完成了这项工作。</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="3df2" class="lk jy hu lg b fv ll lm l ln lo">$ /opt/hadoop/bin/hadoop namenode -format</span></pre><p id="85c8" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">这不应该在群集运行时进行，否则所有数据都会被突然删除。该开动机器了。从主节点</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="d037" class="lk jy hu lg b fv ll lm l ln lo">/opt/hadoop/sbin/start-dfs.sh</span></pre><p id="c0a5" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">此时，一些<em class="la"> ssh </em>连接由主节点向从节点执行(这些是伪节点，因为它们运行在同一台机器上，只是为了本教程的目的)。<em class="la"> ssh </em>将要求对DataNode、SecondaryNameNode和NameNode进行三次身份验证(顺序相反)。对于生产系统来说，这并不理想。因此，应该为将要使用Hadoop的用户提供一个<em class="la"> ssh密钥</em>。为了配置ssh 不要每次都询问密码，在属于集群的所有机器中输入这个</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="70be" class="lk jy hu lg b fv ll lm l ln lo">cat .ssh/id_rsa.pub | ssh b@B 'cat &gt;&gt; .ssh/authorized_keys2' chmod 700 .ssh chmod 640 .ssh/authorized_keys2</span></pre><h2 id="6401" class="lk jy hu bd jz lq lr ls kd lt lu lv kh jj lw lx kl jn ly lz kp jr ma mb kt mc dt translated">摆弄文件系统</h2><p id="4a08" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">为了在Hadoop中创建虚拟文件系统，应该创建一个类似于传统目录树的目录树。在本例中，我将创建<em class="la"> hadoop/data </em>路径。</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="47fc" class="lk jy hu lg b fv ll lm l ln lo">$hadoop fs -mkdir /hadoop $hadoop fs -mkdir /hadoop/data</span></pre><p id="0da8" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">事实上，hadoop根目录列表返回了我所期望的内容。</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="ac4f" class="lk jy hu lg b fv ll lm l ln lo">$ hadoop fs -ls /hadoop Found 1 items drwxr-xr-x - frag supergroup 0 2015-04-20 17:22 /hadoop/data</span></pre><p id="842e" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">好像管用！为了使本地文件在Hadoop基础架构中可见，必须将它从本地文件系统(或任何位置)复制到HDFS。在这种情况下，<em class="la"> localfile.txt </em>被复制到HDFS的根目录下，作为<em class="la"> file.txt </em>。</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="78cd" class="lk jy hu lg b fv ll lm l ln lo">$ bin/hadoop dfs -copyFromLocal ~/localfile.txt  /file.txt</span></pre><h2 id="418c" class="lk jy hu bd jz lq lr ls kd lt lu lv kh jj lw lx kl jn ly lz kp jr ma mb kt mc dt translated">检查</h2><p id="04de" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">java实用程序<em class="la"> jps </em>给出了正在运行的Hadoop进程的打印结果。它应该会返回类似这样的结果</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="1e67" class="lk jy hu lg b fv ll lm l ln lo">$ jps 28130 DataNode 28323 SecondaryNameNode 27968 NameNode 28471 Jps</span></pre><p id="af26" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">好像管用！</p><h2 id="91e4" class="lk jy hu bd jz lq lr ls kd lt lu lv kh jj lw lx kl jn ly lz kp jr ma mb kt mc dt translated">监视</h2><p id="e4e7" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">一旦Hadoop启动并运行，就可以将浏览器指向主节点(在我的例子中是<em class="la">vlab 1</em>)<a class="ae jw" href="http://vlab1:50070" rel="noopener ugc nofollow" target="_blank"><em class="la">http://vlab 1:50070</em></a>，并从主节点<em class="la"> vlab1 </em>监控所有资源和基础架构的完整状态。在<a class="ae jw" href="http://vlab1:8042" rel="noopener ugc nofollow" target="_blank"><em class="la">http://vlab 1:8042</em></a>也有一个HDFS NameNode web接口，其中<em class="la"> vlab1 </em>也是主节点的主机名。</p><p id="d2e1" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">停止Hadoop</p><p id="3538" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">为了停止集群</p><pre class="lb lc ld le fq lf lg lh li aw lj dt"><span id="bcbc" class="lk jy hu lg b fv ll lm l ln lo">/opt/hadoop/sbin/stop-dfs.sh</span></pre><p id="0e4f" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">主节点从每个从节点中停止Hadoop。但是不需要ssh密码。</p><h2 id="38e5" class="lk jy hu bd jz lq lr ls kd lt lu lv kh jj lw lx kl jn ly lz kp jr ma mb kt mc dt translated">虚拟化技术</h2><p id="1d9d" class="pw-post-body-paragraph iy iz hu ja b jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv hn dt translated">在家里，我没有成千上万的物理机器来测试这一点。不过，我确实有虚拟机，想要多少就有多少(直到计算机内存完全耗尽)。在VirtualBox中设置了一个Ubuntu虚拟机，并按照上面的说明安装和配置了所有的东西之后，我只需克隆它就可以在几分钟内获得三个正在运行的虚拟机。该是我享受我的虚拟Hadoop集群的时候了。</p><p id="2d42" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">Hadooping快乐！</p></div><div class="ab cl md me hc mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="hn ho hp hq hr"><p id="0189" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="la">原载于2015年9月7日</em><a class="ae jw" href="http://worldofpiggy.com/2015/09/07/installing-apache-hadoop-in-virtual-machine-based-environments/" rel="noopener ugc nofollow" target="_blank"><em class="la">【worldofpiggy.com】</em></a><em class="la">。</em></p><div class="lb lc ld le fq ab cb"><figure class="mk iv ml mm mn mo mp paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mk iv ml mm mn mo mp paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mk iv ml mm mn mo mp paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="mq mr ms"><p id="f922" class="iy iz la ja b jb jc jd je jf jg jh ji mt jk jl jm mu jo jp jq mv js jt ju jv hn dt translated"><a class="ae jw" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae jw" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae jw" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae jw" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="iy iz la ja b jb jc jd je jf jg jh ji mt jk jl jm mu jo jp jq mv js jt ju jv hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jw" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jw" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="lb lc ld le fq iv fe ff paragraph-image"><a href="https://goo.gl/Ahtev1"><div class="fe ff mw"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></a></figure><figure class="lb lc ld le fq iv"><div class="bz el l di"><div class="mx my l"/></div></figure></div></div>    
</body>
</html>