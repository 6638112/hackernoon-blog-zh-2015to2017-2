<html>
<head>
<title>Train your deep model faster and sharper — two novel techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更快更清晰地训练你的深度模型——两种新颖的技术</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/training-your-deep-model-faster-and-sharper-e85076c3b047?source=collection_archive---------3-----------------------#2017-06-28">https://medium.com/hackernoon/training-your-deep-model-faster-and-sharper-e85076c3b047?source=collection_archive---------3-----------------------#2017-06-28</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="c62a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt jp translated">eep神经网络有许多许多可学习的参数，用于进行推理。通常，这从两个方面提出了一个问题:有时，模型不能做出非常准确的预测。训练他们也需要很长时间。这篇文章谈到了使用两种非常新颖的方法来提高准确性，同时减少训练时间。</p><h2 id="abd8" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">编辑:</h2><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div class="fe ff kt"><img src="../Images/3d0a45a61f1c8746719a6e814594a655.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*4CWM6vRBH8SGLd5ii7zahA.jpeg"/></div></figure><blockquote class="lb lc ld"><p id="ee94" class="ir is le it b iu iv iw ix iy iz ja jb lf jd je jf lg jh ji jj lh jl jm jn jo hn dt translated">这篇文章获得了2017年8月由<a class="ae li" href="http://www.kdnuggets.com/2017/08/train-deep-learning-faster-snapshot-ensembling.html" rel="noopener ugc nofollow" target="_blank"> KDNuggets </a>颁发的第二大最受欢迎博文奖。</p></blockquote></div><div class="ab cl lj lk hc ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hn ho hp hq hr"><p id="3874" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">原文可以在<a class="ae li" href="https://arxiv.org/abs/1704.00109" rel="noopener ugc nofollow" target="_blank">这里</a> ( <strong class="it hv">快照合集</strong>)和<a class="ae li" href="https://arxiv.org/abs/1706.04983" rel="noopener ugc nofollow" target="_blank">这里</a> ( <strong class="it hv">冻结</strong>)。</p><p id="2dda" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="le">本文假设对神经网络有一些熟悉，包括类似</em><strong class="it hv"><em class="le">SGD</em></strong><em class="le">，</em><strong class="it hv"><em class="le">minima</em></strong><em class="le">，</em><strong class="it hv"><em class="le">optimization</em></strong><em class="le">等方面。</em></p></div><div class="ab cl lj lk hc ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hn ho hp hq hr"><h1 id="7e7f" class="lq jz hu bd ka lr ls lt ke lu lv lw ki lx ly lz kl ma mb mc ko md me mf kr mg dt translated">这篇文章的结构</h1><p id="ac24" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">我将谈论两份不同的论文，它们的目的是做不同的事情。<br/>注意，即使有两种不同的想法，它们也不是互斥的，可以同时使用。</p><p id="8e10" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="le">这是一篇很长的帖子，但是分为两个互相排斥的部分</em></p></div><div class="ab cl lj lk hc ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hn ho hp hq hr"><h1 id="e257" class="lq jz hu bd ka lr ls lt ke lu lv lw ki lx ly lz kl ma mb mc ko md me mf kr mg dt translated">1.<a class="ae li" href="https://arxiv.org/pdf/1704.00109.pdf" rel="noopener ugc nofollow" target="_blank">快照组装—成本为1的M个模型</a></h1><h2 id="4ddb" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">正则系综模型</h2><p id="4967" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">集合模型是一组模型，它们共同工作来获得预测。这个想法很简单:<strong class="it hv">使用不同的超参数训练几个模型</strong>，然后<strong class="it hv">对所有这些模型的预测</strong>进行平均。这项技术大大提高了准确性，因为它不依赖于单一的预测模型。大多数高调的机器学习竞赛的获奖作品都使用了合奏。</p><h2 id="74ac" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">那么问题出在哪里？</h2><p id="95d0" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">训练N个不同的模型将需要N倍于训练单个模型所需的时间。大多数没有拥有多个GPU的人通常需要等待很长时间才能测试出这些模型。因此，它使得实验慢了很多。</p><h2 id="c934" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">SGD机械</h2><p id="6a95" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">在我告诉你“新颖”的方法之前，你必须先了解随机梯度下降(SGD)的本质。SGD是贪婪的，它会寻找最陡的下降。但是，有一个非常关键的<strong class="it hv">参数</strong><strong class="it hv">支配</strong> SGD — <strong class="it hv">学习</strong><a class="ae li" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank"><strong class="it hv"/></a><strong class="it hv">速率</strong>。</p><p id="ad91" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果<strong class="it hv">学习率</strong>过高，<strong class="it hv"> SGD </strong>会<strong class="it hv">忽略</strong>非常<strong class="it hv">狭窄</strong> <strong class="it hv">缝隙</strong>(最小值)，大步走(想想路上一辆坦克不受坑洼影响)。</p><p id="c2e4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">另一方面，如果<strong class="it hv">学习率</strong>为<strong class="it hv">小</strong>，则<strong class="it hv"> SGD </strong>将<strong class="it hv">落入这些<strong class="it hv">局部最小值</strong>中的一个</strong>中，并且不能出来。然而，通过<strong class="it hv">增加</strong>学习率<strong class="it hv">，有可能<strong class="it hv">使SGD从局部最小值返回</strong>。</strong></p><h2 id="4712" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">诀窍？</h2><p id="14b6" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">论文作者利用SGD落入和爬出局部极小值的这个<strong class="it hv">可控性质</strong>。<strong class="it hv">不同的局部极小值</strong>可能有非常<strong class="it hv">相似的错误率</strong>，但是他们会犯的<strong class="it hv">错误</strong>将会<strong class="it hv">不同</strong>彼此。</p><p id="b6c6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">他们提供了一个非常有用的图表来解释这个概念:</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff mm"><img src="../Images/78125460c824899583e7353847e10d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5WWecP_EaQWk1yDX15h_w.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek"><em class="mv">Figure 1.0</em>: <strong class="bd mw">Left:</strong> standard SGD trying to find the best local minima. <strong class="bd mw">Right: </strong>SGD is made to fall into a local minima, then brought back up, and the process is repeated. This way you get 3(which are labelled 1,2,3) local minima, each with similar error rates, but with different error characteristics</figcaption></figure><h2 id="0b56" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">什么是集合，也叫快照？</h2><p id="b90e" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者利用局部极小值对其预测具有不同“观点”的特性来创建多个模型。每当SGD达到一个<strong class="it hv">局部最小值</strong>时，该模型的一个<strong class="it hv">快照</strong>被<strong class="it hv">保存</strong>，这将是最终网络集合的一部分。</p><h2 id="46d8" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated"><strong class="ak">循环余弦退火</strong></h2><p id="47d5" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者使用一个函数来自动化这个过程，而不是手动试图找出何时陷入局部最小值或何时跳出局部最小值。</p><p id="2146" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">他们使用具有以下功能的学习速率退火:</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff mx"><img src="../Images/9941931206cb5e15d0b480c369582a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XA3yX6VSeAr91WCA-FtJTQ.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek">Figure1.0</figcaption></figure><h1 id="10c1" class="lq jz hu bd ka lr my lt ke lu mz lw ki lx na lz kl ma nb mc ko md nc mf kr mg dt translated">简化了的</h1><p id="3153" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">这个公式看起来可能很复杂，但其实很简单。他们使用了一个单调递减的T2函数。<em class="le"> α </em>这里是新的学习率，α0是旧的学习率。<strong class="it hv"> T </strong>是你要使用的训练<strong class="it hv">迭代</strong>的总<strong class="it hv">数</strong>(T应该等于batchsize *历元数)。<strong class="it hv"> M </strong>是您想要在合奏中使用的<strong class="it hv">快照</strong>的<strong class="it hv">号</strong>。</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff nd"><img src="../Images/7a53bd142429c111eb8957343475acef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3NpdnSPHmfIURoLU-NuDg.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek"><strong class="bd mw"><em class="mv">Figure1.1</em></strong> M=6 , and Budget=300 epochs. The vertical dotted lines indicate a model snapshot. After 300 epochs a total of 6 models were added to the ensemble.</figcaption></figure><p id="2d48" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">请注意，在每次拍摄快照之前，丢失量是如何快速下降的。这是因为学习率<strong class="it hv">持续下降</strong><strong class="it hv"/>。快照后，学习率为<strong class="it hv">重新启动</strong>返回(他们使用的值为0.1)。这导致梯度路径被带出局部最小值(并且新的局部最小值搜索再次开始)。</p><h2 id="8782" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">给我看看数字</h2><p id="4716" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">我已经包括了作者用来证明他们方法有效性的数字</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff ne"><img src="../Images/b04d3099669b88542f35c364bdb89247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Frr5uavuG2jlzgJtBGo59w.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek"><strong class="bd mw"><em class="mv">Figure1.2</em></strong><em class="mv"> Error Rates(%) on Cifar10,Cifar100,SVHN and Tiny ImageNet. Blue indicates the authors’ work, and bold indicates the best error rate for that category</em></figcaption></figure><h2 id="08da" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">结论</h2><p id="7a0e" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">这是一个有用的策略，可以在不增加额外培训成本的情况下略微提高准确度。该论文讨论了改变不同的参数，例如M和T，以及它如何影响性能。</p></div><div class="ab cl lj lk hc ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hn ho hp hq hr"><h1 id="a9b6" class="lq jz hu bd ka lr ls lt ke lu lv lw ki lx ly lz kl ma mb mc ko md me mf kr mg dt translated"><a class="ae li" href="https://arxiv.org/pdf/1706.04983.pdf" rel="noopener ugc nofollow" target="_blank"> 2。冻结-通过逐渐冻结层来训练加速度</a></h1><p id="9456" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">本文作者提出了一种通过冻结层数来提高训练速度的方法。他们试验了几种不同的冻结层的方法，并证明了训练速度的提高对精度几乎没有(或没有)影响。</p><h2 id="653b" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">冻结一层是什么意思？</h2><p id="ca05" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">冻结层可防止其权重被修改。这种技术经常用在<strong class="it hv">迁移学习中，</strong>基础模型(在一些其他数据集上训练)被冻结。</p><h2 id="3ce1" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated"><strong class="ak">冷冻是如何影响模型的速度的？</strong></h2><p id="6158" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">如果你不想修改一个层的权重，那么到那个层的<strong class="it hv">向后传递</strong>可以被<strong class="it hv">完全避免</strong>，导致<strong class="it hv">速度显著提升</strong>。例如，如果您的模型有一半是冻结的，并且您尝试训练该模型，与完全可训练的模型相比，这将花费大约一半的时间。</p><p id="671d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">另一方面，你<strong class="it hv">还需要</strong> <strong class="it hv">来<strong class="it hv">训练</strong><strong class="it hv">的</strong>型号，所以如果你过早<strong class="it hv">冻结</strong>，就会给<strong class="it hv">不准确的</strong> <strong class="it hv">预测</strong>。</strong></p><h2 id="2d2a" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">什么是“新颖”的方法？</h2><p id="ba21" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者演示了一种<strong class="it hv">方式</strong>到<strong class="it hv">冻结</strong>一层一层<strong class="it hv">尽快</strong>，导致回传球越来越少，进而降低训练时间。</p><p id="2be1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，整个模型是可训练的(与常规模型完全一样)。经过几次迭代后，第一层被冻结，模型的其余部分继续训练。再迭代几次后，下一层冻结，依此类推。</p><h2 id="802a" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated"><strong class="ak">学习速率退火</strong></h2><p id="e641" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者使用学习率退火来控制模型的学习率。他们使用的<strong class="it hv">显著不同的技术</strong>是他们<strong class="it hv">逐层<strong class="it hv">改变</strong>学习速率</strong>而不是整个模型。他们使用了以下等式:</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff nf"><img src="../Images/52842eb47ae7e7a21296ce0383b662b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qBDzgj0ZbLEUMbssF5CVag.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek">Equation 2.0: <strong class="bd mw">α</strong> is the learning rate. <strong class="bd mw">t</strong> is the iteration number. <strong class="bd mw">i</strong> denotes the ith layer of the model</figcaption></figure><h2 id="3d80" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">等式2.0解释</h2><p id="b5c6" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">子<em class="le"> i </em>表示第I层。因此<em class="le"> α </em> sub <em class="le"> i </em>表示第I层的学习速率。类似地，<em class="le"> t sub i </em>表示第I层被训练的迭代次数。<em class="le"> t </em>表示整个模型的总迭代次数。</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/4996b4ac2b1820d3a0038b4c94b52f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*obqh_b33HU1zJdBFBWE3Rg.png"/></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek">Equation 2.1</figcaption></figure><p id="6847" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这表示第I层的初始学习速率。</p><p id="2c07" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">作者试验了方程2.1的不同值</p><h2 id="fcd3" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated">等式2.1的初始学习率</h2><p id="df49" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者尝试调整最初的学习速度，这样每一层的训练时间都是相等的。</p><p id="a431" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">请记住，因为模型的第一层将首先停止，否则它的训练时间将最少。为了解决这个问题，他们调整了每一层的学习速度。</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff nh"><img src="../Images/7096086b4362eb33cdc901190d41e21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BjuzHsXGSiWjbrXVElREw.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek">Figure2.0</figcaption></figure><p id="74de" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">进行缩放以确保所有层的权重在权重空间中相等地移动，即被训练最长的层(后面的层)具有较低的学习率。</p><p id="96b1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">作者还玩了<strong class="it hv">立方缩放</strong>，其中t sub i的值被它自己的立方代替。</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff ni"><img src="../Images/f6486d5b78bc20401fd8d64193c42cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wDiOD9Y7FGWUAcCfvbZEA.png"/></div></div><figcaption class="mr ms fg fe ff mt mu bd b be z ek">Figure2.1: Performance vs Error on DenseNet</figcaption></figure><p id="36a5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">作者包括了更多的基准，他们的方法在精度仅下降<strong class="it hv">3%</strong><strong class="it hv"/>的情况下增加了大约<strong class="it hv"> 20% </strong>的训练加速，在精度没有下降的情况下增加了<strong class="it hv"> 15% </strong>。</p><p id="eb41" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">他们的方法对于不使用跳跃连接的模型(如VGG-16)效果不是很好。在这样的网络中，无论是精度还是加速都没有明显的不同。</p></div><div class="ab cl lj lk hc ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hn ho hp hq hr"><h1 id="cfbb" class="lq jz hu bd ka lr ls lt ke lu lv lw ki lx ly lz kl ma mb mc ko md me mf kr mg dt translated">我的奖励戏法</h1><p id="02e7" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">作者逐渐停止训练每一层，然后他们不计算向后传递。他们似乎已经<strong class="it hv">错过了</strong>来利用<strong class="it hv">预计算</strong> <strong class="it hv">图层</strong> <strong class="it hv">激活</strong>。这样做，你甚至可以防止<strong class="it hv">计算</strong>向前传球。</p><h2 id="07c3" class="jy jz hu bd ka kb kc kd ke kf kg kh ki jc kj kk kl jg km kn ko jk kp kq kr ks dt translated"><strong class="ak">什么是预计算</strong></h2><p id="6925" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">这是迁移学习中使用的一个技巧。这是一般的工作流程。</p><ol class=""><li id="a188" class="nj nk hu it b iu iv iy iz jc nl jg nm jk nn jo no np nq nr dt translated">冻结不想修改的图层</li><li id="2ccb" class="nj nk hu it b iu ns iy nt jc nu jg nv jk nw jo no np nq nr dt translated">从冻结层中计算最后一层的激活(针对整个数据集)</li><li id="eab0" class="nj nk hu it b iu ns iy nt jc nu jg nv jk nw jo no np nq nr dt translated">将这些激活保存到磁盘</li><li id="154d" class="nj nk hu it b iu ns iy nt jc nu jg nv jk nw jo no np nq nr dt translated">使用这些激活作为可训练层的输入</li></ol><p id="1435" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">由于各层被逐步冻结，新模型现在可以被视为一个独立的模型(一个较小的模型)，它只接受最后一层输出的输入。当每一层都冻结时，这可以反复进行。</p><p id="5443" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">与<strong class="it hv">冻结</strong>一起做这件事将导致训练时间的进一步大幅减少，而不会以任何方式影响其他指标(如准确性)。</p><h1 id="adce" class="lq jz hu bd ka lr my lt ke lu mz lw ki lx na lz kl ma nb mc ko md nc mf kr mg dt translated">结论</h1><p id="97d2" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">我演示了两个(和我自己的一半)非常新的技术，通过微调学习率来提高准确性和减少训练时间。通过尽可能地添加预先计算，使用我自己提出的方法可以实现显著的速度提升。</p><h1 id="a414" class="lq jz hu bd ka lr my lt ke lu mz lw ki lx na lz kl ma nb mc ko md nc mf kr mg dt translated">附注(也代表请分享)</h1><p id="9909" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">如果您注意到任何错误或有任何疑问，请评论它们。我会更新我的帖子或尝试解释得更好。</p><p id="8a9d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">另外，如果你喜欢我的文章，请点击❤.推荐它它让我知道我对你有帮助。</p><div class="ku kv kw kx fq ab cb"><figure class="nx ky ny nz oa ob oc paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="nx ky ny nz oa ob oc paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="nx ky ny nz oa ob oc paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="lb lc ld"><p id="f922" class="ir is le it b iu iv iw ix iy iz ja jb lf jd je jf lg jh ji jj lh jl jm jn jo hn dt translated"><a class="ae li" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae li" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae li" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae li" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="ir is le it b iu iv iw ix iy iz ja jb lf jd je jf lg jh ji jj lh jl jm jn jo hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae li" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae li" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff od"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="ku kv kw kx fq ky"><div class="bz el l di"><div class="oe of l"/></div></figure></div></div>    
</body>
</html>