<html>
<head>
<title>Gender and Race Change on Your Selfie with Neural Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用神经网络改变你自拍中的性别和种族</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/gender-and-race-change-on-your-selfie-with-neural-nets-9a9a1c9c5c16?source=collection_archive---------2-----------------------#2017-11-03">https://medium.com/hackernoon/gender-and-race-change-on-your-selfie-with-neural-nets-9a9a1c9c5c16?source=collection_archive---------2-----------------------#2017-11-03</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/42db0532aa2e351d2fdc3df201dcb15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Qj98nzyumsLMmVQ4NSUQQ.png"/></div></div></figure><p id="910f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">今天，我将告诉你如何使用带有几个生成神经网络(GANs)的复杂管道来改变照片上的脸。你大概见过一堆把你的自拍转换成女性或者老人的热门应用。他们没有一直使用深度学习，因为两个主要问题:</p><ul class=""><li id="e590" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki dt translated">GAN处理仍然繁重而缓慢</li><li id="8482" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">经典CV方法的质量足以满足生产水平</li></ul><p id="40a4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但是，无论如何，建议的方法有一些潜力，下面描述的工作证明了GANs适用于这类任务的概念。</p><p id="102e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">转换照片的管道可能如下所示:</p><ol class=""><li id="39fd" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz ko kg kh ki dt translated">从输入图像中检测和提取人脸</li><li id="ffd3" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz ko kg kh ki dt translated">以期望的方式变换提取的面部(转换成女性、亚洲人等。)</li><li id="4b13" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz ko kg kh ki dt translated">高档/增强改造脸</li><li id="5425" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz ko kg kh ki dt translated">将变换后的人脸粘贴回原始图像</li></ol><p id="29df" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这些步骤中的每一步都可以用单独的神经网络来解决，也可以不解决。让我们一步一步地走过这条管道。</p><h1 id="1580" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">人脸检测</h1><p id="a90f" class="pw-post-body-paragraph jc jd hu je b jf ln jh ji jj lo jl jm jn lp jp jq jr lq jt ju jv lr jx jy jz hn dt translated">这是最简单的部分。你可以简单地使用类似<code class="eh ls lt lu lv b">dlib.get_frontal_face_detector()</code> ( <a class="ae lw" href="http://dlib.net/face_detector.py.html" rel="noopener ugc nofollow" target="_blank">例子</a>)的东西。dlib提供的默认人脸检测器对HOG特征使用线性分类。如下例所示，生成的矩形不能适合整张脸，所以最好在每个维度上扩展矩形一些因子。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/66f4460d73ebfe0677c90ea129214138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*Qku2ySacI2O6RRo7z3VVgA.png"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">Detected face with dlib’s default detector</figcaption></figure><p id="e276" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">通过手动调整这些因素，您可能会得到以下代码:</p><figure class="ly lz ma mb fq iv"><div class="bz el l di"><div class="mg mh l"/></div></figure><p id="30fb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">结果如下:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/2daf4491c7500f182617055279545b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*RnWaU4cJm4CaSsSdQwCetg.png"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">Extended face rectangle</figcaption></figure><p id="bc74" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果出于任何原因，你对这种老派方法的表现不满意，你可以尝试SOTA深度学习技术。任何对象检测架构(例如<a class="ae lw" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank"> Faster-RCNN </a>或<a class="ae lw" href="https://arxiv.org/abs/1612.08242" rel="noopener ugc nofollow" target="_blank"> YOLOv2 </a>)都可以轻松处理这项任务。</p><h1 id="c251" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">人脸变换</h1><p id="35f9" class="pw-post-body-paragraph jc jd hu je b jf ln jh ji jj lo jl jm jn lp jp jq jr lq jt ju jv lr jx jy jz hn dt translated">这是最有趣的部分。你可能知道，GANs非常擅长生成和转换图像。而且有很多型号的命名像<em class="mi"> &lt;前缀你自己选&gt; </em>甘。将图像从一个子集(域)变换到另一个子集(域)的问题称为域转移。而我选择的域转移网络是Cycle-GAN。</p><p id="6b74" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">周波-甘</strong></p><p id="6840" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为什么是Cycle-GAN？因为它有效。而且因为上手真的很容易。访问<a class="ae lw" href="https://junyanz.github.io/CycleGAN/" rel="noopener ugc nofollow" target="_blank">项目网站</a>获取应用示例。你可以把画转换成照片，把斑马转换成马，把熊猫转换成熊，甚至把脸转换成拉面(这有多疯狂？！).</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mj"><img src="../Images/a45c1b9c41509c92e6583652057b9b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKe_kwZoefrELGHh06sbuw.jpeg"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">Applications of Cycle-GAN (pic. from <a class="ae lw" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank">original paper</a>)</figcaption></figure><p id="ae8c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要开始，你只需要准备两个文件夹，里面有你两个领域的图片(例如男性照片和女性照片)，用Cycle-GAN的PyTorch实现克隆<a class="ae lw" href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" rel="noopener ugc nofollow" target="_blank">作者的repo </a>，然后开始训练。就是这样。</p><p id="c385" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">工作原理</strong></p><p id="0221" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这张来自原始论文的图简明而完整地描述了这个模型是如何工作的。我喜欢这个想法，因为它简单、优雅，而且会产生很好的效果。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mk"><img src="../Images/6270902e6a17a892c65efa3d80cec322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ztKN2796jaiDh3mP1RiPhg.png"/></div></div></figure><p id="dc5d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">除了GAN丢失和循环一致性丢失之外，作者还添加了身份映射丢失。它的作用就像模型的正则化器，如果图像来自目标域，它希望图像不要改变。例如，如果Zebra-generator的输入是Zebra的图像，那么它根本不应该被转换。这种额外的损失有助于保留输入图像的颜色(见下图)</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ml"><img src="../Images/843a5d70eee41c45094b244976e64109.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*dnjMJ6Qpsaqcglb3RI8PQQ.png"/></div></div></figure><p id="f89b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">网络架构</strong></p><p id="2bea" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">生成器网络包含用于对输入进行两次下采样的两个步长为2的卷积、几个残差块和用于上采样的两个分数步长卷积。ReLu激活和实例规范化用于所有层。</p><p id="feb8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">3层全卷积网络用作鉴别器。该分类器没有任何完全连接的层，因此它接受任何大小的输入图像。论文<a class="ae lw" href="https://arxiv.org/abs/1411.4038" rel="noopener ugc nofollow" target="_blank">中首次介绍了一种FCN架构，用于语义分割的全卷积网络</a>，这种类型的模型如今变得相当流行。</p><p id="1ab7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">全卷积鉴别器将一个输入映射到几个特征图，然后判断图像是真是假。这可以被解释为从输入中提取多个补丁并将它们分类为真/假。斑块的大小(或感受野的大小)由网络的层数控制。这种鉴别器也称为贴片传感器。在这篇文章中，菲利普·伊索拉向我解释了这个网络背后的魔力。通读一下，以便更好地理解。比如1层贴片——GAN看的是16×16的贴片，而5层网络会有286×286的感受野。在Cycle-GAN论文中使用的鉴别器具有70×70感受野，具有3个4x4卷积层，随后是批量归一化和LeakyReLu激活。</p><p id="f2be" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">训练周期-甘</strong></p><p id="81a4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们试着解决将男性照片转换成女性照片的任务，反之亦然。为此，我们需要包含男性和女性图像的数据集。嗯，<a class="ae lw" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank"> CelebA数据集</a>非常适合我们的需求。它是免费的，有20万张图片和40个二元标签，如性别、眼镜、穿着、金发等等。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mm"><img src="../Images/12463173a14ccf1fb7dbeaa37312a859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2T22ENoTc39c44Fvx0-xg.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">CelebA dataset</figcaption></figure><p id="0ca3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这个数据集有9万张男性照片和11万张女性照片。这对我们的领地来说已经足够了。这些图片上人脸的平均尺寸并不大，只有150x150像素。因此，我们将所有提取的人脸尺寸调整为128x128，同时保持宽高比，并使用黑色背景。我们的循环GAN的典型输入可能如下所示:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/b79084133dd76aff60e965ed021e0f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*6l86PXMmp9fx15fP8Wzojg.png"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">128x128 preprocessed input image</figcaption></figure><p id="54ed" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">知觉丧失</strong></p><p id="1d52" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在我们的设置中，我们改变了身份损失的计算方式。我们使用来自预训练的vgg-16网络的风格特征，而不是使用每像素损失。这很合理，依姆霍。如果你想保持图像的风格，为什么要计算像素的差异，当你有层负责表现图像的风格？这个想法首先在论文<a class="ae lw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">中介绍，用于实时风格转换和超分辨率的感知损失</a>，并且广泛用于风格转换任务。这个小小的变化导致了一些有趣的效果，我将在后面描述。</p><p id="b4e8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">训练</strong></p><p id="94e6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">嗯，整体模型相当庞大。我们同时训练4个网络。输入通过它们几次，以计算所有损失，加上所有梯度也必须传播。1在GForce 1080上对200k图像进行一次训练需要大约5个小时，因此很难用不同的超参数进行大量实验。在我们的最终模型中，用感知损失替代身份损失是与原始Cycle-GAN配置的唯一变化。具有少于或多于3层的贴片没有显示出好的结果。beta =(0.5，0.999)的Adam用作优化器。学习率从0.0002开始，每个时期都有小的衰减。Batchsize等于1，到处都使用实例规范化，而不是批处理规范化。我想注意到的一个有趣的技巧是，不是将生成器的最后一个输出提供给鉴别器，而是使用了一个包含50个先前生成的图像的缓冲区，因此来自该缓冲区的一个随机图像被传递给鉴别器。因此，D网络使用早期版本g的图像。这个有用的技巧是Soumith Chintala的精彩笔记中列出的技巧之一。我建议在与GANs合作时，将此列表放在您的面前。我们没有时间尝试所有这些方法，例如LeakyReLu和Generator中的替代上采样层。但是设置和控制生成器-鉴别器对的训练时间表的技巧确实增加了学习过程的稳定性。</p><p id="45a4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">实验</strong></p><p id="b6e3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">最后，我们看到了示例部分。</p><p id="8039" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">训练生成网络与训练其他深度学习模型有点不同。您不会经常看到损耗降低和精度提高的曲线图。对你的模型做得有多好的评估主要是通过可视化查看生成器的输出来完成的。一个典型的循环甘训练过程看起来像这样:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mo"><img src="../Images/e511a667cbf2fe70b1ddc3a331147134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35vcpi2FVpZbJlVpz-wLsg.png"/></div></div></figure><p id="180f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">发电机偏离，其他损失慢慢下降，但尽管如此，模型的输出是相当好的和合理的。顺便说一下，为了获得这种训练过程的可视化，我们使用了visdom，一个由脸书研究所维护的易于使用的开源产品。在每次迭代中，显示以下8张图片:</p><ul class=""><li id="4dd5" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki dt translated">real_A —来自域A的输入</li><li id="8b2f" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">fake_B —由生成器A-&gt;B转换的real_A</li><li id="e8a0" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">rec_A —重建图像，fake_B由生成器B-&gt;A转换</li><li id="6eb4" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">idty_B —实数_A，由生成器B-&gt;A转换</li><li id="ccc7" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">4根据图像从域B转换到域a</li></ul><p id="1b84" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">经过5个时代的训练，你可以期待一个模型产生相当好的图像。看看下面的例子。发电机的损耗并没有减少，但女性发电机仍然可以把长得像G .辛顿的男人的脸变成女人。怎么会呢？！！</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mp"><img src="../Images/6dffe34c4c1e3009242deefe695b5f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zExLtKIXfX77s-EbyMS_hQ.png"/></div></div></figure><p id="099f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">有时候事情会变得很糟糕:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mq"><img src="../Images/c64d52d580af3a7c256d20164a38db2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAWIyGJTO1RCRluKuJKxww.png"/></div></div></figure><p id="4b6c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这种情况下，只需按Ctrl+C并打电话给记者，声称你“刚刚关闭了人工智能”。</p><p id="088b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">总之，尽管有一些伪像和低分辨率，我们可以说Cycle-GAN非常好地处理了这项任务。这是一些样品。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mr"><img src="../Images/da9707010b0ae9d19acd1d6755ede33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FWZHq6gNZejeBApx39evpQ.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">Male &lt;-&gt; Female</figcaption></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ms"><img src="../Images/c7fef3225a58e8df0ed1de4adf070d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ihWND1xfqTNP_uEgZviYw.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mt"><img src="../Images/aad4d3db37f23c9bec8467804e405fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5y4EnPOLudih0-fbyjtzeA.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">White &lt;-&gt; Asian</figcaption></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/378a56ce607068b4b8494c2134c8ad17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yFZY_gIOXP5Squmq0TBItA.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">White &lt;-&gt; Black</figcaption></figure><p id="2360" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">更多名人的例子:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mv"><img src="../Images/b4d1b2b69f26e1bb83da9091d36bf894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkmRQWbcaWhNzdWaJfwSWg.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mw"><img src="../Images/127464e7046a4bc234ad91c9dba4ae28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qnC6nMNHaA5eeMWJetdNAg.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mx"><img src="../Images/6a44313ae1976588dc7085a3e828b0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRV5IkRO8BuMaWgPOsVu9A.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff my"><img src="../Images/699473e3ac515c6756d9b9ab5d76288a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZAizOOCm3B71jpigOQDUQ.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mz"><img src="../Images/8bb5683e70180a97eaf68d6c75584d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SC9m6dZ8YU1Y-pg_wN3JUQ.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff na"><img src="../Images/e9f71804e79e6f1abb4b85a60190aaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__-UehRciiNnQpY6Cqt7dw.png"/></div></div></figure><p id="d5b6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在是一个极端的例子:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nb"><img src="../Images/e356721d11fd3d3f995b851b79236f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEf0YwauRH1lmfNgEXuB1g.png"/></div></div></figure><p id="c8fa" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">不错吧。你注意到对身份图像的有趣影响了吗？如果我们使用原始身份损失，idty_A和idty_B将正好等于它们的原始图像。但是随着感知损失，身份映射现在学习每个域的最强特征，并在输入上增加它们。这就是为什么男人变得更成熟，女人变得更亮的皮肤和更多的化妆品。如果你多次传递一幅图像，效果会增强。那是一个随时可用的美化应用程序。看看这个:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nc"><img src="../Images/bef1051d52ae3ac6f622ec145806a188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MGWAUtCKPFPIw5w6pzzcUw.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nd"><img src="../Images/4880b4bd7949da44bad76b44c50e2298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfPYRgV0Is4j77wjPqSKEQ.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ne"><img src="../Images/ba95efed03076b006ec7ec3593bc32d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhH7zdZm59SOzVnAZEOTsQ.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nf"><img src="../Images/a835ca2758434a02e6f4df4e032dbada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PF2nKg4zLR955jXQQfhK4Q.png"/></div></div></figure><p id="5c23" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">闭嘴，把你的钱给我！</p><h1 id="c070" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">图像超分辨率</h1><p id="b52f" class="pw-post-body-paragraph jc jd hu je b jf ln jh ji jj lo jl jm jn lp jp jq jr lq jt ju jv lr jx jy jz hn dt translated">Cycle-GAN产生的图像分辨率较低。最好增加和增强它们。提高图像分辨率的问题称为超分辨率。在这个领域已经做了大量的研究。我想指出两个能够解决图像超分辨率任务的最先进的深度学习模型:SRResNet和EDSR。</p><p id="be4b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> SRResNet </strong></p><p id="14e5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在论文<a class="ae lw" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">中，作者提出了一种基于ResNet架构的超分辨率生成网络。使用目标图像和超分辨率图像之间的MSE损失作为目标。但是增加了两个额外的术语:VGG特征的鉴别损失和感知损失(看，每个人都在做！)</a></p><p id="ecc1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">生成器使用带有3x3卷积、批量标准化和参数ReLu的残差块。子像素卷积用于上采样。</p><p id="1fc3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">鉴别器使用8个具有3x3内核和泄漏ReLu激活的卷积。缩减像素采样是通过步进卷积完成的(没有池层)。分类是通过最后两个完全连接的层来完成的，在末端具有s形激活。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ng"><img src="../Images/e571bd7c19877b6bed5d479b26bb6363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfo-fM1TGsGMnjoKe4ENuA.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">SRResNet architecture</figcaption></figure><p id="5a50" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> EDSR </strong></p><p id="3f4b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><a class="ae lw" href="https://arxiv.org/abs/1707.02921" rel="noopener ugc nofollow" target="_blank">增强型深度超分辨率网络</a>类似于SRResNet，但改进不多:</p><ul class=""><li id="d0b3" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki dt translated">不使用批处理标准化。这可以节省高达40%的内存，并允许增加层数和过滤器。</li><li id="0844" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">在剩余块之外不使用ReLu。</li><li id="a718" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">所有ResNet块在与单位向量连接之前都按0.1进行缩放。这有助于稳定培训过程。</li></ul><p id="4ac3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">培训</strong></p><p id="2efa" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了训练SR网络，我们需要高分辨率图像的数据集。我们从Instagram上解析了数千张带有#face标签的图片。训练通常在小块图像上进行，而不是在完整图像上进行。这有助于生成器处理细小的细节。在评估模式中，全尺寸图像被传递，这是可能的，因为网络的全卷积方式。在实践中，EDSR，这被认为是一个增强SRResNet，没有显示出更好的结果，并在培训时间缓慢。因此，在我们的最终流水线中，我们使用了在64x64补丁上训练的SRResNet，没有感知损失，也没有鉴别器。下面是训练集中的一些例子。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mv"><img src="../Images/8dc4ba20a8c36c2d6bb1583f5e4be110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R4_P1EcoJ7fEH70D3_-oIA.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nh"><img src="../Images/f028ab0f00532d2193d5c9fdfbd88141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBIC1Nn9C4nqCPkC-BJJzw.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ni"><img src="../Images/9e5e6d951c9bb77e587345d9b16288f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eme7_EXEofuOFGANRGrO5A.png"/></div></div></figure><p id="b1e4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这就是这个网络在我们的人造图像上的表现。不完美，但现在还可以。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nj"><img src="../Images/6d38a3199c7be408043760faf2ad8ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhYiH2oHevLbz7dvA6qUcg.png"/></div></div></figure><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nk"><img src="../Images/c6476f33999bb6a1516c801b520f4b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXKvy-72Ysun3armrSoMew.png"/></div></div></figure><h1 id="a29c" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">将图像粘贴回原始图像</h1><p id="02b2" class="pw-post-body-paragraph jc jd hu je b jf ln jh ji jj lo jl jm jn lp jp jq jr lq jt ju jv lr jx jy jz hn dt translated">甚至这个任务都可以通过深度学习来完成。我发现了一篇关于图像混合的有趣论文。GP-GAN:迈向逼真的高分辨率图像融合。我不会深究细节，只给你看报纸上的图表。</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nl"><img src="../Images/3a1dc95e3df218e366751ee7986c986b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5zcDPvU2E8XoZjDXFoPow.png"/></div></div></figure><p id="15ac" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但是我们已经实现了简单明了的解决方案。变换后的面粘贴到原始图像中，当靠近其边缘时，透明度会增加。结果看起来像这样:</p><figure class="ly lz ma mb fq iv fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/90bdfa9a70de7f5b344e5b815ce2c959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*-ZPoFGO9-A0CxyFQSYfh9g.png"/></div></figure><h1 id="645b" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">结论</h1><p id="ae97" class="pw-post-body-paragraph jc jd hu je b jf ln jh ji jj lo jl jm jn lp jp jq jr lq jt ju jv lr jx jy jz hn dt translated">结果似乎还可以，但是还没有准备好生产。在压缩生成网络以使它们变得轻便和快速的领域，还有很多工作要做。这包括知识提炼、因式分解、量化等方面的实验。然后整个管道可以被部署为移动应用。未来有很多有趣的工作…</p></div></div>    
</body>
</html>