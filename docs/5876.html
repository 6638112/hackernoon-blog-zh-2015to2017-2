<html>
<head>
<title>Digging in Deep: Solving a Real Problem with Haskell Tensor Flow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入挖掘:用Haskell张量流解决一个实际问题</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/digging-in-deep-solving-a-real-problem-with-haskell-tensor-flow-e85939c0ea9d?source=collection_archive---------11-----------------------#2017-08-21">https://medium.com/hackernoon/digging-in-deep-solving-a-real-problem-with-haskell-tensor-flow-e85939c0ea9d?source=collection_archive---------11-----------------------#2017-08-21</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="3b5b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">上周，我们熟悉了张量流的核心概念。我们学习了常量、占位符和变量张量之间的区别。Haskell和Python绑定都有表示这些的函数。不过Python版本要简单一些。一旦我们有了张量，我们就写了一个程序来“学习”一个简单的线性方程。</p><p id="be52" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">本周，我们将解决一个实际的机器学习问题。我们将使用<a class="ae jp" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">鸢尾数据集</a>，它包含不同鸢尾花的测量值。每一种花属于三个物种中的一个。我们的程序将“学习”一个从测量值中选择物种的函数。这项功能将涉及一个完全连接的神经网络。</p><h1 id="f698" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">格式化我们的输入</h1><p id="7eef" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">几乎所有机器学习问题的第一步都是数据处理。毕竟，我们的数据不会神奇地被解析成Haskell数据类型。幸运的是，<a class="ae jp" href="https://hackage.haskell.org/package/cassava" rel="noopener ugc nofollow" target="_blank">木薯</a>是一个很好的图书馆来帮助我们。Iris数据集由中的数据组成。csv文件，每个文件都有一个标题行和一系列记录。它们看起来有点像这样:</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="dab4" class="lc jr hu ky b fv ld le l lf lg">120,4,setosa,versicolor,virginica<br/>6.4,2.8,5.6,2.2,2<br/>5.0,2.3,3.3,1.0,1<br/>4.9,2.5,4.5,1.7,2<br/>4.9,3.1,1.5,0.1,0<br/>...</span></pre><p id="b2ff" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">每行包含一条记录。一个记录有四个花测量值和一个最终标签。在这种情况下，我们有三种类型的花，我们试图进行分类:鸢尾、杂色鸢尾和海滨鸢尾。因此，最后一列包含数字0、1和2，分别对应于这些类别。</p><p id="13e1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们创建一个代表每条记录的数据类型。然后我们可以逐行解析文件。我们的<code class="eh lh li lj ky b">IrisRecord</code>类型将包含特征数据和结果标签。这种类型将作为我们的原始数据和张量格式之间的桥梁，我们将需要运行我们的学习算法。我们将为我们的记录类型派生“通用”typeclass，并使用它来获得<code class="eh lh li lj ky b">FromRecord</code>。一旦我们的类型有了<code class="eh lh li lj ky b">FromRecord</code>的实例，我们就可以轻松地解析它。注意，在整篇文章中，我将<strong class="it hv">从代码示例中省略</strong>imports部分。我在底部的附录中包含了从这些文件导入的完整列表。我们还将全程使用<code class="eh lh li lj ky b">OverloadedLists</code>扩展。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="1db1" class="lc jr hu ky b fv ld le l lf lg">{-# LANGUAGE DeriveGeneric #-}<br/>{-# LANGUAGE OverloadedLists #-}</span><span id="d620" class="lc jr hu ky b fv lk le l lf lg">...</span><span id="880a" class="lc jr hu ky b fv lk le l lf lg">data IrisRecord = IrisRecord<br/>  { field1 :: Float<br/>  , field2 :: Float<br/>  , field3 :: Float<br/>  , field4 :: Float<br/>  , label  :: Int64<br/>  }<br/>  deriving (Generic)</span><span id="5e40" class="lc jr hu ky b fv lk le l lf lg">instance FromRecord IrisRecord</span></pre><p id="7dfe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们有了自己的类型，我们将编写一个函数，<code class="eh lh li lj ky b">readIrisFromFile</code>，它将从一个CSV文件中读入我们的数据。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="b4fc" class="lc jr hu ky b fv ld le l lf lg">readIrisFromFile :: FilePath -&gt; IO (Vector IrisRecord)<br/>readIrisFromFile fp = do<br/>  contents &lt;- readFile fp<br/>  let contentsAsBs = pack contents<br/>  let results = decode <br/>    HasHeader contentsAsBs :: Either String (Vector IrisRecord)<br/>  case results of<br/>    Left err -&gt; error err<br/>    Right records -&gt; return records</span></pre><p id="c1a9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们不想总是将我们的整个数据集输入到我们的训练系统中。因此，给定一系列这些项目，我们应该能够挑选出一个随机样本。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="f0d6" class="lc jr hu ky b fv ld le l lf lg">sampleSize :: Int<br/>sampleSize = 10</span><span id="b1ea" class="lc jr hu ky b fv lk le l lf lg">chooseRandomRecords :: Vector IrisRecord -&gt; IO (Vector IrisRecord)<br/>chooseRandomRecords records = do<br/>  let numRecords = Data.Vector.length records<br/>  chosenIndices &lt;- <br/>    take sampleSize &lt;$&gt; shuffleM [0..(numRecords - 1)]<br/>  return $ fromList $ map (records !) chosenIndices</span></pre><p id="2e85" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">一旦我们选择了每次运行使用的记录向量，我们还没有完成。我们需要获取这些记录，并将它们转换成<code class="eh lh li lj ky b">TensorData</code>,然后输入到我们的算法中。我们通过输入一个形状和一个一维向量值来创建<code class="eh lh li lj ky b">TensorData</code>的项目。首先，我们需要知道输入和输出的形状。这两者都取决于样本中的行数。“输入”将为我们的集合中的四个特性中的每一个都有一列。同时，输出将有一个单独的标签值列。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="baf9" class="lc jr hu ky b fv ld le l lf lg">irisFeatures :: Int64<br/>irisFeatures = 4</span><span id="bd48" class="lc jr hu ky b fv lk le l lf lg">irisLabels :: Int64<br/>irisLabels = 3</span><span id="b9d8" class="lc jr hu ky b fv lk le l lf lg">convertRecordsToTensorData :: Vector IrisRecord <br/>                           -&gt; (TensorData Float, TensorData Int64)<br/>convertRecordsToTensorData records = (input, output)<br/>  where<br/>    numRecords = Data.Vector.length records <br/>    input = encodeTensorData <br/>      [fromIntegral numRecords, irisFeatures] (undefined)<br/>    output = encodeTensorData <br/>      [fromIntegral numRecords] (undefined)</span></pre><p id="7b64" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们需要做的就是把各种记录转化成一维向量进行编码。这是最后一个函数:</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="e6d7" class="lc jr hu ky b fv ld le l lf lg">convertRecordsToTensorData :: Vector IrisRecord <br/>                           -&gt; (TensorData Float, TensorData Int64)<br/>convertRecordsToTensorData records = (input, output)<br/>  where<br/>    numRecords = Data.Vector.length records <br/>    input = encodeTensorData <br/>      [fromIntegral numRecords, irisFeatures]<br/>      (fromList $ concatMap recordToInputs records)<br/>    output = encodeTensorData <br/>      [fromIntegral numRecords] <br/>      (label &lt;$&gt; records)<br/>    recordToInputs :: IrisRecord -&gt; [Float]<br/>    recordToInputs rec = <br/>      [field1 rec, field2 rec, field3 rec, field4 rec]</span></pre><h1 id="d5d0" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">神经网络基础</h1><p id="3983" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">现在我们已经解决了这个问题，我们可以开始编写我们的模型了。记住，我们想要用我们的模型执行两个不同的动作。首先，我们希望能够利用我们的训练输入来训练权重。其次，我们希望能够通过一个测试数据集，并确定错误率。我们可以将这两种不同的功能表示为一个单独的<code class="eh lh li lj ky b">Model</code>对象。记住<code class="eh lh li lj ky b">Session</code>单子，在那里我们运行所有的张量流活动。训练将运行一个改变变量的操作，但不返回任何内容。错误率计算将返回一个浮点值。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="0977" class="lc jr hu ky b fv ld le l lf lg">data Model = Model<br/>  { train :: TensorData Float -- Training input<br/>          -&gt; TensorData Int64 -- Training output<br/>          -&gt; Session ()<br/>  , errorRate :: TensorData Float -- Test input<br/>              -&gt; TensorData Int64 -- Test output<br/>              -&gt; Session Float<br/>  }</span></pre><p id="c8f4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们要建立一个完全连接的神经网络。我们将有4个输入单元(每个不同的特性一个)，然后我们将有3个输出单元(我们试图表示的每个类一个)。在中间，我们将使用一个由10个单元组成的隐藏层。这意味着我们需要两套权重和偏好。我们将写一个函数，当给定维数时，它将给出每一层的变量张量。我们需要权重和偏差张量，加上层的结果张量。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="eab0" class="lc jr hu ky b fv ld le l lf lg">buildNNLayer :: Int64 -&gt; Int64 -&gt; Tensor v Float<br/>             -&gt; Build (Variable Float, Variable Float, Tensor Build Float)<br/>buildNNLayer inputSize outputSize input = do<br/>  weights &lt;- truncatedNormal (vector [inputSize, outputSize]) &gt;&gt;=<br/>    initializedVariable<br/>  bias &lt;- truncatedNormal (vector [outputSize]) &gt;&gt;=<br/>    initializedVariable<br/>  let results = (input `matMul` readValue weights) `add` <br/>                readValue bias<br/>  return (weights, bias, results)</span></pre><p id="5caf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们在<code class="eh lh li lj ky b">Build</code>单子中这样做，单子允许我们构造变量，等等。为了简单起见，我们将对所有变量使用<code class="eh lh li lj ky b">truncatedNormal</code>分布。我们在一个<code class="eh lh li lj ky b">vector</code>张量中指定每个变量的大小，然后初始化它们。然后，我们将通过将输入乘以我们的权重并添加偏差来创建结果张量。</p><h1 id="ae2f" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">构建我们的模型</h1><p id="7c00" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">现在我们将开始构建我们的<code class="eh lh li lj ky b">Model</code>对象，同样是在<code class="eh lh li lj ky b">Build</code>单子内。我们首先指定输入和输出占位符，以及隐藏单元的数量。我们还将使用-1的<code class="eh lh li lj ky b">batchSize</code>来说明我们想要可变数量的输入样本。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="fff6" class="lc jr hu ky b fv ld le l lf lg">irisFeatures :: Int64<br/>irisFeatures = 4</span><span id="e4c8" class="lc jr hu ky b fv lk le l lf lg">irisLabels :: Int64<br/>irisLabels = 3<br/>-- ^^ From above</span><span id="f087" class="lc jr hu ky b fv lk le l lf lg">createModel :: Build Model<br/>createModel = do<br/>  let batchSize = -1 -- Allows variable sized batches<br/>  let numHiddenUnits = 10<br/>  inputs &lt;- placeholder [batchSize, irisFeatures]<br/>  outputs &lt;- placeholder [batchSize]</span></pre><p id="afbe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后我们将得到两层变量的节点，以及它们的结果。在这些层之间，我们将添加一个“整流器”激活函数<code class="eh lh li lj ky b">relu</code>:</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="5a2d" class="lc jr hu ky b fv ld le l lf lg">(hiddenWeights, hiddenBiases, hiddenResults) &lt;- <br/>  buildNNLayer irisFeatures numHiddenUnits inputs<br/>let rectifiedHiddenResults = relu hiddenResults<br/>(finalWeights, finalBiases, finalResults) &lt;-<br/>  buildNNLayer numHiddenUnits irisLabels rectifiedHiddenResults</span></pre><p id="ede0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们必须得到每个输出的推断类。这意味着调用<code class="eh lh li lj ky b">argMax</code>取概率最高的类。我们也将<code class="eh lh li lj ky b">cast</code>向量，然后<code class="eh lh li lj ky b">render</code>它。这是一些Haskell张量流的特定术语，用于将张量转换成正确的类型。接下来，我们将它与输出占位符进行比较，看看有多少是正确的。然后我们将创建一个节点来计算这次运行的错误率。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="7265" class="lc jr hu ky b fv ld le l lf lg">actualOutput &lt;- render $ cast $ <br/>  argMax finalResults (scalar (1 :: Int64))<br/>let correctPredictions = equal actualOutput outputs<br/>errorRate_ &lt;- render $ 1 - (reduceMean (cast correctPredictions))</span></pre><p id="2f0e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们必须实际做训练的工作。首先，我们将为我们的预期输出制作<code class="eh lh li lj ky b">oneHot</code>向量。这意味着将标签<code class="eh lh li lj ky b">0</code>转换成向量<code class="eh lh li lj ky b">[1,0,0]</code>，等等。我们将这些值与我们的结果进行比较(在我们取最大值之前)，这给出了我们的损失函数。然后我们会列出我们想要训练的参数。<code class="eh lh li lj ky b">adam</code>优化器将在修改参数时最小化我们的损失函数。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="2212" class="lc jr hu ky b fv ld le l lf lg">let outputVectors = oneHot outputs (fromIntegral irisLabels) 1 0<br/>let loss = reduceMean $ fst $ softmaxCrossEntropyWithLogits finalResults outputVectors<br/>let params = [hiddenWeights, hiddenBiases, finalWeights, finalBiases]<br/>train_ &lt;- minimizeWith adam loss params</span></pre><p id="9434" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们已经准备好了<code class="eh lh li lj ky b">errorRate_</code>和<code class="eh lh li lj ky b">train_</code>节点。这里还有最后一步。我们必须插入占位符值，并创建接收张量数据的函数。还记得上周<a class="ae jp" href="https://www.mmhaskell.com/blog/2017/8/14/starting-out-with-haskell-tensor-flow" rel="noopener ugc nofollow" target="_blank">的<code class="eh lh li lj ky b">feed</code>模式</a>吗？我们在这里再次使用它。最后，我们的模型完成了！</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="c50c" class="lc jr hu ky b fv ld le l lf lg">return $ Model<br/>  { train = \inputFeed outputFeed -&gt; <br/>      runWithFeeds<br/>        [ feed inputs inputFeed<br/>        , feed outputs outputFeed<br/>        ]<br/>        train_<br/>  , errorRate = \inputFeed outputFeed -&gt; unScalar &lt;$&gt;<br/>      runWithFeeds<br/>        [ feed inputs inputFeed<br/>        , feed outputs outputFeed<br/>        ]<br/>        errorRate_<br/>  }</span></pre><h1 id="4685" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">把这一切联系在一起</h1><p id="3d53" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">现在我们将编写运行会话的主函数。它将有三个阶段。在准备阶段，我们将加载我们的数据，并使用<code class="eh lh li lj ky b">build</code>函数来获取我们的模型。然后，我们将通过选择样本并将我们的记录转换为数据来训练我们的模型1000步。每100步，我们将打印输出。最后，我们将通过使用测试数据来确定最终的错误率。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="0fba" class="lc jr hu ky b fv ld le l lf lg">runIris :: FilePath -&gt; FilePath -&gt; IO ()<br/>runIris trainingFile testingFile = runSession $ do<br/>  -- Preparation<br/>  trainingRecords &lt;- liftIO $ readIrisFromFile trainingFile<br/>  testRecords &lt;- liftIO $ readIrisFromFile testingFile<br/>  model &lt;- build createModel</span><span id="f2e9" class="lc jr hu ky b fv lk le l lf lg">  -- Training<br/>  forM_ ([0..1000] :: [Int]) $ \i -&gt; do<br/>    trainingSample &lt;- liftIO $ chooseRandomRecords trainingRecords<br/>    let (trainingInputs, trainingOutputs) =<br/>          convertRecordsToTensorData trainingSample<br/>    (train model) trainingInputs trainingOutputs<br/>    when (i `mod` 100 == 0) $ do<br/>      err &lt;- (errorRate model) trainingInputs trainingOutputs<br/>      liftIO $ putStrLn $ <br/>        "Current training error " ++ show (err * 100)</span><span id="9885" class="lc jr hu ky b fv lk le l lf lg">  liftIO $ putStrLn ""</span><span id="12ec" class="lc jr hu ky b fv lk le l lf lg">  -- Testing<br/>  let (testingInputs, testingOutputs) = <br/>        convertRecordsToTensorData testRecords<br/>  testingError &lt;- (errorRate model) testingInputs testingOutputs<br/>  liftIO $ putStrLn $ "test error " ++ show (testingError * 100)</span><span id="50c6" class="lc jr hu ky b fv lk le l lf lg">  return ()</span></pre><h1 id="a0d2" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">结果</h1><p id="f1cd" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">因此，当我们实际运行所有这些输出时，我们将在测试集上得到以下结果。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="107c" class="lc jr hu ky b fv ld le l lf lg">Current training error 60.000004<br/>Current training error 30.000002<br/>Current training error 39.999996<br/>Current training error 19.999998<br/>Current training error 10.000002<br/>Current training error 10.000002<br/>Current training error 19.999998<br/>Current training error 19.999998<br/>Current training error 10.000002<br/>Current training error 10.000002<br/>Current training error 0.0</span><span id="5567" class="lc jr hu ky b fv lk le l lf lg">test error 3.333336</span></pre><p id="fc82" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们的测试样本大小是30，所以这意味着我们这次得到了29/30。虽然每次跑步的结果都不同(我显然使用了我找到的最好的结果)。由于我们的样本量如此之小，我们在这里有很高的熵(有时错误率像40%)。一般来说，我们希望在更大的测试集上训练更长的时间，这样我们可以得到更一致的结果，但这是一个好的开始。</p><h1 id="201b" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">结论</h1><p id="cfc9" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在本文中，我们回顾了使用Haskell张量流库创建神经网络的基础知识。我们制作了一个完全连接的神经网络，并输入我们使用<code class="eh lh li lj ky b">Cassava</code>库解析的真实数据。这个网络能够学习一个函数来从Iris数据集中对花进行分类。考虑到数据量小，我们得到了一些好的结果。</p><p id="ade6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下周再来，我们会看到如何给张量流图添加更多的总结信息。我们将使用张量板应用程序以可视格式查看我们的图表。</p><p id="0589" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">关于安装哈斯克尔张量流系统的更多细节，请查看我们的<a class="ae jp" href="https://www.mmhaskell.com/tensorflow" rel="noopener ugc nofollow" target="_blank">深度张量流教程</a>。它将引导您完成在自己的机器上运行代码的重要步骤。</p><p id="51f6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">也许你以前从未尝试过Haskell，想看看它是什么样子的。也许我已经让你相信Haskell实际上是人工智能的未来。在这种情况下，你应该查看一下我们的<a class="ae jp" href="https://www.mmhaskell.com/checklist" rel="noopener ugc nofollow" target="_blank">入门清单</a>，以获得一些开始学习这门语言的工具。</p><h1 id="41ba" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">附录:所有进口</h1><p id="0e23" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">Haskell张量流的文档仍然是正在进行的主要工作。所以我想确保我明确地列出了我们在这里使用的所有不同函数需要导入的模块。</p><pre class="kt ku kv kw fq kx ky kz la aw lb dt"><span id="e28d" class="lc jr hu ky b fv ld le l lf lg">import Control.Monad (forM_, when)<br/>import Control.Monad.IO.Class (liftIO)<br/>import Data.ByteString.Lazy.Char8 (pack)<br/>import Data.Csv (FromRecord, decode, HasHeader(..))<br/>import Data.Int (Int64)<br/>import Data.Vector (Vector, length, fromList, (!))<br/>import GHC.Generics (Generic)<br/>import System.Random.Shuffle (shuffleM)</span><span id="c2cd" class="lc jr hu ky b fv lk le l lf lg">import TensorFlow.Core (TensorData, Session, Build, render, runWithFeeds, feed, unScalar, build,<br/>                        Tensor, encodeTensorData)<br/>import TensorFlow.Minimize (minimizeWith, adam)<br/>import TensorFlow.Ops (placeholder, truncatedNormal, add, matMul, relu,<br/>                      argMax, scalar, cast, oneHot, reduceMean, softmaxCrossEntropyWithLogits, <br/>                      equal, vector)<br/>import TensorFlow.Session (runSession)<br/>import TensorFlow.Variable (readValue, initializedVariable, Variable)</span></pre></div></div>    
</body>
</html>