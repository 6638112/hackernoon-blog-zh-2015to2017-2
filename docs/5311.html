<html>
<head>
<title>Facial Similarity with Siamese Networks in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中与暹罗网络的面部相似性</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7?source=collection_archive---------0-----------------------#2017-07-20">https://medium.com/hackernoon/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7?source=collection_archive---------0-----------------------#2017-07-20</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="3350" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">这是由两部分组成的文章的第二部分。你应该在这里继续之前阅读 <a class="ae jt" href="https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e" rel="noopener ugc nofollow" target="_blank"> <em class="hu">第一部分</em> </a> <em class="hu">。</em></p></blockquote><p id="60f4" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt jx translated"><span class="l jy jz ka bm kb kc kd ke kf di">在</span>中，上一篇文章讨论了one shot <a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank"> learning </a>旨在解决的一类问题，以及暹罗网络如何成为这类问题的良好候选。我们讨论了一个特殊的损失函数，它计算一对图像中两个图像的相似度。我们现在将实现之前在PyTorch中讨论过的所有内容。</p><p id="8917" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><em class="iw">你可以在本文末尾找到Jupyter笔记本的完整代码。</em></p><h1 id="868a" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">建筑</h1><p id="8a4d" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">我们将使用标准的卷积神经<a class="ae jt" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>架构。我们在每个卷积层之后使用批量归一化，然后是丢弃。</p><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="lo lp l"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="ak">Snippet 1.0 </strong>Siamese Network Architecture</figcaption></figure><p id="893e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这个网络没什么特别的。它接受100 <em class="iw"> px </em> *100 <em class="iw"> px </em>的输入，并且在卷积层之后具有3个全连接层。</p><h2 id="9a07" class="lu kh hu bd ki lv lw lx km ly lz ma kq ju mb mc ku jv md me ky jw mf mg lc mh dt translated">但是另一只暹罗猫在哪里？</h2><p id="735f" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">在上一篇文章中，我展示了一对<em class="iw">网络</em>如何处理一对网络中的每张图片。但是在这篇文章中，只有一个网络。因为两个网络的权重被限制为相同，所以我们使用一个模型，并连续输入两个图像。之后，我们使用两幅图像计算损失值，然后反向传播。这在绝对不影响其他指标(比如准确性)的情况下节省了大量内存。</p><h1 id="307c" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">对比损失</h1><p id="1e58" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">我们将对比损失定义为</p><figure class="lj lk ll lm fq ln fe ff paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="fe ff mi"><img src="../Images/725365a5985bbaea09ee1563ab6305f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDo6545MUvW9t-A8Sd-hHw.png"/></div></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek">Equation 1.0</figcaption></figure><p id="3a23" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们将Dw(就是欧几里德距离)定义为:</p><figure class="lj lk ll lm fq ln fe ff paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="fe ff mi"><img src="../Images/a522ef61717d1b04d00caa89d5c9f399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tj-3urWYfhUD7pQ8N_SDCw.png"/></div></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek">Equation 1.1</figcaption></figure><p id="37f7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">Gw是我们的网络对一幅图像的输出。</p><p id="525a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">PyTorch的对比损失是这样的:</p><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="lo lp l"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="ak">Snippet 2.0</strong> Contrastive loss with a default margin value of 2</figcaption></figure><h1 id="c1d3" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">数据集</h1><p id="f297" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">在前一篇文章中，我想用MNIST，但是一些读者建议我用我在同一篇文章中讨论的面部相似性的例子。因此我在&amp; T faces数据集从MNIST/OmniGlot切换到<strong class="ix hv">。</strong></p><p id="9aaf" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">该数据集包含40个对象从不同角度拍摄的图像。为了测试我们的模型，我将培训中的最后3个主题放在一边。</p><div class="lj lk ll lm fq ab cb"><figure class="mp ln mq mr ms mt mu paragraph-image"><img src="../Images/1cca60685b62d9ded13b0d131ae2a966.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*31e_wjr4HgKN80xFF4QIBg.png"/></figure><figure class="mp ln mv mr ms mt mu paragraph-image"><img src="../Images/64327fc27d9e09207f95c006fa8ce08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*19uUmNIb172vv5m0YNmk-Q.png"/><figcaption class="lq lr fg fe ff ls lt bd b be z ek mw di mx my">Figure 1.0. <strong class="bd mz">Left</strong>: Samples from different classes. <strong class="bd mz">Right</strong>: All Samples of one subject</figcaption></figure></div><h1 id="0868" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">数据加载</h1><p id="3f2f" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">我们的架构需要一个输入对，以及标签(相似/不相似)。因此，我创建了自己的定制数据加载器来完成这项工作。它使用图像文件夹从文件夹中读取图像。这意味着您可以在任何想要的数据集上使用它。</p><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="lo lp l"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="ak">Snippet 3.0</strong>. This data set generates a pair of images and the similarity label. Label will be 0 if images are from same class, and 1 if they are from different classes.</figcaption></figure><p id="0c7c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">暹罗网络数据集生成一对图像，以及它们的相似性标签<strong class="ix hv"> (0表示真实，1表示冒名顶替)</strong>。为了防止不平衡，我确保将近一半的图像来自同一个类，而另一半不是。</p><h1 id="aaa9" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">培训暹罗网络</h1><p id="a797" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">暹罗网络的训练过程如下:</p><ol class=""><li id="922c" class="na nb hu ix b iy iz jc jd ju nc jv nd jw ne js nf ng nh ni dt translated">通过网络传递图像对的第一个图像。</li><li id="b347" class="na nb hu ix b iy nj jc nk ju nl jv nm jw nn js nf ng nh ni dt translated">通过网络传递图像对的第二个图像。</li><li id="f1c9" class="na nb hu ix b iy nj jc nk ju nl jv nm jw nn js nf ng nh ni dt translated">使用1和2的输出计算损耗。</li><li id="c6d0" class="na nb hu ix b iy nj jc nk ju nl jv nm jw nn js nf ng nh ni dt translated">反向传播损失以计算梯度。</li><li id="8611" class="na nb hu ix b iy nj jc nk ju nl jv nm jw nn js nf ng nh ni dt translated">使用优化器更新权重。在这个例子中，我们将使用Adam。</li></ol><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="lo lp l"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="ak">Snippet 4.0</strong> Training the siamese network</figcaption></figure><p id="e13c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">使用Adam和0.0005的学习率对网络进行100个时期的训练。损失随时间变化的图表如下所示:</p><figure class="lj lk ll lm fq ln fe ff paragraph-image"><div class="fe ff no"><img src="../Images/15d41f51d1285c3f4b93559673d1ad52.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*GCbnzM6OY7C_WI6wXZSU3g.png"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="bd mz">Figure 2.0 </strong>Loss value over time. The x axis is number of iterations</figcaption></figure><h1 id="558a" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">测试网络</h1><p id="5140" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">我们为测试集提供了3个主题，这些主题将用于评估我们的模型的性能。</p><p id="b905" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">为了计算相似性，我们只需计算Dw(等式1.1)。该距离直接对应于图像对之间的相异度。Dw的高值表示较高的不相似性。</p><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="lo lp l"/></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek"><strong class="ak">Snippet 3.0</strong> Evaluating the model by calculating distance between model outputs.</figcaption></figure><figure class="lj lk ll lm fq ln fe ff paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="fe ff np"><img src="../Images/fbcd61d31f1cb3b3d152d6cd9d43919a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1p1_p5AU_7Ki1jPrsFhsw.png"/></div></div><figcaption class="lq lr fg fe ff ls lt bd b be z ek">Figure 3.0 Some outputs of the model. Lower values indicate more similarity, and higher values indicate less similarity.</figcaption></figure><p id="e485" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">效果挺好的。网络能够区分同一个人，即使他们来自不同的角度。它在区分不同的图像方面也做得很好。</p><h1 id="e5c3" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">结论</h1><p id="3748" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">我们讨论并实现了一个暹罗网络来区分面部识别的人脸对。当特定人脸的训练样本很少(或只有一个)时，这很有用。我们使用判别损失函数来训练神经网络。</p><h2 id="0715" class="lu kh hu bd ki lv lw lx km ly lz ma kq ju mb mc ku jv md me ky jw mf mg lc mh dt translated">你可以在我的回购中找到完整的代码:</h2><div class="nq nr fm fo ns nt"><a href="https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab ej"><div class="nv ab nw cl cj nx"><h2 class="bd hv fv z el ny eo ep nz er et ht dt translated">Harvey slash/py torch中与连体网络相似的面部</h2><div class="oa l"><h3 class="bd b fv z el ny eo ep nz er et ek translated">Pytorch中具有连体网络的面部相似性</h3></div></div><div class="ob l"><div class="oc l od oe of ob og mn nt"/></div></div></a></div><h1 id="0c15" class="kg kh hu bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">附言</h1><p id="668c" class="pw-post-body-paragraph iu iv hu ix b iy le ja jb jc lf je jf ju lg ji jj jv lh jm jn jw li jq jr js hn dt translated">如果你喜欢这篇文章，请❤在下面分享它。欢迎提出建议，如果你有不明白的地方，请随时问我。</p><figure class="lj lk ll lm fq ln"><div class="bz el l di"><div class="oh lp l"/></div></figure></div></div>    
</body>
</html>