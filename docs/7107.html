<html>
<head>
<title>Building Brundage Bot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建筑Brundage机器人</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/building-brundage-bot-10252facf3d1?source=collection_archive---------21-----------------------#2017-10-17">https://medium.com/hackernoon/building-brundage-bot-10252facf3d1?source=collection_archive---------21-----------------------#2017-10-17</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="8033" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">训练神经网络以跟上ArXiv上的最新ML论文</h2></div><h2 id="ea2d" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">TL；速度三角形定位法(dead reckoning)</h2><p id="b174" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">我创建了一个模型来预测Miles Brundage会发哪些arXiv论文，并把它变成了一个Twitter机器人。它以0.7的精度和0.6的召回率预测迈尔斯的推文。代码可在<a class="ae la" href="https://github.com/amauboussin/arxiv-twitterbot" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h2 id="b8ac" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">背景</h2><p id="70f2" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">随着arXiv越来越受欢迎，来自世界各地的研究人员一有新想法就发布预印本，而不是等待会议或同行评审。每周都有数百份与ML相关的预印本发表，所以浏览所有的摘要来找出哪些是与你的兴趣相关的、新颖的、没有重大方法错误的是非常耗时的。</p><p id="9210" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">驾驭永无止境的预印本流的最佳方式是人类策展人。我个人关注几十个Twitter账户，几十个ML社区的人，以及几封电子邮件时事通讯，这些邮件通讯发布了最近的论文(丹尼·布里兹的<a class="ae la" href="https://www.getrevue.co/profile/wildml" rel="noopener ugc nofollow" target="_blank">人工智能中的狂野一周</a>和杰克·克拉克的<a class="ae la" href="https://jack-clark.net/" rel="noopener ugc nofollow" target="_blank">进口人工智能</a>是我的两个最爱)。这对于找到本周最突出的预印本很有效，但是它没有抓住有趣研究的长尾。大多数人都很忙，一周不会策划超过几篇论文，这是可以理解的。</p><p id="d72b" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">唯一的例外是迈尔斯·布伦戴奇。迈尔斯是推特arXiv预印本的迈克尔·乔丹。他每周几次在Twitter上发布20-50篇文章。自2017年初以来，他已经发布了近6000条arXiv链接。一天超过20个。他的推文每周都会可靠地找出一些有趣的论文，否则我是不会看到的。</p><p id="5818" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">过了一会儿，我开始想:迈尔斯是如何选择所有这些论文的？他的过程可能自动化吗？迈尔斯在推特上回答了第一个问题:</p><figure class="lh li lj lk fq ll"><div class="bz el l di"><div class="lm ln l"/></div></figure><figure class="lh li lj lk fq ll"><div class="bz el l di"><div class="lm ln l"/></div></figure><figure class="lh li lj lk fq ll"><div class="bz el l di"><div class="lm ln l"/></div></figure><p id="c884" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">我创造了Brundage Bot来回答第二个问题。</p><h2 id="bd99" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">收集数据集</h2><p id="9fe5" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">首先，我使用Twitter API下载了Miles的所有推文，并解析了所有指向arxiv.org的链接。然后，我使用arXiv API从Miles提到的类别中下载每篇论文的元数据，查看:</p><pre class="lh li lj lk fq lo lp lq lr aw ls dt"><span id="ca46" class="jj jk hu lp b fv lt lu l lv lw">cs.* (Computer Science - All subcategories)<br/>cond-mat.dis-nn (Physics - Disordered Systems and Neural Networks)<br/>q-bio.NC (Quantitative Biology - Neurons and Cognition)<br/>stat.CO (Statistics - Computation)<br/>stat.ML (Statistics - Machine Learning)</span></pre><p id="dea8" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">arXiv API返回每篇论文的标题、摘要和作者。我通过每个URL中的arXiv ID将两个数据集连接在一起。截至今天，该数据集包含27k篇论文，其中约5800篇(21.5%)被Miles发布。这张图表显示了每天有多少篇论文在arXiv上发表，以及有多少篇论文在tweeted上被转发:</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="fe ff lx"><img src="../Images/8dafa9dcdb70e73ba00779420ca48012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I88XhIsNU06yFByxEmpp7A.png"/></div></div></figure><h1 id="ced8" class="me jk hu bd jl mf mg mh jp mi mj mk jt ja ml jb jx jd mm je kb jg mn jh kf mo dt translated">我们能预测Miles会发什么微博吗？</h1><p id="1634" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">接下来，我想看看是否有可能使用来自arXiv API的信息(标题、摘要和作者)来预测Miles是否会在tweet上发布一篇论文。</p><h2 id="70f4" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">基线模型</h2><p id="c20c" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">我将每篇论文的标题与其摘要连接起来，并从文本中创建了<a class="ae la" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> tf-idf </a> n-gram特征(直到三元模型)。然后，我将代表论文作者和arXiv类别的one-hot-encoded向量连接起来。我过滤掉了在训练集中出现不到30次的n-grams(从大约25k的总摘要中)和出现不到3次的作者。这留下了大约17k个特征。</p><p id="d176" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">最后，我拿出随机选择的10%的数据作为测试集，并使用sklearn训练逻辑回归。我添加了L1正则化(通过交叉验证选择参数)和类加权损失损失，以帮助处理大量的特征和类不平衡。</p><p id="fe20" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated"><strong class="kj hv">结果:</strong></p><p id="75e4" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">精确度是0.71，召回率是0.51。换句话说，基于逻辑回归的Brundage机器人发了Miles发的51%的文章。该机器人发布的71%的论文实际上是由迈尔斯发布的(如果这没有意义，维基百科上有一个关于精确度和召回率的很好的<a class="ae la" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">视觉解释)。</a></p><p id="55cf" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">大约900个特征拾取非零系数(由于L1正则化，大多数为0)。以下是n元语法特征的最大系数:</p><p id="a405" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">最正的系数(迈尔斯更有可能在推特上发布论文)</p><pre class="lh li lj lk fq lo lp lq lr aw ls dt"><span id="aa01" class="jj jk hu lp b fv lt lu l lv lw">1. learning<br/>2. mdps<br/>3. neural<br/>4. adversarial<br/>5. training<br/>6. deep<br/>7. tasks<br/>8. fairness<br/>9. recurrent<br/>10. generative<br/>11. human<br/>12. answering<br/>13. dataset<br/>14. reinforcement<br/>15. rl<br/>16. artificial intelligence<br/>17. variational<br/>18. sgd<br/>19. neural networks<br/>20. dnn<br/>21. humans<br/>22. trained<br/>23. shot<br/>24. bias<br/>25. generate</span></pre><p id="bbfe" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">大多数负系数(迈尔斯不太可能在推特上发布论文)</p><pre class="lh li lj lk fq lo lp lq lr aw ls dt"><span id="545b" class="jj jk hu lp b fv lt lu l lv lw">1. investigate<br/>2. stereo<br/>3. proposed<br/>4. svm<br/>5. skin<br/>6. registration<br/>7. presented<br/>8. analysis<br/>9. lesion<br/>10. cameras<br/>11. extracted<br/>12. matching<br/>13. used<br/>14. regression<br/>15. reconstruction<br/>16. time series<br/>17. using<br/>18. breast<br/>19. nlp<br/>20. clustering<br/>21. f1<br/>22. 2d<br/>23. food<br/>24. events<br/>25. edge</span></pre><p id="6739" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">该领域的许多热门话题都出现在正系数中:强化学习、生成式对抗网络、偏见/公平、变分自动编码器。负系数似乎是非ML或应用ML论文的指标。然而，在不知道每个n元语法出现的频率以及它们之间的相关性的情况下，我们不能单独对系数进行过多的解读。</p><h2 id="12f5" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">神经网络模型</h2><p id="b3dd" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">接下来，我使用相同的特性在Keras中实现了一个基于单词的卷积神经网络。该模型为每个单词创建嵌入，然后对它们执行一维卷积和最大池运算。卷积网络在计算上是高效的(这花费了大约与逻辑回归相同的训练时间),并且在文本分类任务中表现良好。我强烈推荐<a class="ae la" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" rel="noopener ugc nofollow" target="_blank"> Denny Britz关于主题</a>的博客文章，了解这些网络如何工作以及为什么它们如此有效的细节。</p><p id="25c7" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">数据有些小且有噪音，所以我在过度拟合方面遇到了很多麻烦。我最终使用了64维嵌入、宽度从1到4个字的过滤器、非常小的12维全连接层、丢弃以及3到4个时期后的早期停止。最终模型的精度为0.70，召回率为0.60(与逻辑回归的精度大致相同，但召回率要高得多)。我们如何能使模型变得更好？</p><p id="415c" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">该模型的一个弱点是它不能准确判断申请的ML论文。在2017年10月16日那批arXiv论文中，<a class="ae la" href="https://twitter.com/BrundageBot/status/919787416461938688" rel="noopener ugc nofollow" target="_blank"> Brundage Bot忽略了Miles发推的</a>几篇应用ML的论文:<a class="ae la" href="https://arxiv.org/abs/1710.04749" rel="noopener ugc nofollow" target="_blank"> ML应用于航空安全</a>，<a class="ae la" href="https://arxiv.org/abs/1710.04743" rel="noopener ugc nofollow" target="_blank">预测哪些Kickstarter项目会按时交付</a>，以及<a class="ae la" href="https://arxiv.org/abs/1710.04689" rel="noopener ugc nofollow" target="_blank">在人群中建模注意力</a>。</p><p id="de87" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">主要问题似乎是应用<a class="ae la" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>论文的质量差异较大。例如，一篇使用<a class="ae la" href="https://hackernoon.com/tagged/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>进行皮肤病学的论文可能是一个未知学生的班级项目，或者是《自然》封面上的<a class="ae la" href="https://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="noopener ugc nofollow" target="_blank">斯坦福论文，使用n-grams很难从摘要中区分出来。另一方面，研究深度学习的数学和理论基础的论文更值得在推特上发布，也更容易通过n元语法特征区分。例如，10月16日的论文</a><a class="ae la" href="https://arxiv.org/abs/1710.04759" rel="noopener ugc nofollow" target="_blank">贝叶斯超网络</a>在其摘要中使用了短语“参数间相关的复杂多模态近似后验”。真正的迈尔斯和布伦戴奇机器人都捡起来了。</p><p id="3432" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">一个部分解决方案是将每个作者的机构隶属关系添加到模型中。除了少数例外，每个作者在数据集中只出现几次，所以作者特征太稀疏，无法提供太多信息。但DeepMind、谷歌或斯坦福等机构可能经常出现在数据中，足以产生重大影响，因此我认为加入作者关系可以提高准确性。然而，为了避免任何机构因其论文在过去被发布的频率而享有特权，将它们排除在外可能是值得的。</p><p id="0589" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">除了摘要文本之外，使用PDF或LaTeX也可能有助于找到最值得在推特上发表的论文。较高质量的论文可能平均有更多的图表和更好的排版。</p><p id="cd47" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">(感谢Miles与我讨论了他的论文选择过程，并贡献了以上见解！)</p><h2 id="4191" class="jj jk hu bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg dt translated">更雄心勃勃的想法</h2><p id="8e13" class="pw-post-body-paragraph kh ki hu kj b kk kl iv km kn ko iy kp ju kq kr ks jy kt ku kv kc kw kx ky kz hn dt translated">我对使用机器学习来策划最新的arXiv论文特别感兴趣，因为似乎没有太多工具可用。Google scholar alerts可以很好地工作，但是论文在发布后需要2-3天才能显示出来。Twitter是一个很好的来源，但很难找到与机器学习的每个子领域相关的账户(有没有按领域组织的积极发Twitter的学术研究人员的目录？).</p><p id="c8fe" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">训练模型以创建关于特定主题的论文流(例如，关于贝叶斯神经网络的所有最新论文)可能是有用的。研究人员可以手动选择一些关键字/相关论文来定义提要，模型可以寻找包含类似术语的新论文。</p><p id="e4c1" class="pw-post-body-paragraph kh ki hu kj b kk lb iv km kn lc iy kp ju ld kr ks jy le ku kv kc lf kx ky kz hn dt translated">在一个完美的世界里，我会自动拥有一份与我的兴趣相关的论文摘要。我觉得这对于用软件跟踪阅读清单的研究人员来说是可能的。我在一系列Evernote笔记本上记录我正在阅读的所有内容。如果可以创建一个管道，从Evernote中提取摘要，我就可以训练一个符合我个人偏好的模型。也有可能为<a class="ae la" href="https://www.readcube.com/papers/" rel="noopener ugc nofollow" target="_blank"> Papers.app </a>或谷歌学术创建扩展。我很想听听这听起来是否有用，或者是否有任何现有的解决方案——我是Twitter上的<a class="ae la" href="https://twitter.com/amaub" rel="noopener ugc nofollow" target="_blank"> amaub </a>，我的电子邮件是andrew.mauboussin@gmail.com。</p><figure class="lh li lj lk fq ll"><div class="bz el l di"><div class="mp ln l"/></div></figure></div></div>    
</body>
</html>