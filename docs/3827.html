<html>
<head>
<title>Neural networks from scratch for Javascript linguists (Part1 — The Perceptron)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Javascript语言学家的神经网络(第一部分——感知器)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron-632a4d1fbad2?source=collection_archive---------0-----------------------#2017-04-26">https://medium.com/hackernoon/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron-632a4d1fbad2?source=collection_archive---------0-----------------------#2017-04-26</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/75f86a9fbd3455df6bcc58c0a8437fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*iXxnl7MpExTHjVpukHPMsg.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Snakes surviving thanks to machine learning: <a class="ae jc" href="http://snakeneuralnetwork.herokuapp.com" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="1591" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">在前几个月的过程中，你很有可能听说过神经网络和人工智能，它们似乎可以实现奇迹，从<a class="ae jc" href="http://karpathy.github.io/2015/10/25/selfie/" rel="noopener ugc nofollow" target="_blank">对你的自拍进行评级</a>到<a class="ae jc" href="http://mashable.com/2014/08/18/siri-fails/#hMwSkraMiPqp" rel="noopener ugc nofollow" target="_blank"> SIRI </a>理解你的声音，在国际象棋和<a class="ae jc" href="http://www.cbc.ca/news/technology/go-google-alphago-lee-sedol-deepmind-1.3488913" rel="noopener ugc nofollow" target="_blank">围棋等游戏中击败玩家</a>，<a class="ae jc" href="https://twitter.com/goodfellow_ian/status/851124988903997440" rel="noopener ugc nofollow" target="_blank">将一匹马变成斑马</a>或者让你看起来更年轻、更老，或者从另一性别看起来更老<a class="ae jc" href="https://www.faceapp.com/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="a4d2" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">事实上，这些关键词在很多不同的上下文中被使用了很多次，使得整个“人工智能”词汇领域如此混乱，以至于它充其量只能与“看起来聪明的东西”相关联。</p><figure class="kb kc kd ke fq iv"><div class="bz el l di"><div class="kf kg l"/></div></figure><p id="015e" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">也许是因为困惑，机器学习看起来太复杂，太难掌握，就像“我打赌有太多的数学，这不是我的！!"。</p><p id="57c4" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">嗯，别担心，我也是，所以让我们开始一段<strong class="jf hv">旅程</strong>，我会告诉你我学到的一切，我的一些误解，如何解释结果，以及一些基本词汇和沿途有趣的事实。</p><h1 id="a996" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">我们在谈论什么？</h1><p id="b45a" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">想象一个<strong class="jf hv">盒子</strong>，你在里面刻上一些洞，然后扔进去一个<strong class="jf hv">预定义的</strong> <strong class="jf hv">数量</strong> <strong class="jf hv">的数字，</strong>要么是<code class="eh lk ll lm ln b">0</code>要么是<code class="eh lk ll lm ln b">1</code> <strong class="jf hv">。然后盒子剧烈震动，盒子从每个孔中喷出一个数字:<code class="eh lk ll lm ln b">0</code>或<code class="eh lk ll lm ln b">1</code>。太好了。</strong></p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lo"><img src="../Images/ecdb9b6e6916df33342344554723b378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qlYvCq6YolCNQiqdPtNdng.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Now what?</figcaption></figure><p id="8f8d" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">最初，你的盒子<strong class="jf hv">笨得要命</strong>，它不会神奇地给你预期的结果，你必须<strong class="jf hv">训练</strong>它才能达到你想要的<strong class="jf hv">目标</strong>。</p><h1 id="a11e" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">通过历史了解</h1><p id="3d41" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">我最大的错误是试图通过只看冰山一角来理解概念，玩库，当它不起作用时就抓狂。有了神经网络，你真的负担不起。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lp"><img src="../Images/b13665abe7a164d98809e7ca70fc885d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/1*EQfIFkckck1Cj1TNLDXigQ.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Time to go back</figcaption></figure><p id="90d8" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">这种奇妙的纸板奶酪的发明起源于1943年左右，当时45岁的神经生理学家沃伦·斯特吉斯·麦卡洛克和他的同事沃尔特·皮茨写了一篇论文，名为:“<a class="ae jc" href="http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf" rel="noopener ugc nofollow" target="_blank">对神经活动</a>中固有思想的逻辑演算”。</p><p id="eac1" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">为了追寻古希腊古典哲学家的探索，他试图用数学方法<strong class="jf hv">模拟大脑的工作方式</strong>。</p><p id="8e42" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">考虑到我们对神经元的了解有多长时间(大约1900年)，以及神经元信号交流的电特性在20世纪50年代末之前不会被证明，这确实是非常光明的。</p><p id="99d5" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">让我们在这里花一点时间记住，发表的论文并不意味着写这篇论文的人绝对<strong class="jf hv">正确</strong>，它意味着这个家伙:<strong class="jf hv">有一个假设</strong>，在这个领域知道一点，有某种结果，有时不适用并发表了它。<br/>然后该领域的其他合格人员必须尝试，<strong class="jf hv">复制结果</strong>并决定它是否是一个很好的基础。</p><p id="c552" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">当任其发展，玩弄<strong class="jf hv"> p-hacking </strong>时，一些论文会产生一些异常，就像2011年的这篇论文所说的<a class="ae jc" href="https://www.youtube.com/watch?v=42QuXLucH3Q" rel="noopener ugc nofollow" target="_blank">人们可以在未来看到的</a>。</p><p id="a040" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">然而，我们现在知道我们的<a class="ae jc" href="https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch" rel="noopener ugc nofollow" target="_blank">麦卡洛克</a>是一个好人，在他的论文中，他试图模仿物理大脑的一些算法，物理大脑由<strong class="jf hv">神经系统</strong>(大多数多细胞动物都有)组成，实际上是一个由<strong class="jf hv">神经元组成的网络。</strong></p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lq"><img src="../Images/b6d5b6f746251804e4858d495e5143a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*NORBXIXJ54IckCJDfcc9Og.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Hoping you won’t lose too many of these reading the following. (<a class="ae jc" href="https://www.youtube.com/watch?v=qPix_X-9t7E" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="a469" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">人类大脑有~ 860亿个这样的家伙，每个都有<strong class="jf hv">轴突</strong>和<strong class="jf hv">树突和突触</strong>，它们将<strong class="jf hv">每个</strong>神经元<strong class="jf hv"> </strong>连接到<strong class="jf hv"> ~7000 </strong>其他<strong class="jf hv">。</strong>几乎和宇宙中<a class="ae jc" href="http://www.esa.int/Our_Activities/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe" rel="noopener ugc nofollow" target="_blank">个星系</a> <strong class="jf hv">一样多的连接。<br/> <br/> </strong>既然我们不擅长形象化大数字，下面就来看看如何看待$1B:</p><figure class="kb kc kd ke fq iv"><div class="bz el l di"><div class="lr kg l"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Now imagine 86 times that</figcaption></figure><p id="28f3" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">麦卡洛克的问题是，1943年的经济环境并不繁荣:我们处于战争中，富兰克林·罗斯福冻结了价格、薪水和工资以防止通货膨胀，尼古拉·特斯拉去世了，当时最好的电脑是<a class="ae jc" href="https://en.wikipedia.org/wiki/ENIAC" rel="noopener ugc nofollow" target="_blank"> ENIAC </a>，它价值700万美元，售价<a class="ae jc" href="http://img04.deviantart.net/73c7/i/2011/249/1/f/your_mom_by_khelsiieexkhaotika-d4943v7.jpg" rel="noopener ugc nofollow" target="_blank"> 30吨</a>(科技领域的性别歧视相当猖獗(呃？)当时以及ENIAC是<strong class="jf hv"> 6女</strong> 发明的<a class="ae jc" href="http://blogs.smithsonianmag.com/smartnews/2013/10/computer-programming-used-to-be-womens-work/" rel="noopener ugc nofollow" target="_blank">这个事实导致她们的男同事错误的低估了它)。<br/>作为对比，2005年的标准三星翻盖手机拥有<strong class="jf hv">1300倍</strong>的ENIAC计算能力<strong class="jf hv">。</strong></a></p><p id="5e05" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">然后在1958年，计算机做得稍好一点<strong class="jf hv">、</strong>和<strong class="jf hv">弗兰克·罗森布拉特</strong>，受麦卡洛克的启发，给了我们<a class="ae jc" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv">感知器</strong> </a>。</p><p id="1d18" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">每个人都乐于深入挖掘，直到11年后，马文·明斯基(Marvin Minsky)决定他不喜欢那个想法(T30)，弗兰克·感知器(Frank Perceptron)不适合这份工作，他在出版的一本书中解释说:“(T31)罗森布拉特的大部分著作……没有科学价值……(T32)。”这本书的影响在于，它耗尽了该领域本已很低的资金。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lt"><img src="../Images/a45e4c607a1ff525f415187def0048c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/1*HSUPKzfZtlBpAgvkQZ752w.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Classic Minsky.</figcaption></figure><p id="535e" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">明斯基为什么反对麦卡洛克学徒？</p><h2 id="b89a" class="lu ki hu bd kj lv lw lx kn ly lz ma kr jo mb mc kv js md me kz jw mf mg ld mh dt translated">线性难题</h2><p id="262b" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">虽然这听起来像是<em class="ls">生活大爆炸</em>的剧集标题，但它实际上代表了明斯基理论的基础，以偏离最初的感知机。</p><p id="6429" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">Rosenblatt感知器看起来与我们的盒子非常相似，这次我们在盒子里<strong class="jf hv">钻了一个单孔</strong>:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lo"><img src="../Images/5bf5368143c6d1ae5394c2fe7b32b41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bj6CgH_GSUsBCRpS_N6XiQ.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">A Neural Network can actually take inputs <strong class="bd mi"><em class="mj">between</em></strong> 0 and 1</figcaption></figure><p id="efe8" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">如果我们的<strong class="jf hv">输入信号(x1…x4) </strong>乘以它们各自的<strong class="jf hv">权重(w1…w4) </strong>加上<strong class="jf hv">偏置(b) </strong>之和足以使<strong class="jf hv">结果</strong>门高于<strong class="jf hv">阈值(T) </strong>，否则，我们的门将释放值<code class="eh lk ll lm ln b">1</code>，<code class="eh lk ll lm ln b">0</code>。</p><p id="61c0" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">为此，将阈值与<a class="ae jc" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv">激活功能</strong> </a> <strong class="jf hv">的结果进行比较。</strong>就像大脑神经元对<strong class="jf hv">刺激</strong>的反应一样。如果刺激太低，神经元就不会沿着轴突向树突发出信号。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/4ac4e253a1cebafe74e51a17ff9f43fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*yMBxDghV-iI7M9ABmXeY1g.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Depolarization of a neuron thanks to the magic of the <a class="ae jc" href="https://www.youtube.com/watch?v=OZG8M_ldA1M" rel="noopener ugc nofollow" target="_blank">sodium-potassium pump</a></figcaption></figure><p id="64b0" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">就代码而言，它在全局看起来是这样的:</p><pre class="kb kc kd ke fq ml ln mm mn aw mo dt"><span id="47a4" class="lu ki hu ln b fv mp mq l mr ms">// Defining the inputs<br/>const x1 = 1;<br/>const x2 = 0.3;<br/>const x3 = 0.2;<br/>const x4 = 0.5;</span><span id="4609" class="lu ki hu ln b fv mt mq l mr ms">// Defining the weights<br/>const w1 = 1.5;<br/>const w2 = 0.2;<br/>const w3 = 1.1;<br/>const w4 = 1.05;<br/></span><span id="0fde" class="lu ki hu ln b fv mt mq l mr ms">const Threshold = 1;<br/>const bias = 0.3;</span><span id="48fc" class="lu ki hu ln b fv mt mq l mr ms">// The value evaluated against the threshold is the sum of the<br/>// inputs multiplied by their weights<br/>// (1*1.5)+(.3*0.2)+(.2*1.1)+(.5*1.05)</span><span id="f3c0" class="lu ki hu ln b fv mt mq l mr ms">const sumInputsWeights = x1*w1 + x2*w2 + x3*w3 + x4*w4; // 2.305<br/>const doorWillOpen = activation(sumInputsWeights + bias) &gt; Threshold; // true</span></pre><p id="c4af" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">在人体中，神经元的电关闭状态为-70mV，当达到-55mV时，其激活阈值为。</p><p id="c973" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">在原始感知器的情况下，该激活由<strong class="jf hv"> Heaviside阶跃函数</strong>处理。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/11e37448cf9420db3b07526211dfd80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*sG_Yqs9TH-v8gEToNU0LoA.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek"><strong class="bd mi">0</strong> if x is negative, <strong class="bd mi">1</strong> if x is null or positive. x being the sumInputsWeights+bias.</figcaption></figure><p id="cbbe" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">其中最广为人知的激活函数是<strong class="jf hv"> sigmoid函数:<br/> </strong> <code class="eh lk ll lm ln b"><strong class="jf hv">f(x) = 1 / (1 + exp(-x))</strong></code> <strong class="jf hv"> </strong>和<strong class="jf hv"> bias </strong>一般用于<strong class="jf hv"> shift </strong> <strong class="jf hv">其激活阈值</strong>:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/ab3ebb5058833049e43c1333fb30d272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*KVPmkQUZrWKe8loTfwMwyA.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Tweaking the activation function can yield to adjusted results altering when the neuron fires</figcaption></figure><p id="2203" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">一些激活函数允许输出负值，一些不允许。当使用一个感知器的激活函数的结果作为另一个感知器的输入时，这将证明它是重要的。</p><p id="2592" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">使用<code class="eh lk ll lm ln b">0</code>作为输入总是抑制其相关权重，导致其连接在总和中不存在。</p><p id="3663" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">因此，它的作用就像是一张弃权票，而不是有降低权重的可能性。然而，有时我们可能希望<code class="eh lk ll lm ln b">0</code>作为一个输入，是对相反候选人的投票。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="fe ff mw"><img src="../Images/8f3d08d5549c9d66e1d12a2727d54916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dq-QWOg88oDkgMKBeCujTg.png"/></div></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Many other activations functions <a class="ae jc" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">exist</a></figcaption></figure><p id="1823" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">感知器被称为<strong class="jf hv">二元分类器</strong>意思是<strong class="jf hv"> </strong>它只能在<strong class="jf hv"> 2 </strong> <strong class="jf hv">选项之间进行分类(</strong>垃圾邮件对非垃圾邮件，橙子对非橙子…等等)</p><p id="651c" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">它也被称为<a class="ae jc" href="https://en.wikipedia.org/wiki/Linear_classifier" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv">线性分类器</strong> </a> <strong class="jf hv"> </strong>意为<strong class="jf hv"> </strong>它的目标是<strong class="jf hv">根据一个对象的<strong class="jf hv">特征</strong>(或“特征”:我们的<strong class="jf hv"> x1到x4</strong>)<strong class="jf hv"/>来识别该对象属于</strong>的哪一类，通过迭代直到它找到<strong class="jf hv">一条单独的线</strong>来正确地将<strong class="jf hv">与</strong>实体从每一个中分离开来</p><p id="dfe8" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">给定一些输入，我们给我们的分类器一些预期结果的例子，它将<strong class="jf hv">训练自己</strong>找到这种分离，通过调整<strong class="jf hv">权重</strong>分配给每个输入<strong class="jf hv"> </strong>和它的<strong class="jf hv">偏差</strong>。</p><p id="ea81" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">作为一个例子，让我们根据两个特征将一些实体分类在“<strong class="jf hv">友好或不友好</strong>”之间:使用感知器的<strong class="jf hv">牙齿</strong>和<strong class="jf hv">大小</strong></p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/8d88adbeab1fe7010557f599c463f11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*QEwusFfiilfoeeNNLXFygg.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Depending on the sources the cat seems very borderline indeed</figcaption></figure><p id="c486" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">现在我们训练了我们的感知机，我们可以预测以前从未见过的样本:</p><p id="d1a2" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated"><strong class="jf hv"><em class="ls"/></strong><a class="ae jc" href="https://rosenblattperceptron.herokuapp.com" rel="noopener ugc nofollow" target="_blank"><strong class="jf hv"><em class="ls">这里的</em> </strong> </a> <strong class="jf hv"> <em class="ls">为现场直播</em> </strong> <a class="ae jc" href="https://rosenblattperceptron.herokuapp.com" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv"> <em class="ls">演示</em></strong><strong class="jf hv"><em class="ls">。不，说真的，检查一下。</em> </strong></a></p><p id="857c" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">明斯基对罗森布拉特的指责是这样的:如果我的训练器械中突然多了一条<strong class="jf hv">巨蛇</strong>，它几乎没有牙齿，但却和大象一样大。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff nc"><img src="../Images/46be2f59e4ad9d25cfcccf40a1cd34c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*izrfOXRu3B1dL2Ys5fBZkg.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">The training set is not separable by one line anymore</figcaption></figure><p id="ea01" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">现在需要<strong class="jf hv"> 2 </strong>条线来正确区分红色实体和绿色实体。使用单个感知器，这种不可能性将导致感知器尝试并永远运行，无法用<strong class="jf hv">单直线</strong> <strong class="jf hv">线</strong>进行分类。</p><p id="4a12" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">到现在为止，你应该能够处理感知器的通常表示:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/2d18213344dcff0794f12fca002e655c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*K_-erXb47r6CeoJU61Yg6w.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">The bias can be applied either after the sum or as an added weight for a fictional input being always 1</figcaption></figure><h2 id="953d" class="lu ki hu bd kj lv lw lx kn ly lz ma kr jo mb mc kv js md me kz jw mf mg ld mh dt translated">解决双线性问题:</h2><p id="d38c" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">解决这个问题的一个方法是简单地训练两个感知器，一个负责左上分离，另一个负责右下分离，我们<strong class="jf hv">用一个<strong class="jf hv">和</strong>规则把它们</strong>插在一起，创建某种<a class="ae jc" href="https://en.wikipedia.org/wiki/Perceptron#Multiclass_perceptron" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv">多类感知器</strong> </a>。</p><pre class="kb kc kd ke fq ml ln mm mn aw mo dt"><span id="c4a9" class="lu ki hu ln b fv mp mq l mr ms">const p1 = new Perceptron();<br/>p1.train(trainingSetTopLeft);<br/>p1.learn(); // P1 ready.</span><span id="6329" class="lu ki hu ln b fv mt mq l mr ms">const p2 = new Perceptron();<br/>p2.train(trainingSetBottomRight);<br/>p2.learn(); // P2 ready.</span><span id="8666" class="lu ki hu ln b fv mt mq l mr ms">const inputs = [x1,x2];</span><span id="30c8" class="lu ki hu ln b fv mt mq l mr ms">const result = p1.predict(inputs) &amp; p2.predict(inputs);</span></pre><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="fe ff ne"><img src="../Images/08e8ec37e06505fd88ded1868c365546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WgDiZ2uR1H0JWFznDuLaQ.png"/></div></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">There are other ways to solve this that we’ll explore in part 2</figcaption></figure><p id="ac50" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">“与”运算又名“A^B”是感知器可以执行的逻辑运算的一部分，通过将<strong class="jf hv"> w5 </strong>和<strong class="jf hv"> w6 </strong>置于<code class="eh lk ll lm ln b">0.6</code>附近，并对我们的和应用偏差<code class="eh lk ll lm ln b">-1</code>，我们确实创建了一个“与”预测器。<br/>记住与w5和w6相连的输入来自Heaviside阶跃函数的激活，仅产生<code class="eh lk ll lm ln b">0</code>或<code class="eh lk ll lm ln b">1</code>作为输出。</p><pre class="kb kc kd ke fq ml ln mm mn aw mo dt"><span id="5c56" class="lu ki hu ln b fv mp mq l mr ms">For 0 &amp; 0 : (0*0.6 + 0*0.6)-1 is &lt;0, Output: 0<br/>For 0 &amp; 1 : (0*0.6 + 1*0.6)-1 is &lt;0, Output: 0<br/>For 1 &amp; 1 : (1*0.6 + 1*0.6)-1 is &gt;0, Output: 1</span></pre><p id="2621" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">通过这种链接和“拉长”我们的原始感知机，我们创造了所谓的<strong class="jf hv">隐藏层，</strong>基本上是一些神经元插在原始输入和最终输出之间。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/3e95954e40c955f5f691db199bfcbc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*FibRm8jGbHO8hlwGOyAmBQ.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">They should really be called “<a class="ae jc" href="https://www.cs.cmu.edu/~dst/pubs/byte-hiddenlayer-1989.pdf" rel="noopener ugc nofollow" target="_blank">feature detectors layers</a>” or “sub problem layers”</figcaption></figure><p id="847b" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">最糟糕的是<strong class="jf hv">明斯基知道这一切</strong>仍然决定专注于最简单版本的感知机来唾弃罗森布拉特的所有工作。罗森布拉特早在1962年就已经在他自己的书<em class="ls">神经动力学原理:感知器和大脑机制理论</em>中介绍了多层&amp;交叉耦合感知器。</p><p id="f79b" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">这有点像出版一本名为<em class="ls">晶体管的书，</em>表示其中单个一文不值，从不承认计算机。</p><h2 id="4386" class="lu ki hu bd kj lv lw lx kn ly lz ma kr jo mb mc kv js md me kz jw mf mg ld mh dt translated">但是感知器线从何而来？</h2><p id="9200" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">下面这些方程你们应该都很熟悉:<code class="eh lk ll lm ln b">y=f(x)<br/>y = mx + c</code>或者<code class="eh lk ll lm ln b">y = ax + b</code>。那么我们如何从我们训练过的感知机中找到这个<code class="eh lk ll lm ln b">m</code>和<code class="eh lk ll lm ln b">c</code>？</p><p id="1be7" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">为了解决这个问题，我们必须记住我们感知机的原始方程:<code class="eh lk ll lm ln b">x1*w1 + x2*w2 + bias &gt; T</code></p><p id="8703" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">偏差和阈值是<a class="ae jc" href="http://stackoverflow.com/questions/16609310/in-neural-networks-does-a-bias-change-the-threshold-of-an-activation-function" rel="noopener ugc nofollow" target="_blank">相同的概念</a>，对于感知器<code class="eh lk ll lm ln b">T = 0</code>来说，这意味着等式变成了<code class="eh lk ll lm ln b">x1*w1 +x2*w2 &gt; -bias</code>，可以改写为:<br/> <code class="eh lk ll lm ln b">x2 &gt; (-w1/w2)*x1 + (-bias/w2)</code>将此与:<br/> <code class="eh lk ll lm ln b">y = m*x + b</code>进行比较，我们可以看到</p><p id="11bd" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated"><code class="eh lk ll lm ln b">y</code>代表<code class="eh lk ll lm ln b">x2<br/>x</code>代表<code class="eh lk ll lm ln b">x1<br/>m</code>代表<code class="eh lk ll lm ln b">(-w1/w2)</code>，而<code class="eh lk ll lm ln b">b</code>代表<code class="eh lk ll lm ln b">(-bias/w2)</code></p><p id="e3d3" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">也就是说我们线的<strong class="jf hv">坡度(m) </strong>由2个权值决定，线与垂直轴相交的<strong class="jf hv">位置由偏差和第2个权值决定。</strong></p><p id="7780" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">我们现在可以简单地选择两个<code class="eh lk ll lm ln b">x</code>值(0和1)，在线方程中替换它们并找到相应的<code class="eh lk ll lm ln b">y</code>值，然后在两点之间画一条线。<code class="eh lk ll lm ln b">(0;f(0))</code>和<code class="eh lk ll lm ln b">(1;f(1))</code>。</p><p id="556e" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">就像一个好的犯罪现场一样，使用等式可以让我们进行一些观察:</p><pre class="kb kc kd ke fq ml ln mm mn aw mo dt"><span id="7ddf" class="lu ki hu ln b fv mp mq l mr ms">y = (-w1/w2)x + (-bias/w2)</span></pre><p id="e3e0" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">线的<strong class="jf hv">坡度(陡度)</strong>仅取决于两个权重<br/><code class="eh lk ll lm ln b">w1</code>前面的<strong class="jf hv">减</strong>符号意味着如果两个权重符号相同，线将像<code class="eh lk ll lm ln b">\</code>一样向下倾斜，如果它们完全不同，线将向上倾斜<code class="eh lk ll lm ln b">/<br/></code>调整<strong class="jf hv"> w1 </strong>将影响陡度，但不影响它与垂直轴相交的位置， <strong class="jf hv"> w2而不是</strong>将对<strong class="jf hv">和<br/> </strong>都有影响，因为<strong class="jf hv">偏差是分子</strong>(分数的上半部分)，增加它将把线推到图中更高的位置。 (将在更高的点切割垂直轴)</p><p id="4b37" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">您可以通过输入<code class="eh lk ll lm ln b">app.perceptron.weights</code>在<a class="ae jc" href="https://rosenblattperceptron.herokuapp.com" rel="noopener ugc nofollow" target="_blank">演示</a>的控制台中检查最终重量</p><p id="4ac7" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">这正是我们的感知机试图优化的，直到它找到正确的线</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/214acf5daac66066c95c46259285f5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/1*1hQqAC67zmS9nomTuxa7Uw.gif"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">I know it might look like 2 lines but it’s really one moving super fast</figcaption></figure><h2 id="1c7b" class="lu ki hu bd kj lv lw lx kn ly lz ma kr jo mb mc kv js md me kz jw mf mg ld mh dt translated">你说的“优化”是什么意思？</h2><p id="837f" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">正如你所猜测的，我们的感知器不能盲目地猜测和尝试它的权重的每个值，直到它找到一条正确分隔实体的线。我们要应用的是<a class="ae jc" href="https://en.wikipedia.org/wiki/Delta_rule" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hv">德尔塔法则</strong> </a> <strong class="jf hv">。</strong></p><p id="cf9d" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">这是一个学习规则，用于更新与单层神经网络中的输入相关联的权重，表示如下:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff nh"><img src="../Images/527d17e773263fa3ae621df70c1839d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*Ba8c5NlO8VkCu9t9go-28Q.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Oh god math!</figcaption></figure><p id="8f95" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">别担心，它可以这样“简化”:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="fe ff ne"><img src="../Images/165d12fc148b55512c650a81b1a7fe8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BCDXvKT7TCVhqeBhWfinA.png"/></div></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">We do this for every item in the training set.</figcaption></figure><p id="4a3d" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">做<code class="eh lk ll lm ln b">expected — actual</code>代表一个<strong class="jf hv">误差</strong>值(或成本)。目标是遍历训练集，并通过在每个输入的权重上增加或减去少量的值，将这种误差/成本降低到最小，直到它验证了所有的训练集预期。</p><p id="2871" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">如果在一些迭代之后，对于训练集中的每个项目，误差<strong class="jf hv">为<strong class="jf hv"> </strong> <code class="eh lk ll lm ln b"><strong class="jf hv">0</strong></code> <strong class="jf hv"> </strong>。我们的感知器被训练，使用这些最终权重的线方程正确地分成两部分。</strong></p><h2 id="ed29" class="lu ki hu bd kj lv lw lx kn ly lz ma kr jo mb mc kv js md me kz jw mf mg ld mh dt translated">当心学习速度</h2><p id="797b" class="pw-post-body-paragraph jd je hu jf b jg lf ji jj jk lg jm jn jo lh jq jr js li ju jv jw lj jy jz ka hn dt translated">在上面的等式中，<strong class="jf hv"> α </strong>代表一个常数:学习率，它将对每个权重的改变量产生影响。</p><p id="43bf" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">如果<strong class="jf hv"> α </strong>太小，将需要<strong class="jf hv">比需要</strong>更多的迭代来找到正确的权重，你可能会陷入局部最小值。<br/>如果<strong class="jf hv"> α </strong>太大<strong class="jf hv">学习可能<strong class="jf hv">永远找不到</strong>一些<strong class="jf hv"> </strong>正确的权重。</strong></p><p id="0537" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">一种理解方式是想象一个穿着金属靴的可怜人想要到达悬崖底部的宝藏，但是他只能通过跳跃米来移动:</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lo"><img src="../Images/6bbed7fbdfdc7a5e48945bde8c832c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c__J4PZOhc6NV5W7hv3R4w.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek"><strong class="bd mi">α</strong> too big</figcaption></figure><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff lo"><img src="../Images/6563c21273686f9e63fe8bbbba153e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LUFC0ug0FEOB0HPJBuTcA.png"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek"><strong class="bd mi">α</strong> too small</figcaption></figure><p id="2e46" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">当你在维基百科上阅读一篇文章时，你可能想做的一件事是前往“<em class="ls"> Talk </em>”部分，该部分讨论了内容中有争议的领域。</p><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div class="fe ff ni"><img src="../Images/12b78d4b3b6f925878be48dc9f5f2349.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*hHnczTt0uvp4g6prYefCNA.png"/></div></figure><p id="4710" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">在德尔塔公式的情况下，<em class="ls">内容</em>说它不能应用于感知器，因为亥维赛导数在<code class="eh lk ll lm ln b">0</code>不存在，但是<em class="ls">谈话部分</em>提供了麻省理工学院教师使用它的文章。</p><p id="7946" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">通过把我们学到的所有东西放进去，我们最终编码了一个感知机:</p><figure class="kb kc kd ke fq iv"><div class="bz el l di"><div class="nj kg l"/></div><figcaption class="iy iz fg fe ff ja jb bd b be z ek">Can’t be used as is. Actual code <a class="ae jc" href="https://github.com/Elyx0/rosenblattperceptronjs/blob/master/src/Perceptron.js" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure></div><div class="ab cl nk nl hc nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="hn ho hp hq hr"><p id="9281" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">此外，感知器属于<strong class="jf hv">前馈神经网络</strong>类别，这只是一种花哨的措辞，表示单元之间的连接不形成循环。</p><p id="da24" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">为了对Minsky宽容一些，虽然感知器算法在线性可分离训练集的情况下保证收敛于<em class="ls">某个</em>解，但是它仍然可以挑选<em class="ls">任何有效的</em>解，并且一些问题可能允许许多不同质量的解。</p><p id="fbf9" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">一旦你的大脑找到了与肌肉对话的正确方式(即:肌肉做出正确的反应)，它就会满足于此。这个大脑到肌肉的代码每个人都不一样！</p><p id="f1e4" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">知识分子喜欢像明克斯和罗森布拉特那样一直争论不休。甚至<strong class="jf hv">爱因斯坦</strong>在<a class="ae jc" href="https://en.wikipedia.org/wiki/Bohr%E2%80%93Einstein_debates" rel="noopener ugc nofollow" target="_blank">量子不确定论</a>上与<strong class="jf hv">尼尔斯·玻尔</strong>的斗争中也被证明是错误的，他反驳了爱因斯坦的名言:“<em class="ls">上帝不和宇宙</em>玩骰子”。</p><p id="cb04" class="pw-post-body-paragraph jd je hu jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hn dt translated">我们将以神经网络之父我们亲爱的沃伦麦卡洛克的一些诗歌来结束(<em class="ls">他</em> <a class="ae jc" href="https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch#Biography" rel="noopener ugc nofollow" target="_blank"> <em class="ls">真的</em> </a> <em class="ls">是个诗人</em>)。我希望你学到了一些东西，我们将在第二部分再见。</p><blockquote class="nr"><p id="04e3" class="ns nt hu bd nu nv nw nx ny nz oa ka ek translated">我们对世界的认识，包括我们自己，就空间而言是不完整的，就时间而言是不确定的。这种隐含在我们所有人大脑中的无知，是使我们的知识变得有用的抽象的对应物。</p></blockquote><div class="ob oc od oe of ab cb"><figure class="og iv oh oi oj ok ol paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="og iv oh oi oj ok ol paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="og iv oh oi oj ok ol paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="om on oo"><p id="f922" class="jd je ls jf b jg jh ji jj jk jl jm jn op jp jq jr oq jt ju jv or jx jy jz ka hn dt translated"><a class="ae jc" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae jc" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae jc" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae jc" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jd je ls jf b jg jh ji jj jk jl jm jn op jp jq jr oq jt ju jv or jx jy jz ka hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jc" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jc" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="kb kc kd ke fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="fe ff os"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure></div></div>    
</body>
</html>