<html>
<head>
<title>Gradient Clipping</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">渐变剪辑</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/gradient-clipping-57f04f0adae?source=collection_archive---------6-----------------------#2017-11-27">https://medium.com/hackernoon/gradient-clipping-57f04f0adae?source=collection_archive---------6-----------------------#2017-11-27</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="c790" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">你可以在推特@bhutanisanyam1上找到我</p></blockquote><p id="b9e0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在深度<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>模型的“训练”过程中，我们通过网络的层反向传播我们的梯度。</p><p id="bf95" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在<a class="ae jt" href="https://hackernoon.com/tagged/experimentation" rel="noopener ugc nofollow" target="_blank">实验</a>期间，一旦梯度值变得非常大，就会导致溢出(即NaN)，这在运行时很容易检测到，或者在不太极端的情况下，模型开始超过我们的最小值；这个问题叫做<strong class="ix hv">渐变爆炸问题</strong>。</p><p id="4a70" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这是指它们与大于1的数相乘而成指数级增长的情况，例如:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div class="fe ff jx"><img src="../Images/8c3a3a5b0af2f967b1b5dd981f61ae2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*CnN8mq74Vnx0DyCVysVJOA.png"/></div><figcaption class="kf kg fg fe ff kh ki bd b be z ek">Source: Hinton’s Coursera Lecture Videos.</figcaption></figure><p id="aac7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">渐变裁剪将“裁剪”渐变或将其限制在某个阈值，以防止渐变变得过大。在上面的图像中，梯度被从过冲中截取，我们的成本函数遵循虚线值，而不是它的原始轨迹。</p></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><h1 id="a16a" class="kq kr hu bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">L2范数剪裁</h1><p id="d36b" class="pw-post-body-paragraph iu iv hu ix b iy lo ja jb jc lp je jf ju lq ji jj jv lr jm jn jw ls jq jr js hn dt translated">存在各种方式来执行梯度裁剪，但是一种常见的方式是当参数向量的L2范数超过某个阈值时归一化参数向量的梯度:</p><pre class="jy jz ka kb fq lt lu lv lw aw lx dt"><span id="189a" class="ly kr hu lu b fv lz ma l mb mc">new_gradients = gradients * threshold / l2_norm(gradients)<br/></span></pre><p id="82b0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们可以在Tensorflow中使用函数来实现这一点</p><pre class="jy jz ka kb fq lt lu lv lw aw lx dt"><span id="5706" class="ly kr hu lu b fv lz ma l mb mc">tf.clip_by_norm(t, clip_norm, axes=None, name=None)</span></pre><p id="8787" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这将使<em class="iw"> t </em>标准化，从而使其<em class="iw"> L2范数</em>小于或等于<em class="iw"> clip_norm </em></p><p id="69cc" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">此操作通常用于在使用优化器应用渐变之前对其进行裁剪。</p><blockquote class="ir is it"><p id="5f3f" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在twitter @bhutanisanyam1上找到我</a></p><p id="a8fb" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://tinyletter.com/sanyambhutani" rel="noopener ugc nofollow" target="_blank">订阅我的时事通讯，获取深度学习、计算机视觉文章的每周精选列表</a></p><p id="fd21" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://becominghuman.ai/a-self-driving-new-year-33284e592f35" rel="noopener ugc nofollow" target="_blank">这里的</a>和<a class="ae jt" href="https://hackernoon.com/a-self-driving-new-year-2-d1bbc5a83570" rel="noopener ugc nofollow" target="_blank">这里的</a>是我学习自动驾驶汽车道路上的两篇文章</p></blockquote><figure class="jy jz ka kb fq kc"><div class="bz el l di"><div class="md me l"/></div></figure></div></div>    
</body>
</html>