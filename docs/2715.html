<html>
<head>
<title>Deep Learning for Noobs [Part 2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向新手的深度学习[第二部分]</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deep-learning-for-noobs-part-2-43d5098e61f6?source=collection_archive---------1-----------------------#2017-02-14">https://medium.com/hackernoon/deep-learning-for-noobs-part-2-43d5098e61f6?source=collection_archive---------1-----------------------#2017-02-14</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/6331135e27796eb3d239d53742c7982d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rL1yoYgk66hhHJmdwTVAog.jpeg"/></div></div></figure><div class=""/><p id="5420" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如何建立自己的卷积神经网络？让我们在这篇文章中尝试解决这个问题。我们将研究一个图像分割问题，我在这个系列的第一部分中讨论过这个问题。</p><p id="7001" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">有很多库可用于创建卷积神经网络。我们将选择<a class="ae ka" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae ka" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>。我想到的第一个问题是:</p><blockquote class="kb kc kd"><p id="2cba" class="jc jd ke je b jf jg jh ji jj jk jl jm kf jo jp jq kg js jt ju kh jw jx jy jz hn dt translated">为什么是这两个？为什么不只是Tensorflow？</p></blockquote><p id="56df" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在机器学习库领域，有很多库。Tensorflow、Theano、PyTorch、Caffe和Torch是其中比较著名的几个。由Soumith Chintala 和团队向PyTorch 大声喊出来。你们创造了一个很棒的图书馆。希望你们能接管这个世界。</p><figure class="kl km kn ko fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff kk"><img src="../Images/4dfcf586ea4def1ea37b285286c5c50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kok8tc3A2s8UP2QTrZ7RlA.png"/></div></div><figcaption class="kp kq fg fe ff kr ks bd b be z ek">PyTorch planning to take over the world :P <a class="ki kj gr" href="https://medium.com/u/ac9d9a35533e?source=post_page-----43d5098e61f6--------------------------------" rel="noopener" target="_blank">Andrej Karpathy</a> has high hopes for Tensorflow</figcaption></figure><p id="ec7b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">都是低级别的库。涉及GPU或CPU加速和矩阵计算优化。所以用它们来建立网络可能会变得很有挑战性。Keras是一个高级的库。它帮助你创造神经元层。它抽象了实现计算的所有复杂性。Keras使用Theano或Tensorflow作为后端。我选择Tensorflow作为后端，因为它有更好的社区支持。</p><blockquote class="kb kc kd"><p id="7e50" class="jc jd ke je b jf jg jh ji jj jk jl jm kf jo jp jq kg js jt ju kh jw jx jy jz hn dt translated">喀拉斯&amp;张量流(KETE)组合岩石。</p></blockquote><h1 id="e512" class="kt ku if bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq dt translated">装置</h1><p id="fc0b" class="pw-post-body-paragraph jc jd if je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">让我们把手弄脏。不要去想哪里可以做。你的常规系统在训练数据集的时候会死掉。因此，让我们得到一个AWS服务器。如果你有一个疯狂的游戏装备，那么你可以在本地随意设置。我们将使用AWS的g 2.2x大型系统。它有26个GPU核心，成本为0.65美元/小时。我们为什么选择它。这是因为这是云上最便宜的GPU系统。它将比我们家现有的大多数硬件性能更好。接下来是使用哪个操作系统。使用Ubuntu 16.04 LTS肯定是有意义的，但是等一下。我们将使用预烘焙的AMI，它内置了许多工具。这样我们可以省去大部分的设置。从AWS搜索深度学习AMI。还有其他关于深度学习的好的ami，请随意探索。我们需要至少在ami中安装Python 2.7和Tensorflow。</p><div class="kl km kn ko fq ab cb"><figure class="lw hw lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/50ced81a5092995492c2fc6a2008b336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*mc1KOPU7j5eUowBdvg2iPw.png"/></div></figure><figure class="lw hw mc ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/b70b8d8dd3ce7d4f360fe777957687cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vVfo8m7-cKD8sUbuz65-_Q.png"/></div><figcaption class="kp kq fg fe ff kr ks bd b be z ek md di me mf">GPU instance and Deep Learning AMI on AWS</figcaption></figure></div><p id="e359" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在选择了实例类型和AMI之后，继续创建一个键。如果您已经有一个预置的密钥，您可以使用它。对于本文，我们将创建一个。假设密钥文件的名称是deepkey.pem。下载密钥并将其安全保存在某个地方。启动实例。创建实例大约需要5- 10分钟。同时，将该项的权限更改为400。否则ssh不会让您登录。</p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="e8c4" class="ml ku if mh b fv mm mn l mo mp">chmod 400 ~/deepkey.pem</span></pre><p id="4afa" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">接下来转到EC2实例的列表视图。从那里选择创建的实例。复制AWS实例公共DNS。它看起来会像这样<strong class="je ig">ec2–52–24–183–62.us-west-2.compute.amazonaws.com</strong></p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="4320" class="ml ku if mh b fv mm mn l mo mp"><strong class="mh ig"># Next lets login to the system<br/></strong>ssh ec2-user@ec2–52–24–183–62.us-west-2.compute.amazonaws.com -i ~/deepkey.pem</span><span id="19b3" class="ml ku if mh b fv mq mn l mo mp"><strong class="mh ig"># The AMI might be a bit backdated, so it's always better to update</strong><br/>sudo yum update</span><span id="2c20" class="ml ku if mh b fv mq mn l mo mp"><strong class="mh ig"># Install pip to get Keras</strong><br/>sudo yum install python-pip</span><span id="322d" class="ml ku if mh b fv mq mn l mo mp"><strong class="mh ig"># Upgrade the pip master that got installed<br/></strong>sudo /usr/local/bin/pip install — upgrade pip</span><span id="0bd8" class="ml ku if mh b fv mq mn l mo mp"><strong class="mh ig"># Install Keras</strong><br/>sudo /usr/local/bin/pip install keras</span></pre><p id="8b23" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">默认情况下，Keras安装时将Theano作为基本配置。我们将使用张量流。所以让我们改变这一点。打开<strong class="je ig"> ~/。keras/keras.conf </strong>并更新如下图所示。该文件应该类似于下面的部分。</p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="b456" class="ml ku if mh b fv mm mn l mo mp"><strong class="mh ig">{<br/>“image_dim_ordering”: “tf”,<br/>“epsilon”: 1e-07,<br/>“floatx”: “float32”,<br/>“backend”: “tensorflow”<br/>}</strong></span></pre><p id="aca5" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我希望你已经正确无误地完成了所有的步骤。让我们测试我们的安装。打开python，然后导入keras进行测试。输出应该如下所示。</p><figure class="kl km kn ko fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mr"><img src="../Images/7e0c821d40379ae34356cabe375eef6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHNwlXz8I-Up6W9pDwXz4w.png"/></div></div><figcaption class="kp kq fg fe ff kr ks bd b be z ek">Test Keras installation</figcaption></figure><p id="8821" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以现在你已经安装了Python，Tensorflow和Keras。AMI还预装了ano和其他东西，但我们不会使用它们。不要费心卸载，因为他们不会干扰。安装够了，让我们深入研究代码。</p><blockquote class="ms"><p id="1332" class="mt mu if bd mv mw mx my mz na nb jz ek translated">不要浪费时间安装，花时间学习和实施。</p></blockquote><p id="7306" class="pw-post-body-paragraph jc jd if je b jf nc jh ji jj nd jl jm jn ne jp jq jr nf jt ju jv ng jx jy jz hn dt translated">我们将训练一个网络，用于对Kaggle的猫狗进行分类。在此之前，我们将开始编写一个简单的模型。这将有助于您了解Keras的工作原理。我从代码开始。如果你注意到，在代码的每一行之前都有注释。这些注释在某种程度上解释了那一行代码中发生了什么。要运行这段代码，您可以使用自己的一套猫和狗，也可以从Kaggle下载示例数据。您必须注册并加入Kaggle竞赛才能下载样本数据。这里是<a class="ae ka" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank"> Kaggle链接</a>。</p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="437d" class="ml ku if mh b fv mm mn l mo mp">from keras.preprocessing.image import ImageDataGenerator<br/>from keras.models import Sequential<br/>from keras.layers import Convolution2D, MaxPooling2D<br/>from keras.layers import Activation, Dropout, Flatten, Dense<br/><br/><br/># expected image size<br/>img_width, img_height = 150, 150</span><span id="7d13" class="ml ku if mh b fv mq mn l mo mp"># folder containing the images on which<br/># the network will train. The train folder <br/># has two sub folders, dogs and cats.<br/>train_data_dir = 'data/train'</span><span id="f6c9" class="ml ku if mh b fv mq mn l mo mp"># folder containing the validation samples<br/># folder structure is same as the training folder<br/>validation_data_dir = 'data/validation'</span><span id="8e3f" class="ml ku if mh b fv mq mn l mo mp"># how many images to be considered for training<br/>train_samples = 2000</span><span id="b0b5" class="ml ku if mh b fv mq mn l mo mp"># how many images to be used for validation<br/>validation_samples = 800</span><span id="3978" class="ml ku if mh b fv mq mn l mo mp"># how many runs will the network make<br/># over the training set before starting on<br/># validation<br/>epoch = 50</span><span id="d901" class="ml ku if mh b fv mq mn l mo mp"># ** Model Begins **<br/>model = Sequential()<br/>model.add(Convolution2D(32, 3, 3, input_shape=(3, img_width, img_height)))<br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/>model.add(Convolution2D(32, 3, 3))<br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/>model.add(Convolution2D(64, 3, 3))<br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/>model.add(Flatten())<br/>model.add(Dense(64))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(1))<br/>model.add(Activation('sigmoid'))<br/># ** Model Ends **</span><span id="f2f9" class="ml ku if mh b fv mq mn l mo mp">model.compile(loss='binary_crossentropy',<br/>              optimizer='rmsprop',<br/>              metrics=['accuracy'])<br/><br/># this is the augmentation configuration we will use for training<br/># we are generating a lot of transformed images so that the model<br/># can handle variety in the real world scenario<br/>train_datagen = ImageDataGenerator(<br/>        rescale=1./255,<br/>        shear_range=0.2,<br/>        zoom_range=0.2,<br/>        horizontal_flip=True)<br/><br/># this is the augmentation configuration we will use for testing:<br/># only rescaling<br/>test_datagen = ImageDataGenerator(rescale=1./255)</span><span id="9c50" class="ml ku if mh b fv mq mn l mo mp"># this section is actually taking images from the folder<br/># and passing on to the ImageGenerator which then<br/># creates a lot of transformed versions<br/>train_generator = train_datagen.flow_from_directory(<br/>        train_data_dir,<br/>        target_size=(img_width, img_height),<br/>        batch_size=32,<br/>        class_mode='binary')<br/><br/>validation_generator = test_datagen.flow_from_directory(<br/>        validation_data_dir,<br/>        target_size=(img_width, img_height),<br/>        batch_size=32,<br/>        class_mode='binary')</span><span id="8423" class="ml ku if mh b fv mq mn l mo mp"># this is where the actual processing happens<br/># it will take some time to run this step.<br/>model.fit_generator(<br/>        train_generator,<br/>        samples_per_epoch=train_samples,<br/>        nb_epoch=epoch,<br/>        validation_data=validation_generator,<br/>        nb_val_samples=validation_samples)<br/><br/>model.save_weights('trial.h5')</span></pre><p id="caa0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">代码非常简单明了。替换“模型存在”和“模型结束”之间的部分，以使用其他模型。您将拥有自己的分类器代码。我会给你们讲解代码。首先，导入一些Keras依赖项。然后，您定义将传递到网络的图像尺寸。之后，您告诉代码图像集在哪里。训练数据集和验证数据集。然后，从模型开始的地方开始构建模型，直到模型结束。我不打算深入研究这个模型，因为这是一个标准的VGGNet实现。有关网络架构的详细信息可在以下arXiv文章中找到:</p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="179a" class="ml ku if mh b fv mm mn l mo mp">Very Deep Convolutional Networks for Large-Scale Image Recognition<br/>K. Simonyan, A. Zisserman<br/>arXiv:1409.1556</span></pre><p id="5f7c" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">代码的下一步是生成一些数据转换。在这里，您可以剪切、拉伸、倾斜数据集，以便网络不会过度训练。您创建生成器，以便代码可以从指定的文件夹中读取图像。之后，处理开始。系统对提到的纪元次数进行训练和验证。最后，我们保存这些权重，以便我们可以在将来使用它们，而不必再次训练网络。如果你还有疑问，请突出显示并提问。我将尽力回答这些问题。</p><p id="f0ba" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">上面的模型是一个简单的模型，只是为了更简单的解释。根据我们现有的数据量，猫和狗的分类可能不会那么成功。所以我们不得不进行迁移学习。在迁移学习中，我们研究模型，我们训练这些模型来解决类似的陈述。我们采用经过训练的权重，并重新使用它们来解决完全不同的语句。我们训练模型，这些模型是我们在图像上预先训练的，用来对不同的事物进行分类。为什么会这样？这是因为我们将要使用的模型也是被训练来进行图像分类的。深入内部的层将总是能够进行一般分类。这些将在检测边缘和曲线的水平上工作。因此，术语迁移学习。你把从一个问题陈述中学到的知识转移到另一个问题陈述中。这对我们可能有好处。但是我们可以让它变得更好。接下来，我们训练顶层。这些层实际上担心实际的元素被分类。我们在我们的训练数据集上训练他们。我们可以称这个数据集为特定领域数据集。这给了网络一个理解，正是我们想要分类的。因此代码如下所示:</p><pre class="kl km kn ko fq mg mh mi mj aw mk dt"><span id="5fbc" class="ml ku if mh b fv mm mn l mo mp">import os<br/>import h5py<br/>import numpy as np<br/>from keras.preprocessing.image import ImageDataGenerator<br/>from keras import optimizers<br/>from keras.models import Sequential<br/>from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D<br/>from keras.layers import Activation, Dropout, Flatten, Dense<br/><br/># path to the model weights files.<br/>weights_path = 'vgg16_weights.h5'<br/>top_model_weights_path = 'fc_model.h5'<br/># dimensions of our images.<br/>img_width, img_height = 150, 150<br/><br/>train_data_dir = 'data/train'<br/>validation_data_dir = 'data/validation'<br/>nb_train_samples = 2000<br/>nb_validation_samples = 800<br/>nb_epoch = 50<br/><br/># build the VGG16 network<br/>model = Sequential()<br/>model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))<br/><br/>model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))<br/>model.add(MaxPooling2D((2, 2), strides=(2, 2)))<br/><br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))<br/>model.add(MaxPooling2D((2, 2), strides=(2, 2)))<br/><br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))<br/>model.add(MaxPooling2D((2, 2), strides=(2, 2)))<br/><br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))<br/>model.add(MaxPooling2D((2, 2), strides=(2, 2)))<br/><br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))<br/>model.add(ZeroPadding2D((1, 1)))<br/>model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))<br/>model.add(MaxPooling2D((2, 2), strides=(2, 2)))<br/><br/># load the weights of the VGG16 networks<br/># (trained on ImageNet, won the ILSVRC competition in 2014)<br/># note: when there is a complete match between your model definition<br/># and your weight savefile, you can simply call model.load_weights(filename)<br/>assert os.path.exists(weights_path), 'Model weights not found (see "weights_path" variable in script).'<br/>f = h5py.File(weights_path)<br/>for k in range(f.attrs['nb_layers']):<br/>    if k &gt;= len(model.layers):<br/>        # we don't look at the last (fully-connected) layers in the savefile<br/>        break<br/>    g = f['layer_{}'.format(k)]<br/>    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]<br/>    model.layers[k].set_weights(weights)<br/>f.close()<br/>print('Model loaded.')<br/><br/># build a classifier model to put on top of the convolutional model<br/>top_model = Sequential()<br/>top_model.add(Flatten(input_shape=model.output_shape[1:]))<br/>top_model.add(Dense(256, activation='relu'))<br/>top_model.add(Dropout(0.5))<br/>top_model.add(Dense(1, activation='sigmoid'))<br/><br/># note that it is necessary to start with a fully-trained<br/># classifier, including the top classifier,<br/># in order to successfully do fine-tuning<br/>top_model.load_weights(top_model_weights_path)<br/><br/># add the model on top of the convolutional base<br/>model.add(top_model)<br/><br/># set the first 25 layers (up to the last conv block)<br/># to non-trainable (weights will not be updated)<br/>for layer in model.layers[:25]:<br/>    layer.trainable = False<br/><br/># compile the model with a SGD/momentum optimizer<br/># and a very slow learning rate.<br/>model.compile(loss='binary_crossentropy',<br/>              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),<br/>              metrics=['accuracy'])<br/><br/># prepare data augmentation configuration<br/>train_datagen = ImageDataGenerator(<br/>        rescale=1./255,<br/>        shear_range=0.2,<br/>        zoom_range=0.2,<br/>        horizontal_flip=True)<br/><br/>test_datagen = ImageDataGenerator(rescale=1./255)<br/><br/>train_generator = train_datagen.flow_from_directory(<br/>        train_data_dir,<br/>        target_size=(img_height, img_width),<br/>        batch_size=32,<br/>        class_mode='binary')<br/><br/>validation_generator = test_datagen.flow_from_directory(<br/>        validation_data_dir,<br/>        target_size=(img_height, img_width),<br/>        batch_size=32,<br/>        class_mode='binary')<br/><br/># fine-tune the model<br/>model.fit_generator(<br/>        train_generator,<br/>        samples_per_epoch=nb_train_samples,<br/>        nb_epoch=nb_epoch,<br/>        validation_data=validation_generator,<br/>        nb_val_samples=nb_validation_samples)</span></pre><p id="2e82" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">VGG16的权重可以从我的<a class="ae ka" href="https://gist.github.com/debarko/6b1983ec3dd0403321082d07ddfea17c#file-readme-md" rel="noopener ugc nofollow" target="_blank"> Github </a> gist中获得。您还可以通过在数据集上运行这段<a class="ae ka" href="https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069" rel="noopener ugc nofollow" target="_blank">代码</a>来获得fc_model权重文件。您可以使用VGG16链接共享的同一组权重。你可以调整<a class="ae ka" href="http://stackoverflow.com/a/31157729" rel="noopener ugc nofollow" target="_blank">纪元</a>的数量来获得更好的学习，但不要走极端，因为这可能会导致<a class="ae ka" href="http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">过拟合</a>。我已经在我的<a class="ae ka" href="http://www.practo.com/" rel="noopener ugc nofollow" target="_blank">工作场所</a>的许多实际用例中使用了这种技术。一个用例是区分处方和非处方。我们使用在猫和狗的ImageNet数据上训练的完全相同的模型来分类处方。我希望你们能在现实世界的实际案例中使用它。一定要回答你用这种方法解决的任何有趣的案例。</p><p id="a49b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这篇文章大量引用了Keras的博客文章。请在twitter上关注我，你也可以注册一个由我维护的小而不常见的邮件列表。如果你喜欢这篇文章，请点击❤按钮推荐它。这将有助于其他媒体用户找到它。</p><div class="kl km kn ko fq ab cb"><figure class="lw hw nh ly lz ma mb paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="lw hw nh ly lz ma mb paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="lw hw nh ly lz ma mb paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="kb kc kd"><p id="f922" class="jc jd ke je b jf jg jh ji jj jk jl jm kf jo jp jq kg js jt ju kh jw jx jy jz hn dt translated"><a class="ae ka" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是AMI家庭的一员。我们现在<a class="ae ka" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae ka" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jc jd ke je b jf jg jh ji jj jk jl jm kf jo jp jq kg js jt ju kh jw jx jy jz hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae ka" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae ka" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="kl km kn ko fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff ni"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure></div></div>    
</body>
</html>