<html>
<head>
<title>DL02: Writing a Neural Network from Scratch (Code)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DL02:从头开始编写神经网络(代码)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257?source=collection_archive---------6-----------------------#2017-10-16">https://medium.com/hackernoon/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257?source=collection_archive---------6-----------------------#2017-10-16</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/3969d4da15b7cc6698f91c4a15424c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ncaK9VKWTVOuXuLR.jpg"/></div></div></figure><p id="6811" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">查看之前的教程:</p><blockquote class="ka kb kc"><p id="18ff" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated"><a class="ae kh" rel="noopener" href="/@thesemicolonguy/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864"> DL01:从头开始编写神经网络(理论)</a></p></blockquote><p id="028a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">你好黑客们，又到了喝咖啡的时间了！</p><p id="7591" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这一次，让我们开始编码。</p><figure class="kj kk kl km fq iv fe ff paragraph-image"><div class="fe ff ki"><img src="../Images/b8bd072de2a9740b7b0571f1d93fdb4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*feBVMPBNrs7_XwZs.gif"/></div></figure><blockquote class="kn"><p id="2d3d" class="ko kp hu bd kq kr ks kt ku kv kw jz ek translated">附带的代码可以在<a class="ae kh" href="https://github.com/thesemicolonguy/neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></blockquote><figure class="kx ky kz la lb iv"><div class="bz el l di"><div class="lc ld l"/></div></figure><p id="144d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Numpy用于Python中的数学计算。</p><p id="b5a0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Dill用于在python文件中存储所有变量，以便以后可以加载它们。使用<code class="eh le lf lg lh b">pip3 install dill</code>安装。</p><p id="9c36" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们为神经网络创建一个类:</p><figure class="kj kk kl km fq iv"><div class="bz el l di"><div class="lc ld l"/></div></figure><p id="7534" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">和一个用于层的类:</p><figure class="kj kk kl km fq iv"><div class="bz el l di"><div class="lc ld l"/></div></figure><p id="8393" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">好吧，我们开始吧！</p><p id="b7cd" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">类神经网络具有以下功能:</p><ul class=""><li id="0020" class="li lj hu je b jf jg jj jk jn lk jr ll jv lm jz ln lo lp lq dt translated">__init__:它接受4个东西作为输入:<br/> 1。层数:网络中的层数。<br/> 2。num_nodes:这是一个num_layers大小的列表，指定了每层中的节点数。<br/> 3。activation_function:也是一个列表，指定每一层的激活函数(第一层的激活函数通常会是<code class="eh le lf lg lh b">None</code>)。可以取值<code class="eh le lf lg lh b">sigmoid</code>、<code class="eh le lf lg lh b">tanh</code>、<code class="eh le lf lg lh b">relu</code>、<code class="eh le lf lg lh b">softmax</code>。<br/> 4。cost_function:计算预测输出和实际标签/目标之间的误差的函数。它可以取值<code class="eh le lf lg lh b">mean_squared</code>、<code class="eh le lf lg lh b">cross_entropy</code>。<br/> —用各层中给定的节点数初始化各层。初始化与每一层相关联的权重。</li><li id="edcc" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">训练:它需要6个输入:<br/> 1。batch_size:梯度下降的最小批量。<br/> 2。输入:给网络的输入。<br/> 3。标签:目标值。<br/> 4。num _ epochs:epochs的数量，即程序应该迭代所有训练的次数。<br/> 5。learning_rate:算法的学习速率，如DL01中所讨论的。6。filename:训练后最终存储所有变量的文件的名称。(文件名必须有扩展名<code class="eh le lf lg lh b">.pkl</code>)。<br/> —首先，有一个循环，用于迭代多个历元。然后，有一个嵌套循环来迭代所有的小批量。之后，通过迷你批处理调用forward_pass、calculate_error和back_pass(这与我们在DL01中所学的一致)。</li><li id="1441" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">forward_pass:只需要1个输入:<br/> 1。投入:小批量投入。<br/> —该函数将输入乘以权重，应用激活函数，并将输出存储为下一层的激活。对所有层重复这个过程，直到我们在输出层有一些激活。</li><li id="518a" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">calculate_error:同样需要1个输入:<br/> 1。标签:小批量标签。<br/> —该函数计算预测输出(即正向传递后输出层的激活)和目标值之间的误差。然后，该误差在<code class="eh le lf lg lh b">back_pass</code>功能中通过网络反向传播。</li><li id="2c4a" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">back_pass:需要1个输入:<br/> 1。标签:小批量标签。<br/> —该函数实现反向传播算法。这个算法将在另一篇文章中详细讨论。基本上，它会计算梯度，乘以学习率，然后从现有权重中减去乘积。从最后一层到第一层，对所有层都这样做。</li><li id="dc0d" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">预测:它需要2个输入:<br/> 1。filename:从中加载训练模型的文件。<br/> 2。输入:我们需要预测的输入。<br/> —它向前传递，然后将输出转换为独热编码，即数组的最大元素为1，所有其他元素为0。</li><li id="6bf5" class="li lj hu je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq dt translated">check_accuracy:需要3个输入:<br/> 1。filename:从中加载训练模型的文件。<br/> 2。输入:输入测试数据。<br/> 3。标签:目标测试数据。<br/> —这个函数做的事情和<code class="eh le lf lg lh b">predict</code>差不多。但是它没有返回预测的输出，而是将预测与标签进行比较，然后计算精确度为<code class="eh le lf lg lh b">correct*100/total</code>。</li></ul><p id="7138" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">类层有以下功能:</p><ul class=""><li id="e715" class="li lj hu je b jf jg jj jk jn lk jr ll jv lm jz ln lo lp lq dt translated">__init__:它需要3个参数:<br/> 1。层中节点数:该层中的节点数。<br/> 2。num_nodes_in_next_layer:下一层的节点数。<br/> 3。activation_function:该层的激活函数。<br/> —该函数由<code class="eh le lf lg lh b">neural_network</code>类的构造函数调用。它一次初始化一层。最后一层的权重设置为<code class="eh le lf lg lh b">None</code>。</li></ul><p id="1245" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这篇文章详细解释了代码。虽然卷积神经网络(CNN)在图像上表现得更好，但我在MNIST上训练了一个神经网络，只是为了感受一下。CNN将在以后的博客文章中介绍。</p><figure class="kj kk kl km fq iv fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/bd99736f6dae6966e3db11d930a079e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/0*d2zc0QfzX3K6VgoI."/></div></figure><p id="b1a2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我希望你现在可以自己从头开始实现一个神经网络。编码快乐！</p></div></div>    
</body>
</html>