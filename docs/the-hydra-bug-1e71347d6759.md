# 九头蛇虫:第三部分

> 原文：<https://medium.com/hackernoon/the-hydra-bug-1e71347d6759>

![](img/74c04a68ce56873317483760c75a8a5b.png)

Image copyright [Andrew Jian](https://www.flickr.com/photos/andrew_jian) [CC-BY](https://creativecommons.org/licenses/by/2.0/)

## 能(不能)的小内核

在解决了[简介](/@vishvananda/the-hydra-bug-4a9ceb208436)中的一个小错误后，[第一部分](/@vishvananda/the-hydra-bug-e4d3af2d61c0)阐述了我是如何花了一周时间为我的基于 KVM/vf 的虚拟机管理原型发现并解决 hydra 错误的。[第二部分](/@vishvananda/the-hydra-bug-d98214029358)解释了我是如何花了一周时间让 iPXE intelxvf 驱动程序变得更加健壮的。

一切都在工作，但我很恼火，我没有真正修复九头蛇的错误，而是解决了它。我可能无法控制启动哪个内核。因此，在 iPXE 端防止挂起是有意义的，但我不知道它为什么会修复这个问题，这让我很困扰。我作弊了，感觉很肮脏。不解决这个问题，我无法休息。请继续阅读，寻找真正的解决方案。

自从我上一次做这个已经过去了几个月，所以我的第一步是重现最初的问题。我恢复了我的解决方法，并尝试启动内核 3.8。同样的问题。我确保下载 UEK3 的最新版本，以防 bug 被修复。运气不好。

我需要知道内核挂在哪里。我的第一个尝试是尝试通过内核命令行启用 sysrq 键,这样我可以强制它进行堆栈跟踪。不幸的是，无论我如何混合 print-screen，挂起的内核都没有响应。也许我在启动过程中太早了？

看起来我不得不开始使用 gdb 来获得堆栈跟踪。我为来宾内核下载了 debuginfo 包，并使用调试监听器(`-s`和`-S`选项)启动 qemu stopped。我远程连接 gdb，加载调试就绪的内核映像，并键入`c`继续。

所有的在线帮助都谈到通过 qemu 直接引导内核来调试内核。qemu 启动时，我的内核没有加载。在最终到达内核代码之前，它会跳过 BIOS、iPXE 和 grub。因此，我假设我必须找出内核在内存中的加载位置，并告诉 gdb。

一旦我确定客户挂起了，我就按 Ctrl-C，让我吃惊的是我看到了源代码。经过进一步的思考，这是有意义的，因为早期的内核引导会将其例程加载到内存中的同一个位置。也可能是广发只是魔术。查看回溯，它似乎卡在 kvm_steal_clock 中的一个 [while 循环](http://lxr.free-electrons.com/source/arch/x86/kernel/kvm.c?v=4.8#L400)中。是时候研究 kvm 时钟是如何工作的了。

kvm 时钟似乎是通过分配一些共享内存来与主机通信的，主机端向这些共享内存写入数据，而客户机从这些共享内存中读取数据。在快速检查该函数中是否有任何代码在 UEK 被修改后，我注意到它被包装在一个`#ifndef CONFIG_DTRACE`中，但是该函数肯定被调用了，所以这是在转移视线。

只是为了验证问题出在这个位置，我试着完全禁用 [kvmclock](https://rwmj.wordpress.com/tag/kvmclock/) 。在 qemu 命令行上传递`cpu=host,-kvmclock`稍微改变了挂起位置，但似乎没有解决问题。我认为 kvmclock 使用的内存在某种程度上与 iPXE 冲突，也许删除它只是破坏了其他东西。

在内核源代码中读取早期的内存块分配，看起来它正在分配一个包含部分保留区域的大块(这是早在[第一部分](/@vishvananda/the-hydra-bug-e4d3af2d61c0)中打印出来的`NODE_DATA`的值)。尽管如此，kvmclock 代码打印出它正在使用的内存位置，并且，虽然它们靠近 iPXE 保留区域，但它们似乎并不直接冲突。

我决定逐步完成 kvm 时钟初始化，看看它是否能提供更多信息。我重启调试会话，键入`b kvmclock_init`，后跟`c`，等待断点命中。我到达了挂起，但没有碰到断点。搞什么？kvmclock_init 是否被跳过？不，控制台从函数中间显示调试语句，所以它肯定运行了。

我在网上搜索 qemu 断点，偶然发现一个[页面](http://wiki.qemu.org/Documentation/Debugging)提到硬件断点可能比软件断点工作得更好。我记得很久以前我用 qemu 开发引导装载程序时遇到了软件断点的问题。关于处于 16 位模式的一些事情让调试器感到困惑。我尝试使用硬件断点。这次它停下来了，但是给了我一个神秘的信息`Remote ‘g’ packet reply is too long`。

## 简介:GDB 和内核调试

为了兼容，所有 x86 处理器都以 16 位实模式启动。当英特尔推出 32 位处理器时，32 位指令隐藏在[保护模式](https://en.wikipedia.org/wiki/Protected_mode)中。即使是现代的 64 位处理器也是以实模式启动，并调用专门的指令，最终进入 64 位指令可用的模式。当您在 vm 执行之前连接到 qemu(使用`qemu -s -S`)时，gdb 检测到处理器处于实模式，并适当地配置自己。

不幸的是，在远程调试会话期间，gdb 没有[很好地处理](https://bugs.launchpad.net/ubuntu/+source/qemu-kvm/+bug/901944)模式切换。软件断点丢失，它不能正确解码来自远程端的数据包。更高版本的 gdb 和/或 gdbserver 可能会解决这个问题，但是有几个变通方法足以让我们重新开始:

1.  使用硬件断点而不是软件断点。
2.  在包回复错误之后，将一个新的 gdb 实例连接到服务器

数据包回复问题通过重新连接解决了，我可以通过 gdb 和几个 qemu [控制台命令](https://en.wikibooks.org/wiki/QEMU/Monitor#x)转储内存。kvmclock 用来与主机通信的位置似乎肯定有一个值，但是不清楚那里有什么数据以及为什么它可能会被破坏。

我用一个新的内核又运行了一次，看看是否能发现任何不同之处。我注意到损坏的内核指的是实例化“引导时钟”，而工作的内核指的是“主时钟”。嗯，实例化主时钟的代码似乎在不同的位置。也许它实际上是在新内核中初始化失败，但由于某种原因没有挂起？

我花了太多时间试图找出主时钟的差异。在 gdb 中沮丧了几个小时后，我终于发现我看到的是一个旧版本的代码。在实际工作的内核代码中，主时钟的重复初始化已被删除，时钟的名称从“引导时钟”更改为“主时钟”。所以新内核实际上运行的是相同的代码。那是多么转移注意力的事啊！

接下来，我试图去理解早期的内存分配过程。memblock 分配器似乎通过查看内核报告的可用内存的末尾，然后切掉一个块，来找到最高的可用地址。据我所知，通过阅读代码，任何保留的部分只有在以后构造页表时才会被考虑。我开始认为在早期引导过程中有一个错误，在页表初始化之前的分配可能与 BIOS 保留的内存冲突。

我的理论有几个问题:

1.  像这样的大规模错误似乎不太可能通过
    10 多个主要版本的内核而不被发现。
2.  工作中的内核报告将 kvmclock 分配在与损坏的内核相同的位置，并且特定的地址看起来没问题。

如果 bug 肯定在 kvmclock 代码中，也许 memblock 分配终究是没问题的。

我再次尝试验证这个 bug 确实在 kvmclock 中。我注意到了几个命令行参数，它们似乎从客户机端控制时钟的初始化。尝试了这些参数的各种组合后，我发现如果我指定了`no-kvmclock`和`no-kvmclock-vsyscall`，那么客户机实际上会启动。这看起来像是问题出在 kvmclock 的铁证。

最后我很幸运。在网上搜索 kvmclock_init 中的各种函数调用时，我偶然发现了一个非常有希望的[错误报告](https://bugzilla.kernel.org/show_bug.cgi?id=59521)。详细信息指的是挂起，因为当有多个处理器时，kvmclock 正在读取未初始化的内存。作为测试，我将处理器的数量减少到 1。内核启动！这个 bug 报告绝对是关于我们的 bug。

这个 bug 的[修复](https://github.com/torvalds/linux/commit/07868fc6aaf57847b0f3a3d53086b7556eb83f4a)是相当琐碎的。这是 3.8 中引入的一个简短的回归，在 3.10 中得到修复。其他发行版已经反向移植了它。我检查了 UEK 的来源，它没有被反向端口。最后一步是验证修复是否真正解决了我们的挂起问题。

我不喜欢做完整的内核构建，所以我启动 gdb 并在有问题的内存分配处设置一个断点。断点之后，我手动调用 memset 来清除内存，并继续引导。找到了。我有一个启用了 kvmclock 的 3.8 内核。我向 UEK 团队发送了一份错误报告，确信 3.8 内核的下一个版本将摆脱这个顽固的小错误。

因此，事实证明，内核 memblock 分配很好。恰好它为 kvmclock 共享区分配了一些以前使用过的内存。由于实现中的一个微小变化，二级处理器的 kvmclock 的内存没有初始化，因此它试图读取一个垃圾值并永远循环下去。

第一部分中的解决方法是将 kvmclock 移动到内存中，而内存碰巧被初始化为零，这是一个副作用。知道这一点很好，因为如果引导过程中的某些东西(BIOS、iPXE 或 grub)碰巧在新位置写入数据，该解决方法将会失败。换句话说，我的解决方法大多是偶然的。

那么这个故事的寓意是什么呢？早点搜索现有的 bug 报告！我曾梦想在内核内存分配中找到一个有趣的问题，但如果我只是更直接地谷歌一下实际挂起周围的一些代码行，我会更快地发现关键错误报告。

对于三周的调试来说，一个错过的反向端口是一个合适的结论吗？也许不是。但至少知道了为什么我可怜的小内核不能启动，我会睡得更安稳。没有留下任何 bug！

[![](img/50ef4044ecd4e250b5d50f368b775d38.png)](http://bit.ly/HackernoonFB)[![](img/979d9a46439d5aebbdcdca574e21dc81.png)](https://goo.gl/k7XYbx)[![](img/2930ba6bd2c12218fdbbf7e02c8746ff.png)](https://goo.gl/4ofytp)

> [黑客中午](http://bit.ly/Hackernoon)是黑客如何开始他们的下午。我们是 [@AMI](http://bit.ly/atAMIatAMI) 家庭的一员。我们现在[接受投稿](http://bit.ly/hackernoonsubmission)并乐意[讨论广告&赞助](mailto:partners@amipublications.com)机会。
> 
> 如果你喜欢这个故事，我们推荐你阅读我们的[最新科技故事](http://bit.ly/hackernoonlatestt)和[趋势科技故事](https://hackernoon.com/trending)。直到下一次，不要把世界的现实想当然！

![](img/be0ca55ba73a573dce11effb2ee80d56.png)