<html>
<head>
<title>Reinforcement Learning (Introduction)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习(简介)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/reinforcement-learning-part-1-d2f469a02e3b?source=collection_archive---------14-----------------------#2017-09-18">https://medium.com/hackernoon/reinforcement-learning-part-1-d2f469a02e3b?source=collection_archive---------14-----------------------#2017-09-18</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/9ea89442130f6bcfb88fd5f74258e00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4xhRsxpu_bCh3QWFjLtTQ.jpeg"/></div></div></figure><blockquote class="jc"><p id="28fa" class="jd je hu bd jf jg jh ji jj jk jl jm ek translated">“强化学习是简单的决策科学。这就是它如此普遍的原因。”—大卫·西尔弗</p></blockquote><h1 id="c961" class="jn jo hu bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk dt translated">摘要</h1><p id="4425" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">你可能已经观察到最近机器<a class="ae li" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>的饱和程度。这实际上是“监督学习”的饱和(可怜的卡格尔)。</p><p id="62c5" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">除了反向传播，我们大多数人都不知道其他任何学习算法。最近有几个类似于'<a class="ae li" href="https://arxiv.org/abs/1602.05179" rel="noopener ugc nofollow" target="_blank">平衡传播</a>和'<a class="ae li" href="https://arxiv.org/abs/1608.05343" rel="noopener ugc nofollow" target="_blank">合成梯度</a>，但或多或少属于类似于反向传播的范例。</p><p id="44f1" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">而无监督学习和强化学习仍然是空白(相对而言，至少是主流)。</p><p id="65f5" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">在这篇博客中，我们将深入探讨强化学习，或者我喜欢称之为'<em class="lo">愚蠢之后是后悔</em>或者'<em class="lo">假设</em>学习。这些名字其实很准确。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff lp"><img src="../Images/057ef86d406976fac8718c11b51b3644.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/1*prPYmCliEU4IPlVfq4-tqA.gif"/></div></figure></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="c1a7" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">介绍</h1><p id="faa9" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated"><strong class="kn hv">强化学习是在“因果”环境中基于交互的学习。</strong></p><p id="2cb9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">结果或后果是基于要达到的目标的行动和原因。</p><p id="4894" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">就像和你喜欢的人聊天一样。目标是给她留下深刻印象，但你说的每一句话都有后果。</p><p id="6503" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">她会:</p><ul class=""><li id="18f6" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">觉得你迷人还是</li><li id="3252" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">随机记起她必须去某个地方</li></ul><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/dc2a0314eb157d5b681ed366fcebbd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*uuCp0yIjPkO-Rp3gExitBg.jpeg"/></div></figure><p id="2b1a" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><em class="lo"># BelieveMeIAmAGuy #内向问题</em></p><p id="c266" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">RL是以目标为导向的学习，学习者(在RL中被称为'<em class="lo"> agent </em>'不是被命令采取什么行动，而是被鼓励去探索和发现产生最佳结果的最佳行动。模糊地说，<em class="lo">探索</em>是基于<em class="lo">试错</em>的方法。</p><p id="fde3" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">在RL中，你现在行为不仅影响你现在的回报(得到那个吻)，还会影响你未来的回报(她同意第二次约会)。所以代理人可能会选择更高的未来回报，而不是当前的高回报。</p><p id="1ef9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">记住下面这句话，你以后会更好理解的:</p><blockquote class="jc"><p id="ba63" class="jd je hu bd jf jg mv mw mx my mz jm ek translated">RL不是通过表征学习方法来定义的，而是通过表征学习问题本身来定义的。</p></blockquote></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="38c4" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">代理人</h1><p id="a1cb" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">RL中的学习者被称为代理。</p><p id="07cb" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">一个'<em class="lo">智能体</em>'具有对其环境的<em class="lo">感知，并能够以<em class="lo">与环境</em>交互的形式采取行动，以最大化从环境</em>获得的<em class="lo">回报。</em></p><p id="c963" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">问)</strong>谁是主管？</p><p id="37d0" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv"> Ans) </strong>在监督学习中，<em class="lo">训练集监督学习者的学习</em>。因此，它就像一个监督者。</p><p id="e857" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">问)</strong>RL代理如何以及何时比主管更优秀？</p><p id="b14f" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">在一个未知的领域，一位主管(你曾经的单身朋友)不太知道该怎么做(打动你的暗恋对象)。在这种情况下，更好的学习可以通过自己的(代理)经验来完成。</p></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="b392" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">勘探-开采权衡</h1><p id="60b7" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">明明有取舍，总有取舍！</p><p id="96f4" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">监督学习具有偏差-方差权衡，而RL具有探索-利用权衡。</p><p id="0d98" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">为了获得高额报酬，代理人可以选择</p><ul class=""><li id="8557" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">过去卓有成效的行动</li><li id="6e06" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">探索更好行动的可能性</li></ul><p id="08c9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">我们来分解一下</p><ul class=""><li id="d3eb" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">利用:利用预先学到的知识获得好的回报</li><li id="72ae" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">探索:尝试新事物以获得更好或更差的回报</li></ul><p id="574b" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><em class="lo">在随机任务中，一个动作必须尝试多次，以获得对其回报的可靠估计。然而，这并不能保证哪一个是最好的行动。代理不能同时探索和利用，因此需要权衡。</em></p><blockquote class="jc"><p id="2236" class="jd je hu bd jf jg mv mw mx my mz jm ek translated">在勘探过程中获得的信息可以在未来的步骤中多次利用。</p></blockquote></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="1737" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">额外的</h1><p id="014e" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">有时RL包括监督学习。这样做是为了确定哪些能力对获得回报至关重要。</p><p id="ac99" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">示例:</strong></p><blockquote class="na nb nc"><p id="c73c" class="kl km lo kn b ko lj kq kr ks lk ku kv nd ll ky kz ne lm lc ld nf ln lg lh jm hn dt translated">进化，确保你知道如何呼吸，否则无论你有多牛逼，你都将一事无成。</p><p id="5233" class="kl km lo kn b ko lj kq kr ks lk ku kv nd ll ky kz ne lm lc ld nf ln lg lh jm hn dt translated">在成长过程中，你是否获得了与坐立不安的纺纱者一起玩的诺贝尔奖不是被监督的部分所关心的。</p></blockquote><p id="b5e8" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">这些算法被归类为深度强化学习。</p><p id="5388" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">稍后会有更多的介绍，现在就让我们看看他们的表现有多好吧！</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ng nh l"/></div></figure></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="503f" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">RL基础</h1><p id="6400" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">RL有4个基本概念:</p><ul class=""><li id="7b82" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">政策</li><li id="da14" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">奖励函数</li><li id="0790" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">价值函数</li><li id="5757" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">(环境的)模型[理想的]</li></ul><h2 id="082c" class="ni jo hu bd jp nj nk nl jt nm nn no jx kw np nq kb la nr ns kf le nt nu kj nv dt translated">政策</h2><p id="22d6" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">它定义了代理在任何给定时间的行为。</p><p id="2c7d" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">粗略地说，策略是当前已知状态和处于这些状态时要采取的行动的映射。</p><p id="bf2c" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">在心理学上，它对应的是刺激反应规则。</p><p id="6aa1" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">策略可以是查找表或广泛搜索过程的形式。</p><h2 id="a1a9" class="ni jo hu bd jp nj nk nl jt nm nn no jx kw np nq kb la nr ns kf le nt nu kj nv dt translated">奖励和价值</h2><p id="6225" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated"><strong class="kn hv">奖励</strong>是指在<strong class="kn hv">直接意义上</strong>什么是好行为的增益。</p><p id="db7e" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">值</strong>是累积增益，用于估计一项行动在<strong class="kn hv">长期</strong>中的效果。</p><p id="3b64" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">例子:</strong></p><blockquote class="na nb nc"><p id="6c56" class="kl km lo kn b ko lj kq kr ks lk ku kv nd ll ky kz ne lm lc ld nf ln lg lh jm hn dt translated">现在吃蛋糕会让我开心。虽然从长远来看，蛋糕对我的健康有害。</p></blockquote><p id="9b24" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">奖励系统</strong>将奖励和价值结合在单一概念下，并传达:</p><ul class=""><li id="857b" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">一个行为的回报有多少</li><li id="a843" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">因此，一个国家是多么令人向往</li></ul><p id="45e3" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">在生物学意义上，它指的是快乐和痛苦。</p><p id="5e54" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">从关系的角度来说，它指的是她说“你看起来很可爱”和“你应该把胡子剃掉”(现在还疼)。</p><p id="63fa" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">大多数情况下，代理人不会改变奖励系统，尽管它可以作为改变政策的基础。</p><p id="fefb" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">举例:</strong></p><blockquote class="na nb nc"><p id="95c7" class="kl km lo kn b ko lj kq kr ks lk ku kv nd ll ky kz ne lm lc ld nf ln lg lh jm hn dt translated">脱离这个循环很痛苦，所以回报很低。我不能改变奖励制度，以痛苦为乐。酪我可以控制我骑车的方式，这样我就不会再摔倒了。</p></blockquote><p id="94b9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">我们主要关心的是价值而不是报酬。</strong></p><p id="8874" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">问)</strong>为什么？</p><p id="8797" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">奖励只能确保眼前的收益是最大的，未来的收益可能会更糟。价值确保我们有最大的整体收益。</p><p id="05e9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">问)</strong>那还有什么问题？</p><p id="a861" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">通常眼前的收益比未来的收益更有保障。</p><p id="5bf1" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">问)</strong>什么是价值函数？</p><p id="5146" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv"> Ans) </strong>顾名思义，价值函数就是用来计算一个动作的价值的函数。棘手的是他们是如何做到的。这些通常使用基于复杂推理的算法。</p><p id="9bb9" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">例如:</strong></p><blockquote class="na nb nc"><p id="0e5d" class="kl km lo kn b ko lj kq kr ks lk ku kv nd ll ky kz ne lm lc ld nf ln lg lh jm hn dt translated">你买一只股票。目前市场状况良好。所以你可以很有把握地在明天卖掉它，获得丰厚的回报。现在，如果你再持有这些股票一年，价值可能会飙升到几倍，但你永远无法确定这一点。</p></blockquote></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><h1 id="ee57" class="jn jo hu bd jp jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk dt translated">进化算法</h1><p id="cca2" class="pw-post-body-paragraph kl km hu kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh jm hn dt translated">虽然听起来很像，但是RL和进化算法不是一回事。</p><p id="3930" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">进化算法不使用价值函数。他们直接搜索政策空间。</p><p id="ad34" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated"><strong class="kn hv">优点:</strong></p><ul class=""><li id="00de" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">当代理对环境没有太多感觉时有效</li></ul><p id="410e" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">缺点:</p><ul class=""><li id="fc3c" class="mg mh hu kn b ko lj ks lk kw mi la mj le mk jm ml mm mn mo dt translated">他们不会注意到代理在其整个生命周期中经历了哪些状态</li><li id="3f41" class="mg mh hu kn b ko mp ks mq kw mr la ms le mt jm ml mm mn mo dt translated">他们没有利用这样一个事实，即他们所寻求的政策是一个从国家到行动的函数。</li></ul></div><div class="ab cl lu lv hc lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hn ho hp hq hr"><p id="eeb6" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">这只是第一章，对RL的基本介绍。在接下来的章节中，我们还有一个世界要探索。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/221c98a117c156df15ec7487e97ce6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*VvOICGpBC6-T7cHdpKQFIA.gif"/></div></figure><p id="08e8" class="pw-post-body-paragraph kl km hu kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh jm hn dt translated">是啊，那是随机的GIF。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="nx nh l"/></div></figure></div></div>    
</body>
</html>