<html>
<head>
<title>What is a CapsNet or Capsule Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是CapsNet或胶囊网络？</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/what-is-a-capsnet-or-capsule-network-2bfbe48769cc?source=collection_archive---------0-----------------------#2017-11-01">https://medium.com/hackernoon/what-is-a-capsnet-or-capsule-network-2bfbe48769cc?source=collection_archive---------0-----------------------#2017-11-01</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/1d8a39bd6fd8ba322410f926c7c28762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JcpMJty2A0x1ryY-ZQe2VA.png"/></div></div></figure><div class=""/><p id="0abd" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="ka">什么是胶囊网？什么是胶囊？CapsNet比卷积神经网络(CNN)更好吗？在这篇文章中，我将谈论以上所有关于Hinton发布的CapsNet或Capsule Network的问题。</em></p><blockquote class="kb kc kd"><p id="8ca0" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">注意:本文不是关于药物胶囊的。它是关于神经网络或机器学习世界中的胶囊。</p></blockquote><p id="195e" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">作为读者，你有一种期待。你需要了解CNN。如果没有，我想让你在<a class="ae kh" href="https://hackernoon.com/" rel="noopener ugc nofollow" target="_blank">杂志</a>上浏览<a class="ae kh" href="https://hackernoon.com/supervised-deep-learning-in-image-classification-for-noobs-part-1-9f831b6d430d" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。接下来，我将简要回顾一下CNN的相关观点。这样你就可以很容易地抓住下面的比较。所以事不宜迟，让我们开始吧。</p><p id="2044" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">CNN本质上是一个系统，我们将许多神经元堆叠在一起。这些网络已经被证明在处理图像分类问题上非常出色。让一个神经网络映射出一幅图像的所有像素是很困难的，因为这在计算上非常昂贵。因此，卷积是一种帮助您在不丢失数据本质的情况下，在很大程度上简化计算的方法。卷积基本上是大量矩阵乘法和这些结果的求和。</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff ki"><img src="../Images/f411a65ddd4b1208593e063978a03e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qd_T1j_0dqGzsN8X-H89Pw.jpeg"/></div></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">image 1.0: Convolutional Neural Network</figcaption></figure><p id="7da3" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在图像被输入网络后，一组内核或过滤器对其进行扫描并执行卷积运算。这导致在网络内部创建特征地图。这些功能接下来依次通过激活层和池层，然后根据网络中的层数继续下去。需要激活层来在网络中引起<a class="ae kh" href="https://stackoverflow.com/a/9783865/2235170" rel="noopener ugc nofollow" target="_blank">非线性</a>的感觉(例如:<a class="ae kh" href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions" rel="noopener ugc nofollow" target="_blank"> ReLU </a>)。合用(例如:最大合用)有助于减少训练时间。汇集的想法是它创建每个子区域的“摘要”。它也给你一点点的位置和物体检测的平移不变性。在网络的末端，它将通过像softmax分类器这样的分类器，给我们一个类。训练是基于与一些标记数据匹配的误差反向传播进行的。非线性也有助于解决这个步骤中的消失梯度。</p><h1 id="c7ca" class="kr ks if bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo dt translated"><strong class="ak">CNN有什么问题？</strong></h1><p id="5aeb" class="pw-post-body-paragraph jc jd if je b jf lp jh ji jj lq jl jm jn lr jp jq jr ls jt ju jv lt jx jy jz hn dt translated">当对非常接近数据集的图像进行分类时，CNN表现得异常出色。如果图像有旋转、倾斜或任何其他不同的方向，那么CNN的性能很差。这个问题是通过在训练过程中添加相同图像的不同变化来解决的。在CNN中，每一层都在更精细的层次上理解图像。让我们通过一个例子来理解这一点。如果你想给船和马分类。最内层或第1层理解小曲线和边缘。第二层可能理解直线或更小的形状，如船的桅杆或整个尾部的弯曲。更高层开始理解更复杂的形状，如整个尾部或船体。最后一层试图看到一个更全面的图片，如整艘船或整匹马。我们在每一层之后使用池化，使其在合理的时间框架内进行计算。但是本质上它也丢失了位置数据。</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lu"><img src="../Images/1233ad310aa7085ad79ef411ea9cbd46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RGB8Eql5j27ujkt--yB_Q.png"/></div></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">image 2.0: Disfiguration transformation</figcaption></figure><p id="9bc8" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">池有助于创建位置不变性。否则，CNN将只适合非常接近训练集的图像或数据。这种不变性也导致对于具有船的部件但顺序不正确的图像触发假阳性。因此，系统可以触发右与上图中的左相匹配。作为一个观察者，你清楚地看到了不同之处。池层也增加了这种不变性。</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lv"><img src="../Images/ab2ac6c8a0dc9aa2795ffce7c8001e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7cUF8V3BdiD3k7e4sHgHw.png"/></div></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">image 2.1 Proportional transformation</figcaption></figure><p id="ea83" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这从来都不是pooling layer的本意。汇集应该做的是引入位置，方向，比例不变。但是我们获得这种用途的方法非常粗糙。实际上，它增加了各种位置不变性。从而导致将图像2.0中的右侧船只检测为正确船只的困境。我们需要的不是不变性，而是等方差。不变性使得CNN能够容忍视点的微小变化。<strong class="je ig">等方差</strong>使CNN了解旋转或比例变化，并相应地调整自己，以便图像内部的空间定位不会丢失。一艘船将仍然是一艘较小的船，但是CNN将缩小它的尺寸来检测它。这让我们想到了胶囊网络的最新进展。</p></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><h1 id="164a" class="kr ks if bd kt ku md kw kx ky me la lb lc mf le lf lg mg li lj lk mh lm ln lo dt translated">什么是胶囊网？</h1><p id="7cf8" class="pw-post-body-paragraph jc jd if je b jf lp jh ji jj lq jl jm jn lr jp jq jr ls jt ju jv lt jx jy jz hn dt translated">每隔几天，神经网络领域就有一项进展。一些聪明的人正在这个领域工作。你几乎可以假设每一篇关于这个主题的论文都是开创性的或者改变了道路的。Sara Sabour、Nicholas Frost和Geoffrey Hinton发布了一篇名为<strong class="je ig"><em class="ka"/></strong><a class="ae kh" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank"><strong class="je ig"><em class="ka">胶囊间动态路由</em></strong></a><strong class="je ig"><em class="ka"/></strong>的论文4天回来了。现在，当深度学习的教父之一Geoffrey Hinton发表一篇论文时，它一定会是开创性的。当你阅读这篇文章时，整个深度学习社区都在为这篇论文而疯狂。因此，这篇文章讨论了胶囊、CapsNet和MNIST挤兑。MNIST是一个带标签的手写数字图像的数据库。结果显示，在重叠数字的情况下，性能显著提高。本文与当前最先进的CNN进行了比较。在这篇论文中，作者提出人类大脑有称为“胶囊”的模块。这些胶囊特别擅长处理不同类型的视觉刺激，并对姿势(位置、大小、方向)、变形、速度、反照率、色调、纹理等进行编码。大脑必须有一种机制，将低级视觉信息“路由”到它认为处理它的最佳胶囊。</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mi"><img src="../Images/863a04280b5e4e939cf792561096bb59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1y-bAF1Wv9-EtdQcsErhA.png"/></div></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">image 3.0: CapsNet Architecture</figcaption></figure><p id="ee00" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">胶囊是一套嵌套的神经层。所以在一个常规的神经网络中，你不断增加更多的层。在CapsNet中，您可以在一个层中添加多个层。或者换句话说，将一个神经层嵌套在另一个神经层中。胶囊内神经元的状态捕获图像内一个实体的上述属性。胶囊输出一个矢量来表示实体的存在。向量的方向代表实体的属性。该向量被发送给神经网络中所有可能的双亲。对于每个可能的亲本，胶囊可以找到预测向量。预测向量是基于乘以其自身的权重和权重矩阵来计算的。无论哪一个亲本具有最大的标量预测矢量积，都会增加胶囊结合。其余的父母减少他们的债券。这种<strong class="je ig">协议路由</strong>方法优于现有的最大池机制。基于低层中检测到的最强特征的最大汇集路由。除了动态路由之外，CapsNet还谈到了给胶囊增加挤压功能。挤压是一种非线性现象。因此，不是像在CNN中那样给每个层添加挤压，而是将挤压添加到一组嵌套的层中。所以挤压函数应用于每个胶囊的矢量输出。</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/d64c05c0beb252b02930099ce5ffc58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*CCxgEjhlsXyui4PaUJQxpw.png"/></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">image 3.1: Novel Squashing Function</figcaption></figure><p id="1b6a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">介绍了一种新的挤压函数。你可以在图3.1中看到。ReLU或类似的非线性函数适用于单个神经元。但是论文发现这种挤压功能在胶囊上效果最好。这将尝试压缩胶囊输出向量的长度。如果它是一个小向量，它会压缩为0，如果向量很长，它会尝试将输出向量限制为1。动态路由增加了一些额外的计算成本。但它无疑带来了额外的优势。</p><p id="ed1a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在我们需要认识到，这篇论文几乎是全新的，胶囊的概念还没有经过全面的测试。它适用于MNIST数据，但仍需要在各种类别的更大数据集上进行验证。已经有(在4天内)关于这篇文章的更新提出了以下问题:</p><blockquote class="kb kc kd"><p id="359d" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">1.它使用姿态向量的长度来表示胶囊所代表的实体存在的概率。为了保持长度小于1，需要无原则的非线性，该非线性防止存在被迭代路由过程最小化的任何可感知的目标函数。</p><p id="e8c0" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">2.它使用两个姿态向量之间的角度余弦来衡量它们在路由上的一致性。与高斯聚类的对数方差不同，余弦不擅长区分相当好的一致性和非常好的一致性。</p><p id="4e21" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">3.它使用一个长度为n的向量，而不是一个有n个元素的矩阵来表示一个姿势，所以它的变换矩阵有n 2个参数，而不是只有n个。</p></blockquote><p id="c5ed" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">胶囊的当前实现具有改进的空间。但是我们也应该记住，辛顿的论文首先只说:</p><blockquote class="kb kc kd"><p id="bc06" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">本文的目的不是探索整个领域，而是简单地说明一个相当简单的实现工作良好，并且动态路由有所帮助。</p></blockquote></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><p id="eb34" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以有很多理论。让我们找点乐子，建一个顶网。我将带您浏览一些代码，为MNIST数据设置一个基本的CapsNet。我将在代码中添加注释，这样你就可以一行一行地理解它是如何工作的。我将带你看一下代码中两个重要的部分。休息你可以去回购，叉它，并开始工作:</p><figure class="kj kk kl km fq hw"><div class="bz el l di"><div class="mk ml l"/></div></figure><p id="3fae" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">以上是整个胶囊层。这是现在堆叠创建一个胶囊网络。CapsNet的代码如下:</p><figure class="kj kk kl km fq hw"><div class="bz el l di"><div class="mk ml l"/></div></figure><p id="6a6e" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">整个代码以及培训和评估模块在<a class="ae kh" href="https://github.com/debarko/CapsNet-Tensorflow" rel="noopener ugc nofollow" target="_blank">这里</a>呈现。它在<a class="ae kh" href="https://github.com/debarko/CapsNet-Tensorflow/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank"> Apache 2.0许可下</a>。可以自由使用。我想把代码归功于<a class="ae kh" href="https://github.com/naturomics" rel="noopener ugc nofollow" target="_blank">自然经济学</a>。</p><h1 id="5b44" class="kr ks if bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo dt translated">摘要</h1><p id="3ad2" class="pw-post-body-paragraph jc jd if je b jf lp jh ji jj lq jl jm jn lr jp jq jr ls jt ju jv lt jx jy jz hn dt translated">因此，我们了解了什么是顶网以及它们是如何构建的。我们试图理解胶囊只不过是高级别的嵌套神经层。我们还研究了CapsNet如何传递旋转和其他不变性。这与图像中每个实体的空间设置是一样的。我确信仍然有问题需要回答。胶囊及其最佳实现可能是最大的问题。但是这篇文章是试图阐明这个话题的第一步。如果您有任何疑问，请分享。我将尽我所知回答这些问题。</p><blockquote class="kb kc kd"><p id="8f7e" class="jc jd ka je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">Siraj Raval 和他的谈话极大地影响了这篇文章。在<a class="ae kh" href="http://bit.ly/2z1z6RU" rel="noopener ugc nofollow" target="_blank">推特</a>上分享这篇文章。请在<a class="ae kh" href="http://twitter.com/debarko" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我的最新消息。如果你喜欢这篇文章，请点击👏按钮来支持它。这将有助于其他媒体用户找到它。在Twitter上分享这篇文章，这样其他人也可以阅读。</p></blockquote></div></div>    
</body>
</html>