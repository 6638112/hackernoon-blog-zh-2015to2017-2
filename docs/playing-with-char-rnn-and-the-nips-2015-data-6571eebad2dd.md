# 使用 char-rnn 和 NIPS 2015 数据

> 原文：<https://medium.com/hackernoon/playing-with-char-rnn-and-the-nips-2015-data-6571eebad2dd>

自从 [Zen-RNN](https://medium.com/u/f3c8148878e1#.vn9ox6zb8) 、 [TED-RNN](/@samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0) 、[奥巴马-RNN](/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0) )，我一直在寻找机会自己尝试一下 [char-nn](https://github.com/karpathy/char-rnn) 库。在今年的神经信息处理系统会议( [NIPS](https://nips.cc/) 2015)上的所有论文都出现在网上之后，一个机会出现了。有什么比一堆大谈 RNNs 的论文更适合和*一个 RNN 玩呢？

## **NIPS 2015 数据集**

该数据集是作为这一竞赛的一部分提供的。它包括一个 CSV 文件，其中包含所有论文的文本，摘自今年接受的[论文](https://nips.cc/Conferences/2015/AcceptedPapers)。

## **预处理**

文本数据(使用工具 [pdftotext](https://en.wikipedia.org/wiki/Pdftotext) 从 pdf 中提取)非常嘈杂:pdftotext 不提取页码、回车断字、等式、章节标题、变量、图形标题、表格、脚注以及研究人员可以用各种 LaTeX 命令放入简单文本的所有其他复杂性。结果是我们显然不想用来训练任何东西的文本。这里有一个相对常见的例子:

> pβt xpiqβt y pjq Q2 mij " max
> minβt WMβ
> (3)
> βPb
> | |β| | 0 ďk

这段文字是由人类“生成”的。对于这个实验*，我希望使用*完整的句子*进行训练，这在技术/算法论文中是很少见的。根据我的经验，它们往往出现在摘要、引言、相关工作、讨论/结论部分。因此，我过滤了每篇论文——手动删除了引言之后和结论之前的所有内容(但这在不同的论文之间有所不同*)——是的，如果你想知道，这是一个巨大的痛苦，而且没有做到完美。

注意:我确实先尝试了一些正则表达式(并且使用了一些来删除引用之类的东西)，但是只能做到这一步:在部分标题中有很多变化，并且总的来说有太多的情况/异常值/异常来合理地自动化这个。

## 培养

char-rnn 库使生活变得非常简单。一旦火炬(等。)已经安装好了，所有的论文都放入了一个单独的 *input.txt* 文件中，训练模型就差一行代码了。

我使用了具有较高压差值(0.5)的标准设置来最小化过度拟合——尤其是因为数据仍然相当嘈杂。

## 比较，无意义的结果

这是一些结果。人类输入的例子来自于从包含种子文本的输入数据中随机选取一行；然后使用相同的种子生成机器文本。

**Seed:“我们要解决的任务”**

> 例子人类:“我们的目标是解决填补高维二进制时间序列数据中多个连续数据点的空白。”
> 
> 机器:“我们的任务旨在解决推理学习问题的证明所提供的方法和算法方法是可能的后验和计算的状态时间的演示是预期的限制，在局部极大极小。”

**种子:【本文我们】**

> 例子人:“在本文中，我们提出了一种新的离散空间模型的参数估计器，它不需要计算归一化常数。”
> 
> 机器:“在本文中，我们提出 f 的输入信息对于每个预测都是增加的，并且 minimax 方法优于下面的分布，并且可以稳健地分析树图像 mint 离散化到 s vret 的计算实验，并且在可能的数据大小中仅可能是线性遗憾中的加速 rime。”

**种子:【总结】**

> 示例人类: *"* 最后，我们在图 1 中用黑色显示了一组优化的伪输入 Z。我们还在实现中叠加了任意单点的边际，通过这里 L 的谱分解可以得到。”
> 
> 机器:“总结来说，每个相对可能发展的分布约束分解的子模型规模的目标是真实的恢复信息的预期概率的预测参数是步骤的算法是一种分析方法，这种分析后的随机性质的广泛程序的最大复杂性集。”

## 好玩的种子

**种子:“生命的意义在于”**

> 机器:“生命的意义是表明扩大我们的线性期望在一个计算机方差的水平生产控制分布的最小逃学特征值的相似状态。”

**种子:“今晚的晚餐是”**

> "今晚的晚餐显示出扩展函数大小点的学习兴趣，因为那时一个分别。"

**种子:“机器学习研究应该”**

> "机器学习的研究应该推广到网络中只有二次共享图像的情况."

**Seed:“我的预言是”**

> “我的预测与模型中的线性和非梯度函数的先验有关，也考虑了后验的 MDM，并定义了用大型优化分析定义为计算 p 的状态的真实值。”

## **一些想法**

*   机器不会很快接管一切。
*   从 PDF 中提取高质量的文本比训练一个 RNN 更难(正如这篇 NIPS 论文指出的“机器学习”是“机器学习系统”的一小部分).
*   这是我第一次使用 char-rnn 库。很可能对它了解得多一点的人会得到更好的结果。
*   我想知道进一步的预处理是否会产生更好的训练数据集。例如，可以使用某种聚类来识别相似的论文(例如，对单词包进行比较)，然后只在一个特定的聚类上进行训练。

## 感谢:

*   由 [samim](https://medium.com/u/f3c8148878e1?source=post_page-----6571eebad2dd--------------------------------) 以前的职位。在这里找到了。
*   github 上 [char-nn 的作者和投稿人](https://github.com/karpathy/char-rnn)。
*   Kaggle，因为它的数据，因为它是一个非常棒的地方。在这里找到了。

[*]这是**不是**科学。