<html>
<head>
<title>Feynman Machine: a New Approach for Cortical and Machine Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">费曼机器:大脑皮层和机器智能的新途径</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70?source=collection_archive---------2-----------------------#2017-02-01">https://medium.com/hackernoon/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70?source=collection_archive---------2-----------------------#2017-02-01</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="119a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/tagged/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>可能不会带来你一直期待的人工智能革命。这一季的炒作仅仅是80年代末的重演，只是规模大得多吗？也许甲壳虫乐队时代的技术再次迎来了冬天，因为它是基于二战的神经科学和维多利亚时代的统计力学。幸运的是，如果一种新方法可以将计算神经科学和应用数学的最新知识与今天的GPU的能力相融合，那么深度学习的新能量、热情和投资不必浪费。<a class="ae jp" href="https://arxiv.org/abs/1609.03971" rel="noopener ugc nofollow" target="_blank">费曼机器</a>既是对大脑实际工作方式的准确描述，也是机器智能的蓝图。结合耦合、通信、混沌动力系统的应用数学和神经科学的最新发现，我们在一年前成立了<a class="ae jp" href="https://ogma.ai" rel="noopener ugc nofollow" target="_blank"> Ogma </a>，将<a class="ae jp" href="https://arxiv.org/abs/1512.05245" rel="noopener ugc nofollow" target="_blank">理论</a>转化为<a class="ae jp" href="https://github.com/ogmacorp/OgmaNeo" rel="noopener ugc nofollow" target="_blank">工作软件</a>，为新的人工智能技术奠定基础。</p><p id="754d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我不会过多地谈论为什么我认为今天的深度学习热潮不是它被大肆宣传的灵丹妙药。DL自己的许多长期被忽视和新近被称赞的领导人已经在这样做了(其中最突出的是<a class="ae jp" href="https://www.quora.com/Is-the-current-hype-about-Deep-Learning-justified?redirected_qid=6578691" rel="noopener ugc nofollow" target="_blank"> Yoshua Bengio </a>、<a class="ae jp" href="https://www.quora.com/What-are-the-limits-of-deep-learning-2/answer/Yann-LeCun" rel="noopener ugc nofollow" target="_blank"> Yann LeCun </a>和<a class="ae jp" href="https://www.youtube.com/watch?v=VIRCybGgHts" rel="noopener ugc nofollow" target="_blank"> Geoff Hinton </a>)，尽管他们最近受到了高调的任命和天文数字的金融承销。在不同程度上，他们现在正超越最近数字图书馆的成功应用，寻找可能提供下一步发展的新想法。</p><p id="2444" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">DL的主要弱点(在我看来)是:依赖尽可能简单的模型神经元(LeCun称之为“卡通化”)；使用19世纪统计力学和统计学的思想，这是能量函数和对数似然方法的基础；这些技术的结合，如反向投影和随机梯度下降，导致应用范围非常有限(离线，主要是批量，监督学习)，需要非常有才华的从业者(又名“<a class="ae jp" href="https://twitter.com/kaydeeb0y/status/626469795391778816" rel="noopener ugc nofollow" target="_blank">随机研究生下降</a>”)，大量昂贵的标记训练数据和计算能力。虽然对于那些可以吸引或购买人才并部署无限资源来收集和处理数据的大公司来说，DL是伟大的，但对我们大多数人来说，它既不容易获得也没有用。</p><p id="5f91" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">另一方面，费曼机器利用从整个研究领域中学习，这在20世纪60年代和70年代之前根本不存在。第一次，它们被结合在一个模型中，该模型描述了新大脑皮层如何处理高速、流动的感觉数据并产生行为和认知，这也是智能机器的一种新架构。我将简要描述这些结果，以及它们是如何在自然和人工费曼机器中使用的。</p><p id="3b12" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先要学习的是应用数学的一个分支，它研究非线性动力系统(NDSs)的特性，也就是通常所说的混沌理论。科学中的大多数现象(包括自然现象和人类现象)只能被精确地建模为NDSs，但这在20世纪60年代早期计算机广泛普及之前并不实用。爱德华·洛伦茨在1963年发表的具有里程碑意义的论文被广泛认为是对一类完全未知的行为:确定性混沌的第一次详细描述。虽然使用传统的数学工具几乎不可能处理，但涉及NDSs的系统具有如此丰富的结构，并且在本质上如此普遍，以至于它们的研究已经主导了应用数学超过40年。</p><p id="7603" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">1978年，<a class="ae jp" href="http://www.csee.wvu.edu/~xinl/library/papers/physics/packard1980.pdf" rel="noopener ugc nofollow" target="_blank">帕卡德<em class="jq">等人</em> </a>发现了来自混沌系统的信号的一些意想不到的东西:它们包含了重建混沌系统的整个时间行为和预测其未来所需的所有信息，而<em class="jq">却没有任何关于驱动系统的实际机制的知识</em>。1981年，芙罗莉丝·塔肯斯根据这个惊人的事实证明了他著名的定理。来自George Sugihara实验室的以下视频简要描述了该定理如何工作，以及如何使用重建来检查和分析NDSs(这里使用Lorenz系统进行说明):</p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="jw jx l"/></div></figure><p id="154f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">视频中描述的因果关系标准后来被称为Sugihara因果关系，它比传统上用于统计和<a class="ae jp" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的格兰杰因果关系更强大，因为它使用了时间序列信号中的时间结构。George Sugihara是在渔业可持续性和基因网络等领域应用这些强大方法的领军人物之一。</p><p id="9701" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">费曼机器的中心思想是，大脑的各个区域形成了一个NDSs网络，它们通过相互发送神经信号的时间序列来进行交流和合作，而认知是从耦合的动力系统的因果相互作用中产生的。应用数学告诉我们这是可能的，但是我们现在需要依靠神经科学来解释这在真实的大脑中是如何发生的。</p><p id="6299" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">1986年，future Palm创始人杰夫·霍金斯(Jeff Hawkins)写了一篇论文提案，其中他描述了一个不断预测自身未来进化的新大脑皮层模型。由于当时对理论神经科学缺乏兴趣，直到2000年代初，霍金斯才能够回到这个想法，并投入全部时间来领导它的发展。他和桑德拉·布莱克斯利一起写了关于智力的<a class="ae jp" href="https://en.wikipedia.org/wiki/On_Intelligence" rel="noopener ugc nofollow" target="_blank"><em class="jq"/></a>，成立了理论神经科学的<a class="ae jp" href="http://redwood.berkeley.edu/" rel="noopener ugc nofollow" target="_blank">雷德伍德中心</a>(现在是加州大学伯克利分校的一部分)，并共同创立了<a class="ae jp" href="http://numenta.com/" rel="noopener ugc nofollow" target="_blank"> Numenta </a>，在那里他的理论，现在被称为分级时间记忆，继续得到研究和发展。霍金斯(以及努门塔的同事苏布泰·阿默德和·崔)将他的所有理论建立在硬神经科学的基础上，除非有强有力的证据证明它存在于大脑中，否则什么也不包括。2013年，Numenta开源了他们的NuPIC HTM软件，我成为了全球社区的一员，与Hawkins一起探索HTM作为皮层模型和一种有前途的机器学习技术。我2015年的<a class="ae jp" href="https://arxiv.org/abs/1509.08255" rel="noopener ugc nofollow" target="_blank">论文</a>是第一份关于HTM如何运作的深度数学描述。</p><p id="bc9a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">那篇论文是概述我在2015年的发现的两篇论文中的第一篇，我的发现是耦合NDS的机制是哺乳动物大脑计算能力的秘密来源。当我看到圣达菲学院的Melanie Mitchell的这个演讲并在复杂性探索MOOC网站上学习她的课程时，我有了这个想法。我太老了，在大学里没有学过这些，所以这是我第一次有机会了解复杂系统的广阔世界，以及最近开发的探索和利用它们的方法的力量。</p><p id="7dbc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我的第二篇论文——<a class="ae jp" href="https://arxiv.org/abs/1512.05245" rel="noopener ugc nofollow" target="_blank">Synapses的交响乐:Neocortex作为一个使用分级时间记忆的通用动力系统建模器</a> —概述了大脑如何作为一种新型的计算机器，使用Takens定理处理流数据，紧急自我组织认知，并智能地行为。HTM被用来证明新皮层各层神经元之间的直接联系和网络NDSs的紧急处理。因为Numenta已经有了可以在小范围内模拟大脑皮层计算的工作软件，所以我提出，只要建立正确的这种模块网络，智能机器就可以实现。</p><p id="ac77" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们现在知道(h/t George Sugihara)在真正的灵长类动物新大脑皮层中有<a class="ae jp" href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004537" rel="noopener ugc nofollow" target="_blank">强有力的证据</a>证明费曼机器过程。有趣的是，这项分析使用了Takens定理和Sugihara的方法，找到了跨越一百多个大脑区域的因果网络(Tajima因这篇论文获得了<a class="ae jp" href="http://www.theassc.org/past_recipients" rel="noopener ugc nofollow" target="_blank">这个奖</a>)。就在几周前，同一个团队<a class="ae jp" href="https://arxiv.org/abs/1701.05157" rel="noopener ugc nofollow" target="_blank">在这种分析和Tononi-Koch信息整合理论之间建立了联系</a>。</p><p id="b47f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">大约在2015年末的同一时间，Eric Laukien正在开发<a class="ae jp" href="https://github.com/222464/NeoRL" rel="noopener ugc nofollow" target="_blank">高性能GPU技术</a>，大致基于我们在HTM社区中讨论的想法。2016年1月，埃里克、我们的朋友理查德·克劳德和我(在埃里克的父亲马克的支持下)成立了<a class="ae jp" href="https://ogma.ai/" rel="noopener ugc nofollow" target="_blank"> Ogma </a>来开发一种新技术，将理论与埃里克在GPU驱动的机器学习方面的经验相融合。奥格马是以奥格马·麦克·埃拉特汉的名字命名的，奥格米奥斯是盖尔人的半神人，也是爱尔兰第一个书写系统奥格汉姆的发明者。</p><p id="0f47" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">去年九月，我们发表了第一篇关于我们现在称之为<em class="jq">费曼机器</em>的论文<a class="ae jp" href="https://arxiv.org/abs/1609.03971" rel="noopener ugc nofollow" target="_blank">，并发布了我们</a><a class="ae jp" href="https://github.com/ogmacorp" rel="noopener ugc nofollow" target="_blank"> OgmaNeo软件套件</a>的第一个版本。</p><p id="f111" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">正如该理论所预测的，大脑区域只是各种自适应学习NDS的一个例子，当这些NDS适当地连接在一起时，它们将自组织并紧急合作，形成费曼机器。没有必要模仿真实神经元的所有细节(如人脑项目模型中所见)，甚至也没有必要模仿HTM中神经元、列和层的更简单的抽象。同样，传统的深度学习给了我们一些并行建模和实现高性能、自适应数据转换的想法，但我们也可以避免它的许多限制，因为费曼机器中的每个模块都是半独立的预测学习器，并且学习是在线的、本地的和无监督的。</p><p id="3069" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">费曼机器是为了纪念我们的英雄理查德·p·费曼而命名的，原因有很多。从洛斯阿拉莫斯到20世纪80年代参与早期大规模并行算法，他都是计算领域的先驱，是约翰·冯·诺依曼的同事，后者现在是现代数字计算机架构的同义词，尽管冯·诺依曼的设计灵感来自于他对皮层神经元的理解。我们希望，如果费曼能活着看到神经科学和应用数学的最新进展，他会理解这些想法的简单性，甚至可能已经自己想出了它们。我们最近发现，他的姐姐琼(Joan)自己也是一位杰出的物理学家，她的一篇开创性论文就是基于塔肯斯定理。我们可能有理由称之为费曼-费曼机器，因为理查德可能有这种洞察力，如果琼有机会解释NDSs的力量。</p><p id="654e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">无论如何，我们的论文在发表的那一周被列为《麻省理工技术评论》<a class="ae jp" href="https://www.technologyreview.com/s/602455/the-best-of-the-physics-arxiv-week-ending-september-24-2016/" rel="noopener ugc nofollow" target="_blank">最佳物理学论文arXiv </a>之一，我们将在3月份由麻省理工学院的<a class="ae jp" href="http://cbmm.mit.edu/" rel="noopener ugc nofollow" target="_blank">大脑、思维和机器中心</a>组织的即将举行的<a class="ae jp" href="https://www.src.org/calendar/e006125" rel="noopener ugc nofollow" target="_blank">神经启发计算元素</a>研讨会和<a class="ae jp" href="http://cbmm.mit.edu/knowledge-transfer/workshops-conferences-symposia/science-intelligence-computational-principles" rel="noopener ugc nofollow" target="_blank"> AAAI智能科学春季研讨会</a>上展示海报(如果你也参加，请顺便来看看我们的海报)。</p><p id="2816" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我将在下一篇文章中解释费曼机器的细节，但同时这里有一个最近在NDSs的“Hello World”上运行的演示，Lorenz系统:</p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="jy jx l"/></div></figure><p id="c99e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">“嘈杂的”洛伦兹吸引子(基于最近的论文)在这里被使用，因为它比普通的NDS更具挑战性。关于这个演示，有几个要点可能不太明显。首先，左上角的步数是FM随机初始化后看到的数据点数，所以这是在线学习，完全从零开始，没有预训练。第二，FM看到的信号只是高度嘈杂的观察结果，而不是大多数视频中显示的平滑Takens轨迹，所以相似之处是因为FM在嘈杂的数据中找到了真实的信号。第三，这代表了在我的Macbook Pro上以每秒60步的速度运行的整个8层网络，其中包括绘制所有的3D图形。</p><p id="76a6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">请继续关注我的第二篇文章，它将描述费曼机器的内部工作原理。</p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="jz jx l"/></div></figure></div></div>    
</body>
</html>