<html>
<head>
<title>Dynamically Expandable Neural Networks — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动态可扩展神经网络—解释</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/dynamically-expandable-neural-networks-ce75ff2b69cf?source=collection_archive---------1-----------------------#2017-09-19">https://medium.com/hackernoon/dynamically-expandable-neural-networks-ce75ff2b69cf?source=collection_archive---------1-----------------------#2017-09-19</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/b2bd5ebd36d3f29c7e8fdb203f14ab9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntFNso1dKykDvPBHfdU_5w.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek">A Machine, Learning. by <a class="ae ih" href="http://robertaguileradesign.com" rel="noopener ugc nofollow" target="_blank">Robert Aguilera</a></figcaption></figure><div class=""/><p id="09bd" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">神经网络可以相当容易地学习复杂的表示。但是，在一些任务中，新数据(或数据类别)会不断变化。例如，你可以训练一个<a class="ae ih" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>来识别8种不同类型的猫的图片。但是将来，你可能会想把它改成12个品种。如果网络必须让<a class="ae ih" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">随着时间的推移不断学习</a>新数据，这被称为持续学习问题。本文讨论了一种非常新的技术，这种技术试图以重新训练整个模型的一小部分成本不断适应新数据。</p><blockquote class="kf kg kh"><p id="7d20" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">本文基于<a class="ae ih" href="https://arxiv.org/pdf/1708.01547v2.pdf" rel="noopener ugc nofollow" target="_blank">的动态扩展网络终身学习</a></p></blockquote><h1 id="f422" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">这篇文章的结构</h1><p id="800a" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">本文沿用原论文。我添加我自己的比特和解释材料来简化它。</p><blockquote class="kf kg kh"><p id="1d4a" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">本文需要一些关于神经网络的知识，包括权重、正则化、神经元等。</p></blockquote><h1 id="5f45" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">什么是持续学习</h1><p id="6088" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">持续学习只是意味着能够随着时间的推移不断学习。数据随着时间的推移按顺序到达，算法必须学会能够预测新数据。通常，使用像迁移学习这样的技术，其中模型根据以前的数据进行训练，并且使用来自该模型的一些特征来学习新数据。这通常是为了减少从零开始训练模型所需的时间。当新数据稀疏时也使用它。</p><h1 id="6faf" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">问题是</h1><p id="19f9" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">执行这种学习的最简单方法是基于更新的数据不断地微调模型。然而，如果<strong class="jj il">新任务</strong>与旧任务<strong class="jj il">非常不同</strong>，模型将无法在该新任务中很好地执行，因为旧任务的功能是无用的，例如，如果一个模型是在一百万张动物图像上训练的，那么如果它在汽车图像上进行微调，它可能不会很好地工作。在试图检测汽车时，从动物身上学到的特征不会很有用。</p><p id="5ed0" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">另一个问题是在<strong class="jj il">微调</strong>后，模型可能开始执行<strong class="jj il">原</strong>任务<strong class="jj il">差</strong>(本例中预测动物)。例如，斑马身上的条纹与条纹t恤或栅栏有着非常不同的含义。微调这样的模型会降低其识别斑马的性能。</p><h1 id="d9ca" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">介绍动态可扩展网络</h1><p id="0b80" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在很高的层面上，扩展网络的想法是非常符合逻辑的。训练一个模型，如果它不能很好地预测，增加它的学习能力。如果一个新的任务与现有的任务有很大的不同，从旧的模型中提取任何有用的信息并训练一个新的模型。作者使用这些逻辑思想和发展的技术，使这样的建设成为可能。</p><p id="feab" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">作者介绍了3种不同的技术来实现这样一个框架。将详细讨论每种方法，但在很高的层次上，它们是:</p><ol class=""><li id="4714" class="lp lq ik jj b jk jl jo jp js lr jw ls ka lt ke lu lv lw lx dt translated"><strong class="jj il">选择性再训练</strong>——找到与新任务相关的神经元并保留下来。</li><li id="ce14" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il">动态网络扩展</strong> —如果模型无法从步骤1 <em class="ki">中学习(即损失高于阈值)</em>，则通过添加更多神经元来增加模型的容量。</li><li id="d206" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il">网络分割/复制</strong> —如果一些新型号的单位已经开始发生巨大变化，复制这些权重，并重新训练这些副本，同时保持旧权重固定。</li></ol><figure class="me mf mg mh fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff md"><img src="../Images/7bfffeabc89374899ccc73542a8f2665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-CXYqqHKxws2vntucWcePg.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Figure 1.0 Left</strong>: Selective training. <strong class="bd mi">Center</strong>: Dynamic Expansion. <strong class="bd mi">Right</strong>: Network Split.</figcaption></figure><p id="1a39" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在上图中，数字<strong class="jj il"> <em class="ki"> t </em> </strong>表示任务号。因此，<strong class="jj il"> <em class="ki"> t-1 </em> </strong>表示前一个任务，<strong class="jj il"> <em class="ki"> t </em> </strong>表示当前任务。</p><h1 id="655b" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated"><strong class="ak">选择性再培训</strong></h1><p id="c9e1" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">训练一个新模型的最简单的方法是在每次新任务到来时训练整个模型。然而，因为深度神经网络可以变得非常大，所以这种方法将变得非常昂贵。</p><p id="b824" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了防止这样的问题，作者提出了一种新的技术。在第一个任务中，用<strong class="jj il"> L1正则化</strong>训练模型。这确保了网络中的稀疏性，即只有一些神经元连接到其他神经元。我们一会儿就会明白为什么这是有用的。</p><figure class="me mf mg mh fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mj"><img src="../Images/a3e3dfa55547ad42f926193dfd1bc3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTn144_8p23YwIRFfPwjgg.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Equation 1.0 </strong>Loss function for initial task.</figcaption></figure><p id="e668" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj il"> W^t </strong>表示模型在时间t的权重。在这种情况下，t =1。<strong class="jj il"> D_t </strong>表示在时间<strong class="jj il"> t </strong>的训练数据。方程的右半部分，从<strong class="jj il"> μ </strong>开始，简单来说就是<strong class="jj il"> L1 </strong>正则项，<strong class="jj il"> μ </strong>是正则化强度。<strong class="jj il"> L </strong>表示网络从第一层到最后一层的层数。这种调节试图使模型的权重接近(或等于)零。你可以在这里  阅读关于l1和l2正规化<a class="ae ih" href="http://cs231n.github.io/neural-networks-2/#reg" rel="noopener ugc nofollow" target="_blank"> <strong class="jj il"> <em class="ki">。</em></strong></a></p><p id="fabe" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">当需要学习下一个任务时，在模型的最后一层安装一个稀疏线性分类器，然后使用以下方法训练网络:</p><figure class="me mf mg mh fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mk"><img src="../Images/67c8da53a64a9eb10e3fbcce69b6f2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kka_xpV3bYp_YdI5_16ZQw.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Equation 1.1 </strong>Training the network for the next task</figcaption></figure><p id="1a50" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj il"> <em class="ki">符号:</em> </strong></p><figure class="me mf mg mh fq hw fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/8c95b5d37349b841f2a3cf6b0dd26258.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*zpWZvZIo6cMQ5g58IogP_g.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Equation 1.2</strong></figcaption></figure><p id="d764" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这意味着除最后一层之外的所有层的权重。所有这些层(从第一层到最后一层)都是固定的，而只有新添加的层使用相同的l1正则化进行优化，以促进稀疏连接。</p><p id="3f67" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">建立这种稀疏连接有助于识别模型其余部分中受影响的神经元！使用<strong class="jj il">广度优先搜索</strong>完成查找，这是一种非常流行的搜索算法。然后，只更新那些权重，节省了大量的计算时间，没有连接的权重不会被触及。这也有助于防止消极学习，即旧任务的表现下降。</p><h1 id="1034" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">动态网络扩展</h1><p id="ad5b" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">选择性再训练适用于与旧任务高度相关的任务。但是当更新的任务有相当不同的分布时，它将开始失败。作者使用另一种技术来确保通过增加网络容量来表示更新的数据。他们通过增加额外的神经元来做到这一点。这里将详细讨论他们的方法。</p><p id="821a" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">假设你希望通过<em class="ki"> k </em>个神经元来扩展网络的第L <em class="ki">层</em>层。该层(和前一层)新加权矩阵将看起来具有维度:</p><figure class="me mf mg mh fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mj"><img src="../Images/d76780298ba00e448671fbeee51c3f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MC0aFab9H51p6IOuKBfuQ.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Equation 3.0</strong></figcaption></figure><p id="7eaf" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">𝒩是加上k个神经元后的神经元总数。</p><p id="f908" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">通常我们不希望增加<strong class="jj il"> <em class="ki"> k </em> </strong>神经元。相反，我们希望网络能够计算出要添加的神经元的正确数量。幸运的是，已经有一种技术使用<a class="ae ih" href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" rel="noopener ugc nofollow" target="_blank">套索</a>来调整网络，使其具有稀疏的权重(然后可以删除)。该技术在论文<a class="ae ih" href="https://arxiv.org/pdf/1607.00485.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络的组稀疏正则化</a>中有详细描述。</p><p id="09c4" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我不会在这里进入细节，但在一个层上使用它会产生这样的结果(套索组是使用的技术):</p><figure class="me mf mg mh fq hw fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/12c9fb1e0231dd69b5408aedf46e9a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*vqgHmCjqELtjwYXMZ53hpA.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Figure 2.0</strong> Comparision between regularisation methods. Grayed cells represent removed connections.</figcaption></figure><p id="1bb8" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">作者使用了一个层基础(只在新增加的k个神经元上)而不是整个网络。该技术用于尽可能多地取消连接，只保留最相关的连接。这些神经元随后被移除，使模型变得紧凑。</p><h1 id="1ecc" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated"><strong class="ak">网络拆分/复制</strong></h1><p id="b14d" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">迁移学习中有一个常见的问题，称为<em class="ki">语义漂移</em>，或<em class="ki">灾难性遗忘</em>，模型缓慢地移动其权重，以至于忘记了原始任务。</p><p id="6c6e" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">虽然可以添加L2正则化，确保权重不会发生显著变化，但如果新任务差异很大，这也无济于事(在某个点之后，模型将无法学习)。</p><p id="a573" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">相反，如果神经元移动超过一定范围，最好复制神经元。如果神经元的值改变超过某个值，则制作该神经元的副本，并且发生分裂，并且该复制单元作为副本被添加到同一层。</p><p id="98a7" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">具体地，对于隐藏单元<em class="ki"> i，</em>如果新权重和旧权重之间的l2距离(ρ_i)是&gt; 𝜎，则进行拆分。𝜎是一个超参数。分裂之后，整个网络将需要再次训练，但是收敛很快，因为初始化不是随机的，而是具有合理的最优值。</p><h1 id="6756" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">培训和评估</h1><h2 id="d3ef" class="mn kn ik bd ko mo mp mq ks mr ms mt kw js mu mv la jw mw mx le ka my mz li na dt translated">数据集</h2><p id="091a" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">使用了三个数据集。它们是:</p><ol class=""><li id="51b0" class="lp lq ik jj b jk jl jo jp js lr jw ls ka lt ke lu lv lw lx dt translated"><strong class="jj il">MNIST——变异</strong>。这个数据集由62，000张从0到9的手写数字图像组成。数字是旋转的，背景中有噪声(不像MNIST)。</li><li id="65de" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il"> CIFAR-10 </strong>。这个数据集由60，000张普通物体的图像组成，包括车辆和动物。</li><li id="efe7" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il"> AWA级。这个数据集由50种动物的30，475张图片组成。</strong></li></ol><h2 id="953b" class="mn kn ik bd ko mo mp mq ks mr ms mt kw js mu mv la jw mw mx le ka my mz li na dt translated"><strong class="ak">车型</strong></h2><p id="70c6" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">为了比较性能，使用了各种模型。它们是:</p><ol class=""><li id="2f43" class="lp lq ik jj b jk jl jo jp js lr jw ls ka lt ke lu lv lw lx dt translated"><strong class="jj il"> DDN-STL。</strong>深度神经网络，针对每项任务分别进行训练。</li><li id="a301" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il"> DNN-MTL </strong>一个训练有素的DNN。</li><li id="118f" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il"> DNN-L2。</strong>经过L2正规化训练的DNN，介于当前任务和先前任务的权重之间。</li><li id="4043" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il"> DNN-EWC。</strong>使用弹性重量巩固训练的DNN。我们在这篇文章中没有详细介绍，但是你可以在这里阅读。</li><li id="b9e2" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il">DNN——进步派。</strong>我们没有在这篇文章中讨论这些，但是你可以在这里<a class="ae ih" href="https://arxiv.org/pdf/1606.04671.pdf" rel="noopener ugc nofollow" target="_blank">读到它们</a>。</li><li id="13a3" class="lp lq ik jj b jk ly jo lz js ma jw mb ka mc ke lu lv lw lx dt translated"><strong class="jj il">狼窝。</strong>动态可扩展网络(本文提到的技术)</li></ol><figure class="me mf mg mh fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff nb"><img src="../Images/b7bfb2d9baeb4ddb23664f7b481dd0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ciXL01sEaixemWDwWae8w.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd mi">Figure 4.0 Top Row</strong>: Average performance per task. <strong class="bd mi">Bottom Row</strong> Performance over network capacity.</figcaption></figure><p id="066a" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">很明显，den保持了它们的准确性，同时也确保了它们不会浪费参数。</p><h1 id="02b6" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">结论</h1><p id="3419" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们经历了一个明智地使用参数的最新技术，保持了准确性，并且与原始再训练相比使用了不到15%的参数。</p><p id="3fe2" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">虽然深度学习对于大型数据集来说肯定是一个很好的工具，但它们在空间使用方面很昂贵，使用这样的技术有助于减轻这些成本。</p><h1 id="fd7d" class="km kn ik bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">行动呼吁</h1><p id="cf2d" class="pw-post-body-paragraph jh ji ik jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">如果你喜欢这篇文章，请鼓掌👏🏻👏🏻你想待多久就待多久。它帮助我知道这个帖子是有用的。</p><p id="8d06" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我用简单的术语解释了深度学习的最新研究，希望每个人都能理解，所以如果你对这个话题感兴趣，请关注我。</p><blockquote class="kf kg kh"><p id="28f9" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">再次为罗伯特制作特色艺术品大声欢呼。你可以在http://robertaguileradesign.com的<a class="ae ih" href="http://robertaguileradesign.com" rel="noopener ugc nofollow" target="_blank">查看他的作品。</a></p></blockquote><figure class="me mf mg mh fq hw"><div class="bz el l di"><div class="nc nd l"/></div></figure></div></div>    
</body>
</html>