# 缺乏技术多样性的长尾危险:单色算法

> 原文：<https://medium.com/hackernoon/the-long-tail-danger-of-the-lack-of-diversity-in-tech-monochromatic-algorithms-6086c64517e8>

18，000 人聚集在格蕾丝·赫柏计算机界女性庆典上已经有几个星期了。

![](img/212bec6d8986cf9a04ceff3a4a96c861.png)

我参加了会议，重点是了解更多关于技术专家在人工智能(A.I .)和物联网(IoT)领域所做的事情。

就我个人而言，我对人工智能感兴趣，并对它如何应用于我作为信息架构师的工作以及聊天机器人和内容推荐引擎如何帮助我的用户更容易、更自然地找到他们正在寻找的东西感兴趣。我对物联网感兴趣，因为它适用于我的电子纺织品新手级工作。

带着如此多的印象和想法，我离开了会议，充满了灵感，却又张口结舌。我知道一些深刻的东西需要被梳理出来，但是什么呢？

在我准备和前往会议的过程中，我关注了 Twitter #ghc17，以获得会议现场的预览。

最上面的推文是一个不知疲倦的男性盟友发出的支持性战斗口号，我受到了鼓舞。随着时间的推移，这条推文仍然是最热门的推文，这让我感到好奇。

在我准备写这篇文章的时候，我查看了媒体本身，看看格蕾丝·赫柏的印象。

最高的结果是一个来自 2016 年格蕾丝·赫柏男性与会者的深思熟虑的帖子。这也让我疑惑。

在这次会议上，关于计算机领域女性的代表性声音——一个在头 24 小时上升到最高位置，另一个在一年后上升到最高位置——怎么可能不是女性的声音呢？

作为一名有色人种妇女，我并不感到震惊，因为我看到一长串不知道我的经历的人，不管他们喜欢与否，都占据了谈论和谈论我的经历的主导地位。

我不是愤怒地问。这里有一个:-)来证明。我这么问是因为这是个有趣的问题。

我认为有三个因素导致了这些结果:

*   内容
*   用户行为
*   算法

**内容**散发出一种信息气味，引发**用户行为**，使得**算法**将其识别为与寻找格蕾丝·赫柏信息的用户高度相关。

这是一剂上膛的药水。

人们可能会猜测，为什么一个男人支持女性的推文会上升到 Twitter 的顶部，胜过女性支持女性的推文。

人们可能会猜测，为什么在 Medium 的顶级搜索结果中没有出现更多关于格蕾丝·赫柏的女性声音，而是让一名男子在一年前的会议上的经历成为头条新闻。

但我想谈谈算法。

> 算法
> 
> 在计算或其它解决问题的操作中，尤指计算机所遵循的过程或一套规则。

为什么是算法？

> 不管是好是坏，算法是机器的上帝，为它们提供动力。(摘自[算法如何统治世界](https://www.theguardian.com/science/2013/jul/01/how-algorithms-rule-world-nsa))

这些媒体和 Twitter 上的例子说明了我从格蕾丝·赫柏那里学到的最重要的问题:单色算法。

> 我们今天看到的技术多样性的缺乏正在点燃一场人道主义危机，在这场危机中，无处不在的单色算法将边缘化任何没有参与其设计和创造的人。

在流程的这一点上，我们听到问题出现在这一层:

*   做同样的工作，女人的工资比男人低。
*   招聘过程中的歧视将女性和少数族裔排除在科技劳动力之外。
*   在科技人才候选人中，我们没有足够的女性和少数族裔。

但要明确的是，我说的是这些初始因素导致的长尾问题。

> 如果我们不采取正确的方式，我们未来的生活将会被那些不理解、不思考、不同情用户群体的单色技术人员所产生的算法所塑造。

以下是这些单色算法的几个例子:

机器偏见:全国各地都有用来预测未来罪犯的软件。而且对黑人有偏见。

[](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) [## 机器偏差- ProPublica

### 2014 年一个春天的下午，布里莎·波顿在去学校接她的神姐时迟到了，她发现了一个…

www.propublica.org](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) 

当你在日常生活中听到以下流行语时——人工智能、机器学习、神经网络、大数据、数据科学、物联网和“智能”任何东西——想想这些技术中的每一项都涉及到如何制定决策规则的算法。

想想这些算法和决策树目前是如何被一群可能无法解释这个世界上广泛的人类经验的哪怕一丁点的劳动力设计和编码的。实际上，这些视野狭窄的劳动力正在决定我们的医疗保健、教育、住房、环境、通信和娱乐的结果。

请记住，直到 1992 年，轮椅通道才成为公共场所的法定要求。

此外，请记住，公司也是人，其中一些公司本质上是算法，如果给予这些权利，这些算法可以在如何控制和塑造我们的生活方面有很大的回旋余地。

如果上面 Pro Publica 故事中引用的预测性警务算法是由一个团队设计的，即使只包括一个有色人种，它的评分系统会将种族作为一个因素吗？

> 然而，当博登和普拉特被关进监狱时，奇怪的事情发生了:一个计算机程序给出了一个分数，预测两人未来犯罪的可能性。Borden 是黑人，被认为是高危人群。Prater 是白人，被评为低风险。
> 
> 两年后，我们知道计算机算法得到了完全相反的结果。波登没有被指控任何新的罪行。普拉特因随后闯入一家仓库并窃取价值数千美元的电子产品而正在服刑八年。

当然，几个世纪以来，我们一直在使用有缺陷的数据，但现在的区别是，这些流行语在承诺一个更美好的世界时令人着迷，而实际上，它们可能只是坏数据的虚假外表，让我们相信这些算法知道得更好。

一个叫大数据的东西怎么会错？

一个物联网怎么可能不好——就是互联网(爱互联网)，但是有物(爱物)。

我以我的爸爸妈妈为例来进行过度概括。

我妈妈相信人工智能会通过变得比我们聪明并让我们屈从于它的意志来毁灭人类。

我爸爸相信人工智能将是仁慈的，不可能造成伤害，因为它将像《星际迷航》中的数据一样，只是知道如何做正确的事情。

随着人工智能越来越接近我们生活的核心，我们必须考虑费-李非博士( [@drfeifei](https://twitter.com/drfeifei4) )在格蕾丝·赫柏 2017 上的话:

> “人工智能一点也不做作。”

人工智能不会像纯粹的、客观的、上帝般的技术一样自行复活。有偏见的人，不管是有意识的还是其他的，都是通过设计和编码算法以及向众包模型提供数据来将“智能”投入人工智能的人，就像上面的谷歌翻译例子一样。

换句话说，人工智能只能和我们一样聪明，或者一样笨。

我的技术英雄杰伦·拉尼尔讲述了人工智能的神话，以及当下列因素存在时，一个危险的回音室是如何产生的:

根数据不正确。

我们盲目相信人工智能会做正确的事。

呈现数据的用户界面不允许我们质疑或指出数据中的缺陷。

[](https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai) [## 人工智能的神话| Edge.org

### 为了更进一步，我将重提我以前提出的一个论点，即它变成了一个经济…

www.edge.org](https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai) 

> 技术多样性的缺乏不仅仅是工作场所的公平和平等待遇，它也是一个更大的危机:在我们周围建立的数字和虚拟世界中的公平和平等待遇。

随着模拟世界继续衰落，虚拟世界越来越多地出现在我们生活的最前沿，我们是要纠正模拟世界中固有的错误和偏见，还是要重新审视这些悲剧？

浅川智惠子在格蕾丝·赫柏分享了她利用技术帮助盲人更加独立和导航世界的工作，我对此感到敬畏。

与此同时，它强调了我们的世界是如何以视觉为中心，以及当你需要改造世界以满足你的需求时，当你的体验在设计中没有被考虑时，这是一场多么艰难的战斗。

我们必须发展我们的技术员工队伍，以反映我们生活的现实世界的多样性，并在能够影响将塑造我们生活的技术的岗位上包括不同的声音。

如果我们允许这些算法和我们居住的虚拟世界只由一小部分人建立，这些系统将不可避免地对那些没有被代表的人产生偏见。

我们将错过文明史上最大的机会之一，去纠正我们在这个世界上生存的最根本和最深刻的错误。

想象一下基于性别和种族的貌相和歧视的终结，以及人类可以在没有这些由有缺陷的陈规定型观念和数据造成的限制的情况下走向何方。

想象一个世界，在这个世界里，我们真正被看待和重视是因为我们的真实行为，而不是因为我们被人为地预测要做什么或想要什么。

想象一下这样一个世界，科技承诺让世界变得更美好，并且实际上成功做到了这一点，不仅仅是为了狭隘的人口统计，而是为了所有人。

[](https://futureoflife.org/ai-principles/) [## 人工智能原理-生命的未来研究所

### 这些原则是结合 2017 年 Asilomar 会议(视频在此)通过流程制定的…

futureoflife.org](https://futureoflife.org/ai-principles/)