<html>
<head>
<title>DL04: Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DL04:反向传播</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/dl04-backpropagation-bbcfbf2528d6?source=collection_archive---------14-----------------------#2017-12-06">https://medium.com/hackernoon/dl04-backpropagation-bbcfbf2528d6?source=collection_archive---------14-----------------------#2017-12-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="9570" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">之前的三个帖子可以在这里找到:<br/> <a class="ae jp" href="https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864" rel="noopener ugc nofollow" target="_blank"> DL01:神经网络理论</a> <br/> <a class="ae jp" href="https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257" rel="noopener ugc nofollow" target="_blank"> DL02:从零开始写一个神经网络(代码)</a> <br/> <a class="ae jp" href="https://hackernoon.com/dl03-gradient-descent-719aff91c7d6" rel="noopener ugc nofollow" target="_blank"> DL03:梯度下降</a></p><p id="a88b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，欢迎阅读本系列的第4部分！这需要一点数学知识，所以基础微积分是先决条件。</p><p id="ac32" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在这篇文章中，我将尝试用一个非常简单的神经网络来解释backprop，如下所示:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/b2933744673c20577f6d1d3a2fa7fa34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0Zju-mXN8QdznSrHSOClw.jpeg"/></div></div></figure><p id="883b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">L1、L2和L3代表神经网络中的层。方括号中的数字代表层数。我把每一层的每个节点都编号了。例如，第一层的第二节点被编号为2[1]，等等。我也给每个重量都贴上了标签。例如，连接第二层的第二节点(2[2])和第三层的第一节点(1[3])的权重是w21[2]。</p><p id="bd20" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我假设我们用的是激活函数g(z)。</p><p id="af48" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">反向传播背后的基本概念是计算误差导数。前向通过网络后，我们计算误差，然后通过<a class="ae jp" href="https://hackernoon.com/dl03-gradient-descent-719aff91c7d6" rel="noopener ugc nofollow" target="_blank">梯度下降</a>更新权重(使用反向传播计算的误差导数)。</p><p id="3e39" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果你理解微分中的<a class="ae jp" href="http://www.mathcentre.ac.uk/resources/uploaded/mc-ty-chain-2009-1.pdf" rel="noopener ugc nofollow" target="_blank">链式法则</a>，你将很容易理解反向传播。</p><p id="975f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，让我们写出向前传球的方程式。</p><p id="c759" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">设x1和x2为L1的输入。</p><p id="c1bb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我将<code class="eh kc kd ke kf b">z</code>表示为前一层的加权和，<code class="eh kc kd ke kf b">a</code>表示应用非线性/激活函数<code class="eh kc kd ke kf b">g</code>后一个节点的输出。</p><blockquote class="kg kh ki"><p id="5300" class="ir is kj it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">Z1[2]= w11[1]* x1+w21[1]* x2<br/>a1[2]= g(Z1[2])<br/>z2[2]= w12[1]* x1+w22[1]* x2<br/>a2[2]= g(z2[2])<br/>z3[2]= w13[1]* x1+w23[1]* x2<br/>a3[2]= g(z3[2])</p><p id="e2d7" class="ir is kj it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">Z1[3]= w11[2]* a1[2]+w21[2]* a2[2]+w31[2]* a3[2]<br/>a1[3]= g(Z1[3])</p></blockquote><p id="c392" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我用MSE作为损失函数。</p><blockquote class="kg kh ki"><p id="1087" class="ir is kj it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">e =(1/2)×(a1[3]—t1)，其中t1为目标标签。</p></blockquote><p id="922c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了使用梯度下降来反向传播网络，我们必须计算这个误差对每个权重的导数，然后执行权重更新。</p><p id="80ec" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们需要找到dE/dwij[k]，其中，wij是第k层的权重。</p><p id="83f8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">根据链式法则，我们有</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kn"><img src="../Images/80e23fc9674ea5f82aec41bdb272239f.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*UUOmsvG0OahUucVQqL0GBg.jpeg"/></div></figure><p id="262d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在，我们可以计算RHS上的这三项，如下所示:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ko"><img src="../Images/db64fe9c46af7cec852f6bd5d09b609a.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*N_6gvkugnzzgS4GPuFyLog.jpeg"/></div></figure><p id="202c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我们有</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kp"><img src="../Images/d8160d14a3a11a4c6a015066040cbae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*VREB_naHOT0k057iD8qqbg.jpeg"/></div></figure><p id="76a5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">同样的，</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kq"><img src="../Images/7759d7e19b1070279057eb55dc7d3e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*TBbylbnWyCtEhazhJBcNGg.png"/></div></figure><blockquote class="kg kh ki"><p id="326e" class="ir is kj it b iu iv iw ix iy iz ja jb kk jd je jf kl jh ji jj km jl jm jn jo hn dt translated">设δ1[3] = (a1[3] — t) * (g'(z1[3])。</p></blockquote><p id="2d88" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我们有</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kr"><img src="../Images/5c877828697e6de99fba995e7947632c.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*wbFkxtG5voNtfJ5i0IJIbw.png"/></div></figure><p id="7627" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里，我们将δ1[3]称为第3层的节点1传播的误差。</p><p id="51a3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在，我们想回到上一层。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ks"><img src="../Images/488fb0a1722eff15b880eb99a22cb4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*WccIMtwvqXdQeMllLPRZDA.jpeg"/></div></figure><p id="7a07" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">关于简化，我们得到</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kt"><img src="../Images/390c2b2ecc3267e2e3d86c67443ca7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*JrhOKyqYAUDRzr82EzVfrQ.jpeg"/></div><figcaption class="ku kv fg fe ff kw kx bd b be z ek">NOTE: The value of weight w11[2] is to be used before the update was performed on it. This applies to all equations and weights below.</figcaption></figure><p id="e3d8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">同样的，</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ky"><img src="../Images/a90c21cf92e07b36a11c296a530da184.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*7oEfJi_L2eDbA6-BglKSmw.jpeg"/></div></figure><p id="c929" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">类似地，通过L2的其他节点，我们得到</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ky"><img src="../Images/081f9a4b3fa4158794867c03d8534715.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*l_2601RnCyK__RRyWhsudA.jpeg"/></div></figure><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ky"><img src="../Images/33ab1487aab32d9d6ac488320bad54df.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*4A0HGmnDnGMAwgb333BTyg.jpeg"/></div></figure><p id="2843" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">就δ而言，这些可以写成</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kz"><img src="../Images/65a19bdc0ebd1f7dd05a1a4e96464ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*FXp3V3KeL0RiEIL6nURM7g.png"/></div></figure><p id="c810" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当我们得到所有的误差导数时，权重更新为:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff la"><img src="../Images/7abeae4c08dd4c8d426d755de9d33582.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*pusHVu4zXwvKKGztKKwO1Q.png"/></div><figcaption class="ku kv fg fe ff kw kx bd b be z ek">(wij[k] refers to weight wij at layer k)</figcaption></figure><p id="6ed3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">其中η称为“学习率”，</p><p id="fe36" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以，这是对你的支持！如果你的大脑现在是这样的话，这是完全可以理解的:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lb"><img src="../Images/3b531300051945aa0666b6f6d9e1dd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1U-zVev1IWd9sZbwsvD5w.jpeg"/></div></div></figure><p id="a4e4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了更好地理解这一点，您可以尝试自己推导它。</p><p id="9658" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">你甚至可以对一些其他的误差函数，比如交叉熵损失(用softmax)来做。或者用于特定的激活功能，如sigmoid、tanh、relu或leaky rely。</p><p id="4d2a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我希望通过这篇文章，我能够弄清楚反向传播的基础。我们为此投入了大量的时间和精力，因此非常感谢您的反馈！</p></div></div>    
</body>
</html>