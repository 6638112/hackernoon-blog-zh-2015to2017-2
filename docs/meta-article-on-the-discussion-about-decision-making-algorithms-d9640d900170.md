# 关于决策算法讨论的元文章(第 1 部分)

> 原文：<https://medium.com/hackernoon/meta-article-on-the-discussion-about-decision-making-algorithms-d9640d900170>

![](img/0dee2b89540b16f0515c1a5f191a1f34.png)

[http://sloanreview.mit.edu/article/what-to-expect-from-artificial-intelligence/](http://sloanreview.mit.edu/article/what-to-expect-from-artificial-intelligence/)

[注:更多文章摘要和信息更新见下面的[原理算法](http://theprincipledalgorithm.com/index.php/2017/06/03/article-summary-discussion-around-decision-making-algorithms/)

**2014，HBR.org:** [**一个人-算法决策的过程**](https://hbr.org/2014/09/a-process-for-human-algorithm-decision-making)

*   组织决策=收集事实>选项列表>做出选择>采取行动
*   分析(即一个算法过程)可以自动化这个过程的一部分
*   重新设计流程，使人们的角色与之相适应
*   “将决策流程效率提高多达 25%”
*   可以导致广泛的组织变革

**2017，NPR.org:** [**算法会侵蚀我们的决策技能吗？**](http://www.npr.org/sections/alltechconsidered/2017/02/08/514120713/will-algorithms-erode-our-decision-making-skills)

*   引用了皮尤研究中心的报告
*   担心算法降低我们的决策能力；我们会变得过于依赖科技
*   “在我们能够想象的几乎任何领域，算法都是人类决策的新仲裁者”
*   将人非人化，作为流程的“输入”
*   继续引用皮尤报告中的参与者

**2017，Farnam Street 博客:** [**算法是否在复杂决策方面击败了我们？**](https://www.farnamstreetblog.com/2017/03/algorithms-complex-decision-making/)

*   引用保罗·米尔 1954 年的书:发现“数据驱动的算法可以比训练有素的临床心理学家更好地预测人类行为——而且标准更简单”
*   “给定同一组数据两次，我们会做出两种不同的决定。噪音。内部矛盾。”
*   大量引用卡尼曼的“[思考，快与慢](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)”

**2016，article19.org:** [**预防犯罪背景下的算法和自动化决策**](https://www.article19.org/resources.php/resource/38579/en/algorithms-and-automated-decision-making-in-the-context-of-crime-prevention)

*   文章是[链接报告](https://www.article19.org/data/files/medialibrary/38579/Algorithms-and-Automated-Decision-Making-in-the-Context-of-Crime-Prevention-Final.pdf)的执行摘要
*   总之:“第 19 条认为，确保在算法决策的背景下保护人权是重要的”
*   通过算法使用自动化处理行业中的许多示例

**2017，public technology . net:**[**决策质询中的算法:斯蒂芬妮·马西森论挑战议员追究责任**](https://www.publictechnology.net/articles/opinion/algorithms-decision-making-inquiry-stephanie-mathisen-challenging-mps-investigate)

*   “算法已经取代人类做出影响我们生活许多方面的决策，其规模足以深刻影响社会”
*   “计算机算法的不同之处在于决策中可以考虑的数据的庞大数量和复杂性，以及系统地应用错误和辨别的潜力”
*   “算法缺乏透明度是一个严重的问题”
*   “算法也只和它们所利用的数据一样公正”
*   “去年 11 月发布了一份[建议行为准则](http://www.fatml.org/resources/principles-for-accountable-algorithms)，其中包括好算法的五项原则:责任、可解释性、准确性、可审计性和公平性”[也可参考下面的麻省理工学院技术文章——如何让算法负责]]

**2017，守护者:** [**人工智能看门狗需要规范自动化决策，专家说**](https://www.theguardian.com/technology/2017/jan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions)

*   “应该建立人工智能监管机构，以确保人们不受歧视”
*   “这些系统可以，而且确实会做出严重影响人们生活的错误决定”
*   在英国,[数据保护法案](https://www.gov.uk/data-protection/the-data-protection-act)允许对自动决策提出质疑
*   “去年通过的欧盟 GDPR 最终版本不包含对‘解释权’的法律保障。”
*   “可能会发现很难监管算法”…“因为一些现代人工智能方法，如深度学习，是‘根本不可思议的’”
*   “算法做出的决定需要根据它们的用途以不同的方式进行解释”
*   各种算法变坏的例子

**2017，bossemergingleaders . com . au:**[**算法是否做出更好的决策？**](http://www.bossemergingleaders.com.au/2017/05/06/do-algorithms-make-better-decisions/)

*   “这种(深度学习)算法只有在问题领域得到很好理解并且有训练数据可用的情况下才有效”
*   “他们需要一个稳定的环境，未来的模式与过去的模式相似”
*   "机器学习算法就像用来[训练]它们的数据一样没有偏见. "
*   “没有人知道，甚至这些算法的创造者也不知道，这些算法究竟是如何做出决定的”
*   “将决策委托给这样的算法，将意味着我们将决策的责任转移给那些负责训练它们的人，实际上是将我们的道德外包出去”
*   “具有讽刺意味的是，由于过去数据的基础，这种被认为是颠覆性的[技术](https://hackernoon.com/tagged/technology)无法很好地应对颠覆性的变化”

**2017，boingboing.net:** [**算法决策:熵、程序员和裁判的军备竞赛**](http://boingboing.net/2017/06/01/adversarial-computing.html)

*   “‘熵力’使算法决策工具随着时间的推移变得越来越差，需要不断地维护和改进它们”
*   “鉴于这些不对称，机器学习可以安全部署来解决的各种问题的分类”
*   文章基于内斯塔的胡安·马特奥斯·加西亚的论文

**2016，麻省理工科技评论:** [**如何让算法承担责任**](https://www.technologyreview.com/s/602933/how-to-hold-algorithms-accountable/)

*   “鉴于这些算法决定的字面意义上的改变生活的性质，他们应该得到仔细的关注，并对负面后果负责”
*   “(机器学习算法的)结果是由人为的设计决策、关于优化什么的规则以及关于使用什么训练数据的选择形成的”
*   文章从五个核心原则的角度考虑了“问责制:责任、可解释性、准确性、可审计性和公平性”

**2016，ZDNet:** [**黑匣子里面:理解 AI 决策**](http://www.zdnet.com/article/inside-the-black-box-understanding-ai-decision-making/)

*   “现在*正在发生的事情*正在以越来越快的速度，将人工智能算法应用到能够显著影响人们生活的各种过程中——在工作中、在家里以及在旅行中。”
*   “这些算法中有许多是不值得仔细研究的”
*   “训练的关键是一个称为“[反向传播](http://neuralnetworksanddeeplearning.com/chap2.html)的过程，在这个过程中，带标签的例子被输入到系统中，中间层的设置被逐步修改，直到输出层提供与输入层的最佳匹配”
*   “我们相信，这种由人来查看材料并做出最终决定的管理方式，对于关键应用来说是正确的方式”
*   “你会希望可视化各层上发生的事情，以及它们如何处理数据，并使哪条证据导致哪项决策变得更加透明，这样网络不仅会产生结果，还会指出证据和推理过程”
*   “我们应该关注人们可能会用今天的人工智能技术做什么”

**2016，Pro Publica:** [**制作算法问责**](https://www.propublica.org/article/making-algorithms-accountable)

*   要求在基于数据的决策中遵循正当程序。参考了 Kate Crawford 的文章，并提到了欧盟 GDPR，其中获得解释的权利“可能会影响少数自动决策”
*   白宫“呼吁测试自动化决策工具的公平性，并开发‘算法审计’”
*   描述了他们在 Northpointe 的一个系统中发现的偏见[注:似乎他们已经改名为，或者已经被，[equivent](http://www.equivant.com/challenges/supervision-and-compliance-monitoring)]买断
*   “然而，随着我们迅速进入自动决策时代，我们应该要求更多的警告标签”

**2016，融合:** [**欧盟公民可能会获得对算法做出的决定的“解释权”**](http://fusion.kinja.com/eu-citizens-might-get-a-right-to-explanation-about-the-1793859992)

*   “欧盟成员国的公民可能很快就会有办法要求解释与他们相关的决策算法”
*   Calo 在电子邮件中解释了使用算法的公司是如何轻而易举地规避新规定的
*   “随着算法所依赖的系统(如神经网络)变得更加复杂，解释算法做出的决策只会变得更加困难”

**2016，对话:** [**以下是我们如何保护自己免受影响我们生活的隐藏算法**](http://theconversation.com/heres-how-we-can-protect-ourselves-from-the-hidden-algorithms-that-influence-our-lives-70674)

*   “算法可能被[编程为有偏差](https://theconversation.com/its-not-big-data-that-discriminates-its-the-people-that-use-it-55591)，或者无意的偏差可能会悄悄进入系统”
*   “英国工党最近呼吁不仅要加强对科技公司的监管，还要加强对算法本身的监管。”
*   “算法通常具有商业敏感性，利润极高”
*   “监管的焦点需要转移到算法的输入和输出上”
*   “公司必须能够在认为合适的时候使用自己的算法，并在事后对其滥用行为负责”
*   “一般公众仍然不知道这些法律方法[即“如果自动决策对他们有重大影响，人们可以反对这种决策”]，以控制公司活动”
*   “一个新的、全面的超级监管机构将会过于昂贵、笨拙，而且影响有限”

**2015，石板将来时:** [**政策机**](http://www.slate.com/articles/technology/future_tense/2015/04/the_dangers_of_letting_algorithms_enforce_policy.html)

*   “根据法律学者 Danielle Keats Citron 的说法，像预测性警务或远程福利资格这样的自动决策系统不再只是帮助政府机构中的人应用程序规则；相反，他们已经成为公共政策的主要决策者
*   “当算法决策超越筛选搜索结果，进入公共政策领域时，它就具有了新的意义”
*   “他们还提出了公平和公正的问题，挑战现有的正当程序规则，并可能威胁到美国人的福祉”
*   1) *我们需要了解更多关于策略算法如何工作的知识*
*   2) *我们需要解决算法的政治背景*
*   3) *我们需要解决算法*中的累积劣势沉淀
*   4) *我们需要尊重宪法原则，执行法律权利，加强正当程序*
*   “决策算法是一种在远处进行的政治形式，会产生大量令人不安的情绪波动”

**2013，ABC RN 后视(AU):** [**无人机打击的未来可以通过算法**](http://www.abc.net.au/radionational/programs/rearvision/drones/4703792) 来看执行情况

*   “五角大楼正在讨论用计算机算法取代人类无人机操作员的可能性”
*   “已经有人担心流动杀人机器将来会自动化”
*   “使用无人机进行战争的方式正在超越以往国际公约的限制，很可能需要制定新的交战规则”
*   “无人机不仅变得自主，还变得合作、更小、更灵活”
*   “无人机在非常精确的位置瞄准个人”
*   “关于无人机的一个大问题是，它们会改变决定部署致命武力的人的心理吗？我认为，在这一点上，很多人都会回答“是”

**2014，中:** [**大数据有多不公平**](/@mrtz/how-big-data-is-unfair-9aa544d739de)

*   “如果训练数据反映了针对某个少数民族的现有社会偏见，那么算法很可能会包含这些偏见”
*   “例如，种族和性别通常在任何足够丰富的特征空间中被冗余编码,不管它们是否明确存在”
*   “根据定义，关于少数民族的可用数据总是相对较少，这是事实”
*   “不同组之间分类准确性的差异是不公平的一个主要且未被充分认识的来源”
*   “如果公平迫使我们寻找更复杂的决策规则，那么实现公平可能在计算上代价高昂”

[![](img/50ef4044ecd4e250b5d50f368b775d38.png)](http://bit.ly/HackernoonFB)[![](img/979d9a46439d5aebbdcdca574e21dc81.png)](https://goo.gl/k7XYbx)[![](img/2930ba6bd2c12218fdbbf7e02c8746ff.png)](https://goo.gl/4ofytp)

> [黑客中午](http://bit.ly/Hackernoon)是黑客如何开始他们的下午。我们是 [@AMI](http://bit.ly/atAMIatAMI) 家庭的一员。我们现在[接受投稿](http://bit.ly/hackernoonsubmission)，并乐意[讨论广告&赞助](mailto:partners@amipublications.com)机会。
> 
> 如果你喜欢这个故事，我们推荐你阅读我们的[最新科技故事](http://bit.ly/hackernoonlatestt)和[趋势科技故事](https://hackernoon.com/trending)。直到下一次，不要把世界的现实想当然！

![](img/be0ca55ba73a573dce11effb2ee80d56.png)