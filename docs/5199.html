<html>
<head>
<title>One Shot Learning with Siamese Networks in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch的暹罗网络一次性学习</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e?source=collection_archive---------0-----------------------#2017-07-15">https://medium.com/hackernoon/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e?source=collection_archive---------0-----------------------#2017-07-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/146738a20725916974927720678f6c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A22PPyMSQRL5E_6IaJqhdg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Example of One Shot learning. <a class="ae jg" href="https://arxiv.org/pdf/1606.04080v1.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><blockquote class="jh ji jj"><p id="e9fe" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated">这是由两部分组成的文章的第1部分。你可以在这里阅读第二部分<a class="ae jg" rel="noopener" href="/@harshsayshi/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7"/></p></blockquote><p id="499c" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> D </span> eep神经网络是图像分类的首选算法。这部分是因为它们可以具有任意大量的可训练参数。然而，这是以需要大量数据为代价的，而这些数据有时是不可用的。我将在PyTorch中讨论一次<a class="ae jg" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>，它旨在缓解这样的问题，以及如何实现一个能够使用它的神经网络。</p><p id="b953" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><em class="jm">本文假设对</em> <strong class="jn hv"> <em class="jm">神经网络</em> </strong> <em class="jm">有些熟悉。</em></p><h2 id="1f4a" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">这篇文章的结构</h2><p id="62ba" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">这是一篇由两部分组成的文章。我将在<strong class="jn hv">第一部分</strong>中讲述<strong class="jn hv">理论</strong>，在<strong class="jn hv">第二部分</strong>中讲述理论的PyTorch <strong class="jn hv">实现</strong>。</p><p id="cb06" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">这篇文章从这篇论文中得到启示。</p><h1 id="059c" class="lv kw hu bd kx lw lx ly lb lz ma mb lf mc md me li mf mg mh ll mi mj mk lo ml dt translated">标准分类与一次性分类</h1><p id="94ec" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">几乎所有的分类模型都使用标准分类。输入被输入到一系列层中，最后，输出类别概率。如果您想要从猫中预测狗，您可以在预测时间内使用您预期的相似(但不相同)的狗/猫图片来训练模型。当然，这要求您拥有一个与使用模型进行预测时的预期相似的数据集。</p><p id="e1d6" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn hv">一次分类</strong>模型，另一方面，要求你只有<strong class="jn hv">一个你想预测的每一类的训练例子</strong>。该模型仍然在几个实例上被训练，但是它们只需要在与你的训练示例相似的域中。</p><p id="9871" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">面部识别就是一个很好的例子。您可以在包含各种角度、光照等的数据集上训练一个单镜头分类模型。一些人。然后，如果您想要识别某个人X是否在某个图像中，您可以拍摄该人的一张照片，然后询问模型该人是否在该图像中(<em class="jm">注意，该模型没有使用任何人X的照片进行训练</em>)。</p><p id="6212" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">作为人类，我们只要见过一次面就可以通过他/她的脸来识别一个人，这对计算机来说是可取的，因为很多时候数据是最少的。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><h1 id="9bae" class="lv kw hu bd kx lw mt ly lb lz mu mb lf mc mv me li mf mw mh ll mi mx mk lo ml dt translated">进入暹罗网络</h1><p id="e42c" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">暹罗网络是一种特殊类型的神经<a class="ae jg" href="https://upscri.be/hackernoon/network" rel="noopener ugc nofollow" target="_blank">网络</a>架构。不是模型学习<em class="jm">对其输入进行分类</em>，而是神经网络学习<em class="jm">区分两个输入之间的</em>。它学习它们之间的相似性。</p><h2 id="6714" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">建筑</h2><p id="0c34" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">连体网络由两个相同的神经网络组成，每个神经网络获取两个输入图像中的一个。然后，两个网络的最后一层被馈送到对比损失函数，该函数计算两幅图像之间的相似性。我制作了一个插图来帮助解释这个架构。</p><figure class="mz na nb nc fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff my"><img src="../Images/a4b433c2621c9bb5942aecf4f84d9743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzVUiq-3lYFtZEW3XfmKqg.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><em class="nd">Figure 1.0</em></figcaption></figure><p id="627e" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">有两个姐妹网络，它们是相同的神经网络，具有完全相同的权重。</p><p id="aac9" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">图像对中的每个图像都被馈送到这些网络之一。</p><p id="6dce" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">使用对比损失函数优化网络(我们将得到精确的函数)。</p><h2 id="6b22" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">对比损失函数</h2><p id="21f2" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">暹罗架构的目标不是分类输入图像，而是<em class="jm">区分它们之间的</em>。因此，分类损失函数(如交叉熵)并不是最合适的。相反，这种架构更适合使用对比函数。直观地说，这个函数只是评估网络如何区分给定的一对图像。</p><p id="7a0f" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">你可以阅读更多细节<a class="ae jg" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="0455" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">对比损失函数如下所示:</p><figure class="mz na nb nc fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ne"><img src="../Images/a1c5ff37d50709a53c09e4d6fbe7d1ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzGB6D97tHWR_-NJ8FKknw.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Equation 1.0</figcaption></figure><p id="b728" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">其中<strong class="jn hv"> <em class="jm"> Dw </em> </strong> <em class="jm"> </em>被定义为姐妹连体网络的输出之间的欧几里德距离。数学上欧几里得距离是:</p><figure class="mz na nb nc fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nf"><img src="../Images/051c2382be3eddfbef61fcc2e048f2ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6JCpYpYVJnpgYwupVIHSpg.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Equation 1.1</figcaption></figure><p id="e213" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">其中<strong class="jn hv"> <em class="jm"> Gw </em> </strong>是其中一个姐妹网络的输出。<strong class="jn hv"> X1 </strong>和<strong class="jn hv"> X2 </strong>是输入数据对。</p><h2 id="1cb0" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">等式1.0解释</h2><p id="13f3" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated"><strong class="jn hv"> Y </strong>不是1就是0。如果输入来自同一个类，则Y的值为0，否则Y为1</p><p id="55fe" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn hv"> max() </strong>是表示<strong class="jn hv"> 0 </strong>和<strong class="jn hv"> m-Dw </strong>之间较大值的函数。</p><p id="bc69" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn hv"> m </strong>是大于0的<strong class="jn hv">余量</strong>值。具有裕量表示超出该裕量的不同线对不会造成损耗。这是有意义的，因为你只想基于实际上不相似但网络认为相当相似的对来优化网络。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><h1 id="3246" class="lv kw hu bd kx lw mt ly lb lz mu mb lf mc mv me li mf mw mh ll mi mx mk lo ml dt translated">数据集</h1><p id="b46a" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">我们将使用两个数据集，经典的MNIST和OmniGlot。MNIST将用于训练模型，以了解如何区分字符，然后我们将在OmniGlot上测试该模型。</p><h2 id="da76" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">OmniGlot数据集</h2><p id="8bdd" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">OmniGlot数据集包含来自50种国际语言的示例。每种语言的每个字母只有20个例子。这被认为是MNIST的“转置”,那里的班级数量较少(10个),训练的例子很多。在OmniGlot中，类的数量非常多，但每个类的例子很少。</p><figure class="mz na nb nc fq iv fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/e6e40b6cb4062496abdd44a66e9df357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*sAGJLAeomb8Qp1iDMEq96A.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Figure 2.0. Some examples from the OmniGlot dataset</figcaption></figure><p id="e4d2" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">OmniGlot将被用作我们的on shot分类数据集，能够从少数几个例子中识别许多不同的类别。</p><h1 id="1250" class="lv kw hu bd kx lw lx ly lb lz ma mb lf mc md me li mf mg mh ll mi mj mk lo ml dt translated">结论</h1><p id="8ddf" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">我们掩饰了一次性学习的一般前提，并试图使用一种称为暹罗网络的神经网络架构来解决它。我们讨论了区分输入对的损失函数。</p><p id="7fcf" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在本文的第2部分，我们将实现这样一个架构，并在MNIST上训练它，然后在OmniGlot上预测它。</p><h2 id="cf6e" class="kv kw hu bd kx ky kz la lb lc ld le lf kj lg lh li kk lj lk ll kl lm ln lo lp dt translated">附言</h2><p id="0eab" class="pw-post-body-paragraph jk jl hu jn b jo lq jq jr js lr ju jv kj ls jy jz kk lt kc kd kl lu kg kh ki hn dt translated">如果你喜欢这篇文章，一定要留下❤.它让我知道我在帮忙。继续阅读<a class="ae jg" href="https://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7" rel="noopener ugc nofollow" target="_blank">第2部分</a>，该部分涵盖了所讨论想法的实施。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><p id="0177" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">继续第二部分:</p><p id="4f1e" class="pw-post-body-paragraph jk jl hu jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><a class="ae jg" href="https://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7" rel="noopener ugc nofollow" target="_blank">面部与PyTorch中的暹罗网络相似</a></p><figure class="mz na nb nc fq iv"><div class="bz el l di"><div class="nh ni l"/></div></figure></div></div>    
</body>
</html>