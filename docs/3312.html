<html>
<head>
<title>Neural Networks Without a PhD: Components of a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无博士学位的神经网络:神经网络的组成部分</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/neural-networks-without-a-phd-components-of-a-neural-network-9d98b056995b?source=collection_archive---------5-----------------------#2017-03-26">https://medium.com/hackernoon/neural-networks-without-a-phd-components-of-a-neural-network-9d98b056995b?source=collection_archive---------5-----------------------#2017-03-26</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="4a02" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">神经网络(也称为连接主义系统)是一种计算方法，它基于大量神经单元(也称为人工神经元)的集合，松散地模拟生物大脑解决由轴突连接的大型生物神经元集群问题的方式。— <a class="ae jt" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="3ad0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">本质上，<strong class="ix hv">神经网络</strong>是其生物对应物的数字表示，虽然这听起来令人生畏——除非你有很强的统计学或认知科学背景——但一旦你理解了神经网络的各个组成部分，神经网络就不会那么复杂。</p><p id="f34e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">为了便于说明，让我们看看最简单的神经网络，<strong class="ix hv">感知器</strong>:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/d996d916bbb1be03afa1ae96e3086cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wJhHYI0ZIQvn0O6m.png"/></div></div></figure><p id="a6fd" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">感知器是单个神经元的计算模型，正如我们所见，它由3个基本神经节点组成，每个节点都有独特的功能:</p><ul class=""><li id="30a7" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated"><strong class="ix hv">输入</strong>:也称为传感器，负责与软件或硬件通信，并将信号(数据)传递给神经网络，例如软件情况下的数据源或硬件情况下的网络摄像头。</li><li id="bfb2" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated"><strong class="ix hv">神经元</strong>:计算的基本单位，它从其他节点(传感器)获取输入，并进行计算和输出。</li><li id="5e4c" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated"><strong class="ix hv">输出</strong>:也称为执行器，允许我们的神经网络与其环境交互，输出可以是硬件或软件，例如返回真或假值或激活步进电机。</li></ul><h1 id="8165" class="kx ky hu bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">层</h1><p id="5157" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">除了神经元可以有更多层之外，每种节点类型通常都安排在一个层中，为了清楚起见，让我们来看一个更复杂的神经网络类型，一个<strong class="ix hv">前馈神经网络</strong>:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ma"><img src="../Images/114a9d70ac9ec046d9bb99c1ad46a130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dW6F1SvcHRdwg76x.png"/></div></div></figure><p id="643b" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">输入和输出层是不言自明的，但是什么是<strong class="ix hv">隐藏层</strong>？</p><h2 id="2ad1" class="mb ky hu bd kz mc md me ld mf mg mh lh ju mi mj ll jv mk ml lp jw mm mn lt mo dt translated">隐藏层</h2><p id="a761" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">隐藏层是一层或多层<strong class="ix hv">人工神经元</strong>的集合，它们负责进行计算，将输入转换成输出层可以使用的东西。为了理解这个隐藏层内部发生了什么，我们需要首先理解涌现的概念:</p><blockquote class="ir is it"><p id="7d75" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">在哲学、系统理论、科学和艺术中，涌现是一种现象，其中较大的实体通过较小或较简单的实体之间的相互作用而出现，使得较大的实体表现出较小/较简单的实体没有表现出的特性。— <a class="ae jt" href="https://en.wikipedia.org/wiki/Emergence" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="1453" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">每个隐藏层神经元都有一个内部机制，该机制控制神经元是否变得活跃，并向下一层发送自己的信号，该层可以是另一个隐藏层或输出层本身。</p><p id="49c0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">如果我们想分解它，我们可以说神经网络接受一个或多个输入，<strong class="ix hv">执行某种计算</strong>并返回一个值——通常以向量的形式——可用于进一步处理。</p><h1 id="3618" class="kx ky hu bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">神经元如何做决定</h1><p id="9c2c" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">为了理解神经元如何计算输入值并进而做出决策——也就是返回输出——我们需要为我们的神经网络理解增加两个元素。</p><ul class=""><li id="31ca" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated"><strong class="ix hv">连接</strong>:每个节点(输入、神经元、输出)之间的独立连接。</li><li id="8555" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated"><strong class="ix hv">连接权重</strong>:每个单独的权重代表节点间连接的强度。另一种思考方式是每个“神经元”对特定输入值的关心程度。</li></ul><p id="9638" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">有了这两个新组件，让我们回顾一下我们最初的感知器图:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/073b33f49ca0de3acbdec1da78ea3020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Sw8xIW7ldoNdsoLG.png"/></div></div></figure><p id="6ef8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">好了，现在我们已经扩展了关于神经元如何连接的知识，让我们看看它们如何根据输入做出决定。</p><h2 id="7ec4" class="mb ky hu bd kz mc md me ld mf mg mh lh ju mi mj ll jv mk ml lp jw mm mn lt mo dt translated">激活功能</h2><p id="2ebf" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">每个神经元将采用所有连接的输入及其相应的权重，并应用所谓的激活函数，然后返回单个输出值。</p><p id="72f2" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">现在，我们可以从这里开始进入神经网络背后的<strong class="ix hv">数学；但正如承诺，但现在我们不会；我们需要知道的是，神经网络可以使用许多常见的激活函数。举几个例子:</strong></p><ul class=""><li id="9cd8" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">阶跃函数</li><li id="946f" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">乙状结肠的</li><li id="4cd9" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">线性的</li><li id="3da7" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">高斯的</li></ul><p id="f677" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在我们的感知器将使用<strong class="ix hv">阶跃函数</strong>的情况下，将返回两个值<strong class="ix hv"> 0(关)或1(开)</strong>中的一个，这意味着我们的神经元将总是一个<strong class="ix hv">二进制值</strong>——也就是说，神经元要么激活，要么不激活，该函数的图形表示如下:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div class="fe ff mp"><img src="../Images/eac48f1e5f84ffac27bfab4dbbc5bf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*gL1s5KULzuZtLqAp.png"/></div></figure><h1 id="caf4" class="kx ky hu bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">把所有的放在一起</h1><p id="2402" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">让我们快速地把我们所学的东西放在一起，写一个简单的感知器:</p><figure class="jy jz ka kb fq kc"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="ca63" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这个python感知器正在做一些我们还没有涉及到的事情，比如<strong class="ix hv">训练</strong>，以及根据误差调整连接权重。目前，需要理解的唯一重要的事情是，我们的感知器接受3个输入，对这些值应用阶跃函数，并尝试预测输出值。</p><p id="5faa" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">运行感知器将返回如下内容:</p><pre class="jy jz ka kb fq ms mt mu mv aw mw dt"><span id="ceb9" class="mb ky hu mt b fv mx my l mz na">Training Perceptron for 1000 iterations<br/>Starting weights: [ 0.90185083  0.75965753  0.10658775]<br/>.......................................................<br/>Training completed<br/>Weights after training: [ 0.90185083  0.35965753 -0.49341225]<br/>Running trained Network against Test Data<br/>[0 0 0]: 0.0 -&gt; 0<br/>[0 1 0]: 0.359657525652 -&gt; 1<br/>[1 1 1]: 0.768096100245 -&gt; 1<br/>[0 0 1]: -0.49341225179 -&gt; 0<br/>[1 0 0]: 0.901850826383 -&gt; 1</span></pre><h2 id="ca3d" class="mb ky hu bd kz mc md me ld mf mg mh lh ju mi mj ll jv mk ml lp jw mm mn lt mo dt translated">结果呢</h2><p id="cd15" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">你能发现神经网络应该预测的模式吗？如果你认为你弄清楚了模式以及我们的感知机做得有多好，请留下评论。</p><h1 id="14a9" class="kx ky hu bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">摘要</h1><p id="4f0a" class="pw-post-body-paragraph iu iv hu ix b iy lv ja jb jc lw je jf ju lx ji jj jv ly jm jn jw lz jq jr js hn dt translated">我们在这个系列的第二篇文章中讨论了很多内容，现在我们对神经网络中的单个元素有了更好的理解，接下来我们将讨论T2神经网络拓扑或如何组织和连接我们所有的神经元。</p><p id="b7fb" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><em class="iw">本文原帖</em> <a class="ae jt" href="http://coderoncode.com/machine-learning/2017/03/26/neural-networks-without-a-phd-part2.html" rel="noopener ugc nofollow" target="_blank"> <em class="iw">在我自己的网站</em> </a> <em class="iw">。</em></p><div class="jy jz ka kb fq ab cb"><figure class="nb kc nc nd ne nf ng paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="nb kc nc nd ne nf ng paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="nb kc nc nd ne nf ng paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="ir is it"><p id="f922" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是AMI家庭的一员。我们现在<a class="ae jt" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae jt" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jt" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jt" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nh"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure></div></div>    
</body>
</html>