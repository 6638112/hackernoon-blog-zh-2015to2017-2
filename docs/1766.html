<html>
<head>
<title>Deep Learning Cheat Sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习备忘单</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deep-learning-cheat-sheet-25421411e460?source=collection_archive---------2-----------------------#2016-12-06">https://medium.com/hackernoon/deep-learning-cheat-sheet-25421411e460?source=collection_archive---------2-----------------------#2016-12-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/9d2f5c6b441ba93aafe16245fdd788e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcdHE40-TqZ3anN-YHk5uQ.png"/></div></div></figure><p id="6a1f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对一门新学科来说，深入的学习会让人不知所措。这里有一些帮助你度过难关的秘籍和技巧。</p><h2 id="a797" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">那些是什么？！？！</h2><p id="bf60" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">在本文中，我们将回顾在深度学习中发现的常见概念，以帮助开始这个令人惊叹的主题。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div class="fe ff lb"><img src="../Images/ab7f2b2cdf574b088463c445c27f1d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*WKE8FNkYjqJk_qfeuvio6A.png"/></div><figcaption class="lg lh fg fe ff li lj bd b be z ek">Visualization of gradient. The red arrows are the gradient of the blue plotted function.</figcaption></figure><h2 id="2e97" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">梯度∇</h2><p id="5be3" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">梯度是接受多个向量并输出单个值的函数的偏导数(即我们在神经网络中的成本函数)。梯度告诉我们，如果我们增加我们的可变输入，在图上哪个方向会增加我们的输出。我们使用梯度，并在相反的方向，因为我们想减少我们的损失。</p><h2 id="7cc1" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">反向传播</h2><p id="4359" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">也称为反向传播(back prop ),这是通过网络前向传播输入后，通过<a class="ae ka" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>的权重反向跟踪误差的过程。这是通过应用微积分中的链式法则来使用的。</p><h2 id="b21a" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">s形σ</h2><p id="8d3d" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">用于在[0，1]的区间内激活我们的网络中的权重的函数。这个函数看起来像一个S，这就是这个函数的名字，S在希腊语中是sigma。也称为逻辑功能</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lk"><img src="../Images/5b8ee9951d607a50d686525eb6ed2493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8WcbQSLFIxlCiN_YQSyJw.png"/></div></div><figcaption class="lg lh fg fe ff li lj bd b be z ek">Formula for ReLU from Geoffrey Hinton</figcaption></figure><h2 id="4f0f" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">校正线性单位或ReLU</h2><p id="adb8" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">sigmoid函数的区间为[0，1]，而ReLU的范围为[0，无穷大]。这意味着sigmoid更适合逻辑回归，而ReLU更适合表示正数。ReLU没有消失梯度问题。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ll"><img src="../Images/1581c9b71b9b3a67ceded625051c9094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYeGYddNRbrBJjkNxzw9FQ.png"/></div></div><figcaption class="lg lh fg fe ff li lj bd b be z ek">Tanh function</figcaption></figure><h2 id="e7c5" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">双曲正切</h2><p id="f69e" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">Tanh是一个函数，用于初始化[-1，1]网络的权重。假设你的数据是标准化的，我们将有更强的梯度:因为数据以0为中心，导数更高。要看到这一点，计算双曲正切函数的导数，注意输入值在范围[0，1]内。双曲正切函数的范围是[-1，1]，sigmoid函数的范围是[0，1]。这也避免了梯度中的偏差。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div class="fe ff lm"><img src="../Images/014e8a9494588c41110b6a4763780923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*K9g9EOeQ9Ca0jdOMmXKrQg.png"/></div></figure><h2 id="b761" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">LSTM/GRU</h2><p id="6833" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">通常在递归神经网络中发现，但正在扩展到在其他网络中使用。这些是小的“记忆单元”,用于保持训练输入之间的状态，并帮助解决消失梯度问题，其中在大约7个时间步长之后，RNN失去了先前输入的上下文。</p><h2 id="8eaf" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">Softmax</h2><p id="984b" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">Softmax是通常用在神经网络末端用于分类的函数。此函数进行多项式逻辑回归，通常用于多类分类。通常与交叉熵配对作为损失函数。</p><h2 id="0d39" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">L1和L2正规化</h2><p id="bb33" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">这些正则化方法通过对系数施加惩罚来防止过拟合。L1能产生稀疏模型，而L2不能。正则化用于指定模型复杂性。这一点很重要，因为它允许您的模型更好地概括，而不会过度适应训练数据。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ln"><img src="../Images/608e40943481d5642bf64b815a4b11f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkDC2Iwb9jSyRIWBUoDFtQ.png"/></div></div></figure><h2 id="e946" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">退出</h2><p id="65f9" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">[1]“它防止了过度拟合，并提供了一种有效地近似指数组合许多不同神经网络架构的方法”(Hinton)。这种方法随机选择可见和隐藏的单位从网络中删除。这通常是通过选择一个层百分比下降来确定的。</p><h2 id="c7f5" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">批量标准化</h2><p id="9911" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">[1]当网络有许多深层时，就出现了内部协变量转移的问题。这种转变是“由于训练期间网络参数的变化而导致的网络激活分布的变化。”(赛格迪)。如果我们能减少内部协变量的变化，我们就能训练得更快更好。批次归一化通过均值和方差将每个批次归一化到网络中来解决这个问题。</p><h2 id="f68e" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">目标函数</h2><p id="1ad3" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">又称<strong class="je hv">损失函数、成本函数</strong>或<strong class="je hv">优化得分函数</strong>。网络的目标是最小化损耗以最大化网络的准确性。</p><p id="5720" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">一个更技术性的解释[3]:</p><p id="2084" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">损失/成本/优化/目标函数是根据网络预测计算的函数。您的网络将输出一个预测值y_hat，我们将把它与y的期望输出值进行比较。然后，在反向传播中使用该函数，为我们提供梯度，从而优化我们的网络。这些函数的例子有f1/f得分、分类交叉熵、均方误差、平均绝对误差、铰链损耗等。</p><h2 id="bbad" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">F1/F分数</h2><p id="fddf" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">通过使用精确度和召回率来衡量模型的精确程度，公式如下:</p><pre class="lc ld le lf fq lo lp lq lr aw ls dt"><span id="73ed" class="kb kc hu lp b fv lt lu l lv lw">F1 = 2 * (Precision * Recall) / (Precision + Recall)</span></pre><p id="64d5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">精确的:在每一个预测中，哪些预测实际上是积极的？</p><pre class="lc ld le lf fq lo lp lq lr aw ls dt"><span id="b7fe" class="kb kc hu lp b fv lt lu l lv lw">Precision = True Positives / (True Positives + False Positives)</span></pre><p id="3827" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">回忆:在所有实际上有正面预测的人中，有多少是正面的？</p><pre class="lc ld le lf fq lo lp lq lr aw ls dt"><span id="d801" class="kb kc hu lp b fv lt lu l lv lw">Recall = True Positives / (True Positives + False Negatives)</span></pre><h2 id="1d3d" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">交叉熵</h2><p id="08ca" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">用来计算你的标签预测有多远。有时用CE表示。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/2f6f8864ef18490cc1fa21981a98828d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*9ZBskBY_piVwqC4GdZRl8g.png"/></div></figure><p id="fbc6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">交叉熵是一个损失函数，是与熵有关的热力学熵概念。这用于多类分类中，以发现预测中的错误。</p><p id="2da6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">学习速度【2】</strong></p><p id="40d0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">学习率是反向传播后优化过程中调整网络权重的幅度。学习率是一个超参数，对于各种问题都是不同的。这应该在上进行交叉验证。</p><pre class="lc ld le lf fq lo lp lq lr aw ls dt"><span id="8ede" class="kb kc hu lp b fv lt lu l lv lw"># ---- Vanilla Gradient Descent ----<br/># W is your matrix of weights<br/># dW is the gradient of W</span><span id="6237" class="kb kc hu lp b fv ly lu l lv lw"># Adjust the weights of our layer by multiplying our gradient times our learning_rate (Which is a scalar value)</span><span id="9fed" class="kb kc hu lp b fv ly lu l lv lw">W -= learning_rate * dW</span></pre><blockquote class="lz ma mb"><p id="61d8" class="jc jd mc je b jf jg jh ji jj jk jl jm md jo jp jq me js jt ju mf jw jx jy jz hn dt translated">这是一个活的文档，如果你认为有什么需要补充的，请告诉我，我会补充并感谢你。</p></blockquote><p id="21eb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">[1] *建议者:<a class="mg mh gr" href="https://medium.com/u/7cca9fed98b6?source=post_page-----25421411e460--------------------------------" rel="noopener" target="_blank"> InflationAaron </a></p><p id="9624" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">[2] *建议者:<a class="mg mh gr" href="https://medium.com/u/9b9998a144da?source=post_page-----25421411e460--------------------------------" rel="noopener" target="_blank">胡安·德迪奥斯·桑托斯</a></p><p id="96eb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">[3] *建议者:<a class="mg mh gr" href="https://medium.com/u/cc95bbf8c598?source=post_page-----25421411e460--------------------------------" rel="noopener" target="_blank">布兰登·弗莱斯利</a></p><h2 id="6198" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">要查看不同类型的模型和示例代码，请简单查看<a class="ae ka" href="https://hackernoon.com/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930#.ad4t6k2l9" rel="noopener ugc nofollow" target="_blank">tensor flow—第三部分:所有模型</a>。</h2><p id="3dba" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">一如既往地检查我的其他文章在<a class="ae ka" href="http://camron.xyz" rel="noopener ugc nofollow" target="_blank"> camron.xyz </a></p><div class="lc ld le lf fq ab cb"><figure class="mi iv mj mk ml mm mn paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mi iv mj mk ml mm mn paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mi iv mj mk ml mm mn paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="lz ma mb"><p id="f922" class="jc jd mc je b jf jg jh ji jj jk jl jm md jo jp jq me js jt ju mf jw jx jy jz hn dt translated"><a class="ae ka" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae ka" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae ka" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae ka" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jc jd mc je b jf jg jh ji jj jk jl jm md jo jp jq me js jt ju mf jw jx jy jz hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae ka" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae ka" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mo"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="lc ld le lf fq iv"><div class="bz el l di"><div class="mp mq l"/></div></figure></div></div>    
</body>
</html>