<html>
<head>
<title>Remastering Classic Films in Tensorflow with Pix2Pix</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Pix2Pix在Tensorflow中翻拍经典电影</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503?source=collection_archive---------3-----------------------#2017-01-12">https://medium.com/hackernoon/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503?source=collection_archive---------3-----------------------#2017-01-12</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/f070572d3c6bff9a1d8cc869d486d163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nM43-zZOGc1Uz_cKPebbIg.png"/></div></div></figure><p id="a059" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">新年快乐在2017年的第一篇帖子中，我想做一些有趣和有点不同的事情，暂时从RL转向生殖网络。在过去的几天里，我一直在与一个名为<a class="ae ka" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank"> Pix2Pix </a>的生成性对抗网络合作，并希望分享该项目的成果。这个框架来自最近发表的论文“<em class="kb">用条件对抗网络进行图像到图像的翻译</em>”。与接受噪声输入并生成图像的vanilla GANs不同，Pix2Pix学习接受一幅图像，并使用对抗框架将其转换为另一幅图像。这方面的例子包括将街道地图转换为航空摄影，将图纸转换为照片，将白天的照片转换为夜晚的照片。理论上，任何两个保持相同结构的意象之间的翻译都是可能的。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff kc"><img src="../Images/adcfcec7b5cd4189f5fa311c6f1ed89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ji9lcYTEjRl7mQCFZ13j4Q.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Examples of translated images using Pix2Pix.</figcaption></figure><p id="661c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我印象深刻的一个可能的和令人兴奋的用途是给黑白照片着色的能力，以及填补图像中缺失的空白。这两种能力加在一起，可以用来对20世纪50年代及更早的电影进行某种形式的重新灌制。以4:3纵横比拍摄的黑白电影尤其能从这一过程中受益。这种“重制版”既可以着色，也可以将宽高比扩展到更熟悉的16:9。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff kl"><img src="../Images/b92e5adc42c571dae670e9c82e039068.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*2S4YLLRBWggl0g685v3WTA.gif"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Top: input. Middle: My Pix2Pix remaster. Bottom: Original. Taken from <a class="ae ka" href="http://www.imdb.com/title/tt0047396/?ref_=nv_sr_1" rel="noopener ugc nofollow" target="_blank">Rear Window</a>.</figcaption></figure><p id="c330" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这篇文章中，我将介绍Pix2Pix的设计，提供一个<a class="ae ka" href="https://hackernoon.com/tagged/tensorflow" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>的实现，并展示一些电影重制过程的结果。如果你是GANs的新手，我推荐阅读我之前的两篇关于GANs的文章，以便更好地了解它们是如何工作的。Pix2Pix实际上只是一个GAN加上一些额外的架构变化。</p><div class="km kn fm fo ko kp"><a rel="noopener follow" target="_blank" href="/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39"><div class="kq ab ej"><div class="kr ab ks cl cj kt"><h2 class="bd hv fv z el ku eo ep kv er et ht dt translated">用经典的海绵宝宝插曲来解释生成性对抗网络</h2><div class="kw l"><h3 class="bd b fv z el ku eo ep kv er et ek translated">外加一个Tensorflow教程来实现您自己的GAN</h3></div><div class="kx l"><p class="bd b gc z el ku eo ep kv er et ek translated">medium.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld ja kp"/></div></div></a></div><h2 id="737a" class="le lf hu bd lg lh li lj lk ll lm ln lo jn lp lq lr jr ls lt lu jv lv lw lx ly dt translated">Pix2Pix</h2><p id="dc28" class="pw-post-body-paragraph jc jd hu je b jf lz jh ji jj ma jl jm jn mb jp jq jr mc jt ju jv md jx jy jz hn dt translated">Pix2Pix以非常直观的方式构建了GAN架构。在GAN公式中，我们有一个发生器<code class="eh me mf mg mh b"><em class="kb">G</em></code>和一个鉴别器<code class="eh me mf mg mh b"><em class="kb">D</em></code>，它们是敌对训练的。训练发生器从噪声输入<code class="eh me mf mg mh b">z</code>产生逼真的图像，训练鉴别器区分真实图像<code class="eh me mf mg mh b">x</code>和发生器<code class="eh me mf mg mh b">G(x)</code>产生的图像。使用来自鉴别器的反馈，生成器可以改进其过程，以生成将来更可能欺骗鉴别器的图像。这样做可以产生更逼真的图像。</p><p id="71f3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当从GAN到Pix2Pix时，最明显的变化是，代替噪声<code class="eh me mf mg mh b"><em class="kb">z</em></code>发生器被馈送一个实际的图像<code class="eh me mf mg mh b"><em class="kb">x</em></code>，我们想将其“翻译”成另一个结构相似的图像<code class="eh me mf mg mh b"><em class="kb">y</em></code>。我们的发电机现在生产<code class="eh me mf mg mh b"><em class="kb">G(x)</em></code>，我们希望与<code class="eh me mf mg mh b"><em class="kb">y</em></code>区别开来。</p><p id="fae6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">除了原始的GAN损失之外，我们还利用了<code class="eh me mf mg mh b">L1</code>损失，这只是生成图像上像素绝对值的损失。在这种情况下，我们强迫发电机近似为<code class="eh me mf mg mh b"><em class="kb">G(x) = y</em></code>附加损耗:</p><p id="db90" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh me mf mg mh b">L1 = |G(x) - y|</code></p><p id="53d5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在传统的氮化镓中，我们永远不会利用这种损耗，因为它会阻止发生器产生新的图像。然而，在图像翻译的情况下，我们关心的是准确的图像翻译，而不是新颖的图像翻译。这种对精确图像的渴望也是我们不完全取消网络中GAN部分的原因。<code class="eh me mf mg mh b">L1</code>丢失本身会由于试图生成“平均”正确的图像而产生模糊或褪色的图像。通过保持GAN的损耗，我们鼓励网络产生清晰的图像，在视觉上与真实情况难以区分。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mi"><img src="../Images/02e6d8287b37c90105312f626a5f6911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYqFl0zS15KRAWiGZnLf3g.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Left: image translation with only L1 loss. Right: Image translation with L1 and GAN losses.</figcaption></figure><p id="5d8a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">沿着确保精确图像的路线，对Pix2Pix的第三个改变是在生成器中使用了U-Net架构。简单地说，U-Net是一个自动编码器，其中一半网络中编码器的输出与另一半网络中解码器的镜像输出相连。通过包含这些跳过连接，我们可以防止网络中间成为信息流的瓶颈。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mj"><img src="../Images/dabefd145854c3f8f3acac34f536b894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6RTDQFBoehmTeQg0QXoFg.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">U — Net Architecture</figcaption></figure><p id="b865" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Pix2Pix的情况下，我们的输入是我们想要翻译的图像<code class="eh me mf mg mh b">x</code>，输出是我们想要它变成的图像<code class="eh me mf mg mh b">G(x)</code>。通过连接镜像层，我们能够确保原始图像的结构直接传送到网络的解码器。当考虑彩色化的任务时，在提供彩色化图像的结构方面，在编码器的每个尺度上学习的表示对于解码器是非常有用的。</p><p id="e053" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Pix2Pix论文中，作者还讨论了利用称为PatchGAN的不同鉴别器架构。PatchGAN背后的想法是，不是为整个图像产生单个鉴别器分数，而是为图像的每个补丁产生一组单独的分数，然后将这些分数平均在一起产生最终分数。这种方法可以通过依赖更少的参数来提高性能，并且正确调整的面片大小可以提高图像质量。因此，补丁大小与图像大小相同的补丁根相当于传统的鉴别器架构。为了简单起见，我的Pix2Pix实现使用了传统的鉴别器，或者对图像进行修补。</p><h2 id="c59e" class="le lf hu bd lg lh li lj lk ll lm ln lo jn lp lq lr jr ls lt lu jv lv lw lx ly dt translated">重新灌制电影</h2><p id="6402" class="pw-post-body-paragraph jc jd hu je b jf lz jh ji jj ma jl jm jn mb jp jq jr mc jt ju jv md jx jy jz hn dt translated">鉴于电影的悠久历史，媒体中的许多经典作品都是在半个多世纪前创作的，因此与今天的电影相比，它们是用非常不同的技术制作的。《卡萨布兰卡》、《公民凯恩》、《大都会》、《美好生活》等经典影片都是在有限的宽高比下以黑白方式拍摄的。尽管有视觉上的限制，这些电影仍然唤起了复杂和完全实现的世界。看着它们，很容易闭上眼睛，想象它们在全彩色和更广阔的视野下可能是什么样子。如果有足够的时间，艺术家甚至可以着色和扩展画面，就像有时对经典电影所做的那样。凭借Pix2Pix框架的力量，我想知道是否有可能让神经网络自动学习做这件事。</p><p id="d649" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了实现这一点，我将我的GAN实现转换为如上所述的Pix2Pix架构，并开始在一组经典电影上训练它。设置很简单:我决定利用阿尔弗雷德·希区柯克电影集(<em class="kb">《迷魂记》</em>、<em class="kb">《后窗》</em>、<em class="kb">《西北偏北》</em>)作为训练集。我选择这些电影是因为它们是彩色的、宽屏的，但在当时，许多电影还是黑白的。因此，很多服装、灯光和建筑都与我想要翻拍的更老的电影相似。我选择它们也是因为它们是那个时代我的最爱。</p><p id="79fd" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">每个视频的大小都调整为256x144，图像帧以2fps的速度提取。从那时起，每一帧都被转换为灰度进行预处理，并被裁剪为4:3的宽高比。这组帧组成了训练输入<code class="eh me mf mg mh b">x</code>。原始帧被用作期望输出<code class="eh me mf mg mh b">y</code>。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff kl"><img src="../Images/7dcf0eed6c6f94f6b024e041005c2517.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*YCGOolQbd8w7nKCHji57Vw.gif"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Top: input x. Middle: output G(x). Bottom: original y. Taken from <a class="ae ka" href="http://www.imdb.com/title/tt0053125/?ref_=fn_al_tt_1" rel="noopener ugc nofollow" target="_blank">North by Northwest</a>.</figcaption></figure><p id="77a6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">经过一天的训练，有骄人的成绩！在训练影片本身上，网络能够相当精确地再现原始图像(如上所示)。</p><p id="b05e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当然，在ML领域，这基本上是欺骗。需要一个单独的测试集来获得真正的性能。下面是利用网络重现的《T2》中的一个例子《T3》(希区柯克的另一部经典)。至关重要的是，该网络从未使用这些帧或电影中的任何其他帧进行训练。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mk"><img src="../Images/2103e09dffcda7948347bed9ac7b70a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*bqRQne9VwKDqx7XEBMJUBg.gif"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Left: input x. Middle: output G(x). Right: original y. Taken from <a class="ae ka" href="http://www.imdb.com/title/tt0056869/?ref_=nv_sr_1" rel="noopener ugc nofollow" target="_blank">The Birds</a>.</figcaption></figure><p id="e56d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">至少在“同一个导演的电影”的转让领域，该网络似乎表现得惊人地好。另一个更长的重制序列来自<em class="kb">的《鸟》</em>:</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="ml mm l"/></div></figure><p id="795d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然而，这样做的主要原因是考虑到没有彩色或宽屏版本的老电影的重新制作。下面是一些例子，比如《让·达可的激情》、<em class="kb">大都会</em>和<em class="kb">美好生活</em>都是用经过训练的Pix2Pix网络“重新录制”的:</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/4502745cd87cdcd50828070daf8315a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*FoBI1XRcED7IICNTzAVFTQ.gif"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Left: Original. Right: Remastered. Taken from <a class="ae ka" href="http://The Passion of Joan of Arc" rel="noopener ugc nofollow" target="_blank">The Passion of Joan of Arc</a>.</figcaption></figure><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/993ede766ca5ae4ae41b0a26e95549df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*-8SyFsTiI7WjEsrmkRpcsg.gif"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Left: Original. Right: Remastered. Taken from <a class="ae ka" href="http://www.imdb.com/title/tt0017136/?ref_=nv_sr_1" rel="noopener ugc nofollow" target="_blank">Metropolis</a>.</figcaption></figure><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/16c8a93320798717acdfaf13ef77ada5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*NeCXyZxuz6qvPE2QWkjhjQ.gif"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Left: Original. Right: Remastered. Taken from <a class="ae ka" href="http://www.imdb.com/title/tt0038650/?ref_=nv_sr_1" rel="noopener ugc nofollow" target="_blank">It’s a Wonderful Life</a>.</figcaption></figure><p id="46a1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">结果并不完美，但我对它们相当满意，因为训练时间相对较短，训练集较小，并且对基线框架的更改很小。随着训练时间的延长和所用影片的增多，结果无疑会有所改善。该体系结构还增加了许多可以提高性能的功能。最大的缺点之一是网络没有利用视频的时间结构。一帧中屏幕外的对象可能出现在另一帧中。这样，可以利用RNN或滑动窗口来捕获帧之间的信息。我把这项工作留给任何感兴趣的人去做。</p><p id="3553" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这个<a class="ae ka" href="https://github.com/awjuliani/Pix2Pix-Film" rel="noopener ugc nofollow" target="_blank"> Github库</a>包含了你想要自己训练和运行Pix2Pix所需要的一切。此外，我已经上传了一个预先训练好的模型<a class="ae ka" href="https://drive.google.com/open?id=0B8x0IeJAaBccNFVQMkQ0QW15TjQ" rel="noopener ugc nofollow" target="_blank">在这里</a>，如果你只是想尝试重新录制你自己最喜欢的老电影，而不是从头开始重新训练一个模型。存储库中提供了启动和运行的说明。</p><p id="4ae8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这种工作仍处于早期阶段，但我相信，不需要多少年，我们就能坐下来观看《大都会》或无数其他经典电影，拥有我们能想象到的所有视觉细节。向旧媒体注入活力的能力是深度学习令人兴奋的潜力，这种潜力只会随着技术的成熟而增长。</p></div><div class="ab cl mo mp hc mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hn ho hp hq hr"><p id="ba20" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果你想关注我关于深度学习、人工智能和认知科学的文章，请在Medium @ <a class="mv mw gr" href="https://medium.com/u/18dfe63fa7f0?source=post_page-----f4d551fa0503--------------------------------" rel="noopener" target="_blank"> Arthur Juliani </a>或twitter <a class="ae ka" href="https://twitter.com/awjuliani" rel="noopener ugc nofollow" target="_blank"> @awjliani </a>上关注我。</p><p id="15a2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">请考虑捐赠 来帮助支持未来的教程、文章和实现。任何贡献都非常感谢！</p><div class="kd ke kf kg fq ab cb"><figure class="mx iv my mz na nb nc paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mx iv my mz na nb nc paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mx iv my mz na nb nc paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="nd ne nf"><p id="f922" class="jc jd kb je b jf jg jh ji jj jk jl jm ng jo jp jq nh js jt ju ni jw jx jy jz hn dt translated"><a class="ae ka" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是阿妹家庭的一员。我们现在<a class="ae ka" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae ka" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jc jd kb je b jf jg jh ji jj jk jl jm ng jo jp jq nh js jt ju ni jw jx jy jz hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae ka" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae ka" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nj"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="nk mm l"/></div></figure></div></div>    
</body>
</html>