<html>
<head>
<title>When Smaller’s Better</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">越小越好</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/when-smallers-better-4b54cedc3402?source=collection_archive---------7-----------------------#2017-01-02">https://medium.com/hackernoon/when-smallers-better-4b54cedc3402?source=collection_archive---------7-----------------------#2017-01-02</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="925e" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">张量流图形文件中神经网络的有损压缩</h2></div><p id="8fbe" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">(此贴激励并描述了来自</em> <a class="ae kg" href="https://arxiv.org/abs/1510.00149" rel="noopener ugc nofollow" target="_blank"> <em class="kf">韩、毛和戴利的关于神经网络压缩的论文</em> </a> <em class="kf">的发现在TensorFlow图形文件上的应用。感谢迈克尔·舒米欣</em><em class="kf"/><a class="ae kg" href="http://www.anishathalye.com/" rel="noopener ugc nofollow" target="_blank"><em class="kf">安尼施·阿萨莱</em> </a> <em class="kf">和</em> <a class="ae kg" href="https://github.com/iahs" rel="noopener ugc nofollow" target="_blank"> <em class="kf">沙伊·苏兰斯基</em></a><em class="kf">——他们帮助激励和推进了这项工作。)</em></p><p id="c1e5" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jl hv">TL；神经网络很大。有时当它们更小的时候是有用的(这样你可以存储更多，或者用更少的带宽部署它们)。<a class="ae kg" href="https://arxiv.org/abs/1510.00149" rel="noopener ugc nofollow" target="_blank">本文</a>阐述了对神经网络进行理论压缩的方法。您可以使用现成的压缩算法和<a class="ae kg" href="https://github.com/tomasreimers/tensorflow-graph-compression" rel="noopener ugc nofollow" target="_blank">我们的库</a>来实现TF图表文件大小的3倍缩减，而不必重新训练图表，并且没有(注意到)显著的准确性损失。</strong></p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><p id="28e8" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">人工神经网络在过去几年里越来越受欢迎…</p><p id="3331" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">…是一种保守的说法。机器<a class="ae kg" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>可能是<em class="kf">目前计算机科学中最热门的领域</em>，神经网络被用于几乎所有的“机器学习”:情感分析、决策、图像分类等。神经网络的工作原理是接受一组“训练”数据(输入和输出对)，并根据输入学习预测输出；一旦经过训练，神经网络可以预测“测试”集的输出(以前没有看到的数据)。输入和输出对的一个例子可能是图像的像素，输出可以是值0或1，指示该图像是否包含狗。另一个例子可以是来自图像的像素输入，并且输出是三维向量，其中第一维度是0或1以指示狗的存在，第二维度指示猫的存在，并且第三维度指示鸟的存在。总的来说，神经网络已经被证明是不可思议的强大，目前已经在许多顶级公司的生产中部署，包括<a class="ae kg" href="https://hackernoon.com/tagged/google" rel="noopener ugc nofollow" target="_blank">谷歌</a>/脸书/亚马逊等。</p><figure class="kp kq kr ks fq kt fe ff paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="fe ff ko"><img src="../Images/377d93feaa1e33ac62107ba46f2d3f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dOLD0OnEG55x0p7g."/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek"><em class="le">Canonical image of a neural net. Figure from </em><a class="ae kg" href="http://cs231n.github.io/neural-networks-1/" rel="noopener ugc nofollow" target="_blank"><em class="le">http://cs231n.github.io/neural-networks-1/</em></a></figcaption></figure><p id="7e68" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">神经网络的本质是一系列加权线性回归，结合回归输出的变换(这些变换范围从恒等函数到tanh，再到<a class="ae kg" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLUs </a>)。神经网络可以通过一种称为反向传播的算法来“学习”这些权重，这种算法依赖于梯度下降来不断调整权重，以提高训练数据集的准确性。</p><p id="f1fc" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">注意:这是一个过于简化的问题，CNN</em><em class="kf">和</em><a class="ae kg" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"><em class="kf">RNNs</em></a><em class="kf">可能会使这个问题变得复杂——但是它可以用于演示目的。如果你有兴趣了解更多，我强烈推荐阅读斯坦福大学CS231n </em>  <em class="kf">的介绍。</em></p><p id="2510" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">回想一下八年级的代数，线性回归接受一个值，通过一个系数和一个偏差产生一个值。这是经典的<code class="eh lf lg lh li b">y=mx+b</code>方程。其中<code class="eh lf lg lh li b">m</code>是权重，<code class="eh lf lg lh li b">b</code>是偏差。在多维空间中，这也是一样的，唯一的区别是<code class="eh lf lg lh li b">x</code>和<code class="eh lf lg lh li b">m</code>是向量。</p><p id="6a14" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在一个神经网络中，人们正在进行许多许多高维回归——权重和偏差的值会占用大量内存。以Google开源的热门机器学习库<a class="ae kg" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>为例:一个经过训练的深度mnist图(对<a class="ae kg" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">手写数字</a>进行分类，并作为TensorFlow 的<a class="ae kg" href="https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/" rel="noopener ugc nofollow" target="_blank">入门演示)占用<strong class="jl hv"> 12MiB </strong>。</a><a class="ae kg" href="https://arxiv.org/abs/1512.00567" rel="noopener ugc nofollow" target="_blank"> Google Inception V3图</a>(用于<a class="ae kg" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image" rel="noopener ugc nofollow" target="_blank">图片分类</a>)占用<strong class="jl hv">91mb</strong>(此处找到<a class="ae kg" href="https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip" rel="noopener ugc nofollow" target="_blank">，警告:下载量大)。当我们继续在这些网络上迭代并存储多个版本时，大小就成问题了。如果有一个更小的表示来存储多个版本，通过网络发送到服务器(或自动驾驶汽车……)上，并存储在移动设备上，那就太好了。</a></p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><h2 id="d221" class="lj lk hu bd ll lm ln lo lp lq lr ls lt js lu lv lw jw lx ly lz ka ma mb mc md dt translated">张量流图速成班</h2><p id="ffa5" class="pw-post-body-paragraph jj jk hu jl b jm me iv jo jp mf iy jr js mg ju jv jw mh jy jz ka mi kc kd ke hn dt translated">图中张量流结构的计算。这些图形存储操作(ops)以及操作的输入和先前操作的输出之间的关系。</p><p id="8aca" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这些关系存储为“张量”，基本上是n维数组，其值不一定已知，其形状可能部分已知。您可以运行一个张量流图，方法是向图中输入一些张量的值，并通过运算运行它们，直到达到所需的输出。例如，如果我想在TensorFlow中对方程<code class="eh lf lg lh li b">y=mx+b</code>建模，我可以用下面的Python来实现:</p><pre class="kp kq kr ks fq mj li mk ml aw mm dt"><span id="cb48" class="lj lk hu li b fv mn mo l mp mq">import tensorflow as tf</span><span id="e40f" class="lj lk hu li b fv mr mo l mp mq">y = tf.add(<br/>    tf.mul(<br/>        tf.Variable(2, name="m"),<br/>        tf.placeholder(tf.int32, shape=1, name="x")<br/>    ),<br/>    tf.Variable(1, name="b")<br/>)</span></pre><figure class="kp kq kr ks fq kt fe ff paragraph-image"><div class="fe ff ms"><img src="../Images/dfbe9675dc959e6a057ee85b2ac6c106.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*vK97pFAU4H3KSGFWQPvIhw.png"/></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">The example TensorFlow graph for y=mx+b.</figcaption></figure><p id="984c" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如你所见，ops是:“Add”、“Mul”、“m”、“b”和“x”。张量由<code class="eh lf lg lh li b">OPERATION:OUTPUT_INDEX</code>命名，所以例如<code class="eh lf lg lh li b">Mul</code>有一个输出(<code class="eh lf lg lh li b">m*x</code>的结果)，它将被命名为<code class="eh lf lg lh li b">Mul:0</code>。变量和占位符(占位符是其值必须在任何图形运行中输入的变量)也是具有一个输出张量(它们自己的值)的操作。在这个例子中，<code class="eh lf lg lh li b">m:0</code>、<code class="eh lf lg lh li b">x:0</code>、<code class="eh lf lg lh li b">b:0</code>的值是相当明显的，<code class="eh lf lg lh li b">Mul</code>操作接受<code class="eh lf lg lh li b">m:0</code>和<code class="eh lf lg lh li b">x:0</code>作为输入来产生<code class="eh lf lg lh li b">Mul:0</code>，而<code class="eh lf lg lh li b">Add</code>接受<code class="eh lf lg lh li b">Mul:0</code>和<code class="eh lf lg lh li b">b:0</code>来产生<code class="eh lf lg lh li b">Add:0</code>。你可能会注意到在这个图中没有<code class="eh lf lg lh li b">y</code>，这是因为<code class="eh lf lg lh li b">y</code>实际上是<code class="eh lf lg lh li b">Add:0</code>，尽管如果我们真的想要的话，你也可以创建一个<code class="eh lf lg lh li b">y</code>变量并把它赋给<code class="eh lf lg lh li b">Add:0</code>(尽管这看起来有些多余)。要运行这个图，可以使用类似于<code class="eh lf lg lh li b">sess.run(["Add:0"], feed_dict={"x:0": (5,)})</code>的命令，为<code class="eh lf lg lh li b">x:0</code>输入一个值并请求<code class="eh lf lg lh li b">Add:0</code>。</p><p id="b3b6" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这些图形被编码并存储在一个<a class="ae kg" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto" rel="noopener ugc nofollow" target="_blank"> GraphDef </a> protobuf中。变量值不存储在图形定义中(因为它们可以在多次执行中赋值)，因此不是图形的一部分；但是，变量可以保存在检查点文件中(基本上是一个包含变量名和值的字典……至少根据Google的说法，文件类型比GraphDefs不透明得多)。当我们拟合回归(或训练神经网络)时，变量是有用的，但是一旦我们知道我们将不再改变它们，将它们保持为变量就没有多大意义了。相反，我们可以将它们转换为常量值(使用<a class="ae kg" href="https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/framework/graph_util_impl.py#L178" rel="noopener ugc nofollow" target="_blank">convert _ variables _ to _ constants</a>函数或与TensorFlow一起发布的<a class="ae kg" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py" rel="noopener ugc nofollow" target="_blank"> freeze_graph </a>工具)并简化我们的图形。冻结我们的toy <code class="eh lf lg lh li b">mx+b</code>模型后的示例GraphDef包含一组节点和(可能)它们所包含的张量的值:</p><pre class="kp kq kr ks fq mj li mk ml aw mm dt"><span id="8b6e" class="lj lk hu li b fv mn mo l mp mq">node {<br/>  name: "m"<br/>  op: "Const"<br/>  attr {<br/>    key: "dtype"<br/>    value {<br/>      type: DT_INT32<br/>    }<br/>  }<br/>  attr {<br/>    key: "value"<br/>    value {<br/>      tensor {<br/>        dtype: DT_INT32<br/>        tensor_shape {<br/>        }<br/>        int_val: 2<br/>      }<br/>    }<br/>  }<br/>}</span><span id="8fbc" class="lj lk hu li b fv mr mo l mp mq">...</span><span id="dd69" class="lj lk hu li b fv mr mo l mp mq">node {<br/>  name: "Add"<br/>  op: "Add"<br/>  input: "Mul"<br/>  input: "b/read"<br/>  attr {<br/>    key: "T"<br/>    value {<br/>      type: DT_INT32<br/>    }<br/>  }<br/>}</span></pre><p id="cedf" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">注意:具有多个标量值的张量作为</em> <a class="ae kg" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto#L36" rel="noopener ugc nofollow" target="_blank"> <em class="kf">压缩字节存储在字符串</em> </a> <em class="kf">中。它们可以与带有</em><a class="ae kg" href="https://www.tensorflow.org/api_docs/python/contrib.util/" rel="noopener ugc nofollow" target="_blank"><em class="kf">TF . contrib . util . make _ ndarray和的数组相互转换。make_tensor_proto函数</em> </a> <em class="kf">。</em></p><p id="a569" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">要了解更多关于TensorFlow模型文件的内容，<a class="ae kg" href="https://www.tensorflow.org/versions/master/how_tos/tool_developers/" rel="noopener ugc nofollow" target="_blank">请点击这里</a>。</p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><h2 id="1398" class="lj lk hu bd ll lm ln lo lp lq lr ls lt js lu lv lw jw lx ly lz ka ma mb mc md dt translated">压缩</h2><p id="92e2" class="pw-post-body-paragraph jj jk hu jl b jm me iv jo jp mf iy jr js mg ju jv jw mh jy jz ka mi kc kd ke hn dt translated"><a class="ae kg" href="https://github.com/tomasreimers/tensorflow-graph-compression" rel="noopener ugc nofollow" target="_blank">https://github . com/to MAS reimers/tensor flow-graph-compression</a></p><p id="5693" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">想要压缩TensorFlow图形文件，我们做的第一件事就是尝试gzip，一个流行的压缩工具。Gzip并不是非常有效。它并没有显著减小保存的深度mnist图的文件大小(压缩前后舍入到<strong class="jl hv">12 MIB</strong><strong class="jl hv"/>)并将初始图的文件大小从<strong class="jl hv"> 91MiB </strong>减小到<strong class="jl hv"> 85MiB </strong>(即使使用了<code class="eh lf lg lh li b">gzip -9</code>压缩)。面对其他无损压缩算法的类似结果，我决定考虑有损神经网络压缩。</p><p id="48f2" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">张量流图形文件很大程度上是权重。从GraphDef文件中剥离包含各种权重的张量，深度mnist图只有<strong class="jl hv"> 3.7KiB </strong>，初始图只有<strong class="jl hv"> 120KiB </strong>。知道了这一点，我开始寻找有损压缩权重的方法。</p><p id="04cc" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">神经网络重量不精确已经开始成为一个研究领域，原因有很多:硬件制造商对它感兴趣，因为它可能允许他们制造更多<a class="ae kg" href="https://www.technologyreview.com/s/601263/why-a-chip-thats-bad-at-math-can-help-computers-tackle-harder-problems/" rel="noopener ugc nofollow" target="_blank">不精确的硬件，可以更快地运行神经网络</a>(NVidia的TensorRT对INT8和Float16模式提供硬件级支持，可以更快地运行网络)，软件工程师对它感兴趣，因为它可能允许他们<a class="ae kg" href="https://arxiv.org/abs/1602.07360" rel="noopener ugc nofollow" target="_blank">减少神经网络</a>占用的空间，甚至可能<a class="ae kg" href="http://www.jmlr.org/proceedings/papers/v37/gupta15.pdf" rel="noopener ugc nofollow" target="_blank">软件级加速或</a></p><p id="eb8c" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在权重压缩领域的开创性研究是由<a class="ae kg" href="https://arxiv.org/abs/1510.00149" rel="noopener ugc nofollow" target="_blank">韩、毛和戴利进行的，研究表明神经网络可以通过剪枝、量化和霍夫曼编码来减小规模</a>。这项研究假设人们仍然可以访问神经网络的训练集；然而，出于我们的目的，我们将假设我们无法访问该数据集(例如，如果我们有一个来自模型动物园的预训练网络)。这意味着我们不能做剪枝(这是建立在<a class="ae kg" href="https://arxiv.org/abs/1506.02626" rel="noopener ugc nofollow" target="_blank">韩之前的工作</a>之上的)，因为那需要能够训练网络；然而我们仍然可以做量子化。</p><p id="ac63" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">量化背后的见解是，神经网络中的权重从根本上说是不精确的，这是由反向传播算法的本质决定的，这意味着它们不一定需要我们为它们分配的所有精度。目前，权重(假设它们是标准的32b浮点数)可以取2个不同值中的任何一个，这意味着它们的表示必须至少有32位。然而，如果我们只让权重采用2⁸值(8是任意的，我们可以在这里选择任何整数&lt; 32), we would only need 8 bits to represent them (you could imagine replacing each weight with an integer 0–255, and having a codebook of negligible size to translate the integer to a floating point value for that weight).</p><figure class="kp kq kr ks fq kt fe ff paragraph-image"><div class="fe ff mt"><img src="../Images/d007567b5eeebe22bb6730fe18ef4fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*GaEsC2D-U-udWunsaKGA4g.png"/></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">The dynamic programming solution to find optimal clustering in 1D.</figcaption></figure><p id="20ab" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">So now the problem becomes, how can we select 2⁸ values to appropriately represent all of our weights? Well this becomes a clustering problem, and Hans et al. did this using K-Means clustering (which finds a local optima) on the collection of scalar values from all of the weights, and replacing each scalar with its cluster centroid. We implemented this algorithm, and realized we could actually improve it (shout out to Shai), because the collection of the scalars in all the weights exists in one-dimensional space, and we can use dynamic programming to find the global optima for clustering in 1D in O(kN²) time and O(kN) space, where k is the number of clusters and N is the number of points being clustered. We implemented optimal clustering <a class="ae kg" href="https://github.com/tomasreimers/tensorflow-graph-compression/blob/master/optimal_cluster.py" rel="noopener ugc nofollow" target="_blank"/>，尽管如果我们希望它足够快以用于实际网络，它可能需要在C++中重新实现。</p><p id="3d42" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">注意:我们一直在最小化每个标量和我们替换它的值之间的距离，使用预定数量的聚类；然而，如果你对你的网络有更多的了解，并想使用另一个损失函数，你应该看看这篇文章</em><a class="ae kg" href="https://arxiv.org/pdf/math/0309285.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kf"/></a><em class="kf">，它提出了一个O(N)的聚类，而没有预先确定的聚类数。</em></p><p id="de59" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">用聚类的质心替换了所有权重标量后，我们应该有完全相同的文件大小，因为聚类质心仍然具有与它们替换的值相同的位数。为了减小尺寸，我们需要实现一个码本，并用索引到码本中的较低位表示代替每个权重。如果我们将32位权重替换为码本中的8位索引，这将实现<strong class="jl hv"> 4x的缩减</strong>。此外，韩等人指出，我们可以使用<a class="ae kg" href="https://en.wikipedia.org/wiki/Huffman_coding" rel="noopener ugc nofollow" target="_blank">霍夫曼编码</a>来实现更大的压缩比(假设权重不是均匀分布的)。</p><div class="kp kq kr ks fq ab cb"><figure class="mu kt mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/ca7c24478e9e8aebe4bd55cc3dde27d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*oGeD92fOEJS7vDY4mXVH_w.png"/></div></figure><figure class="mu kt na mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/91385092e37e03fd5b9778f1bde464b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*iUXJpmGMaopvxRH8aGiEvw.png"/></div><figcaption class="la lb fg fe ff lc ld bd b be z ek nb di nc nd">Evidence that weights are not evenly distributed in nets, and that Huffman encoding may be useful. The images above depict 98% of the data, I’ve removed the 1st and 99th percentile as they are outliers and extend the range by so much it renders the graphs difficult to read.</figcaption></figure></div><p id="fa2d" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">然而，用Huffman编码实现一个码本是痛苦的(或者至少是耗时的),我们没有理由必须自己去做(大声告诉Michael指出这一点):通过用重心替换权重，我们增加了文件中的冗余；这意味着我们可以使用现成的压缩算法，他们将为我们创建一个霍夫曼编码的码本(但可能比我们手动创建码本的压缩比更低)。</p><p id="a522" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">对新文件使用GZip，我们将mnist从<strong class="jl hv">12mb压缩到5mb</strong>(使用全局码本)，将inception从<strong class="jl hv">91mb压缩到37mb</strong>(使用每层码本)。我们还发现<a class="ae kg" href="http://facebook.github.io/zstd/" rel="noopener ugc nofollow" target="_blank">脸书的zstd </a>异常，并进一步将mnist减少到<strong class="jl hv"> 4MiB </strong>和inception减少到<strong class="jl hv">34 MIB；</strong>虽然这些比我们理论上应该看到的4倍要少，但它仍然比我们最初的压缩比要高，并且我们不依赖于自定义编码。</p><p id="8d76" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">具有新权重的网络没有显示出显著的误差增加。使用砝码替换的Mnist获得了<strong class="jl hv"> 0.9644 </strong>的精度(与砝码替换前的<strong class="jl hv"> 0.9642 </strong>相比……精度的提高很可能是随机的)。进一步研究的一个有趣领域是尝试用更少的比特对权重进行编码(想象一个2⁶条目码本，其中每个权重可以用6比特进行编码),并绘制大小/精度权衡图。</p><p id="8c44" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">我已经开源了权重压缩脚本，这里</em>  <em class="kf">有</em> <a class="ae kg" href="https://github.com/tomasreimers/tensorflow-graph-compression" rel="noopener ugc nofollow" target="_blank"> <em class="kf">。欢迎改进和拉动请求:)</em></a></p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><h2 id="3a12" class="lj lk hu bd ll lm ln lo lp lq lr ls lt js lu lv lw jw lx ly lz ka ma mb mc md dt translated">结论</h2><p id="1c61" class="pw-post-body-paragraph jj jk hu jl b jm me iv jo jp mf iy jr js mg ju jv jw mh jy jz ka mi kc kd ke hn dt translated">神经网络非常强大，被部署在越来越多的地方。目前，神经网络的表示是空间密集型的，随着我们开始将神经网络导出到越来越多的平台，这可能会成为一个问题(无论是移动平台还是web平台，想想像<a class="ae kg" href="https://transcranial.github.io/keras-js/" rel="noopener ugc nofollow" target="_blank"> Keras.js </a>这样的项目，它令人难以置信地兴奋，推动了可能性的前沿，但其<a class="ae kg" href="https://transcranial.github.io/keras-js/" rel="noopener ugc nofollow" target="_blank">演示</a>要求用户下载约100兆字节的权重，以在浏览器中运行inception)。我们证明了我们可以对权重进行聚类编码，以减少文件大小。此外，我们表明不需要实现定制的解码器，因为通过简单地用一些类似的值替换权重，我们可以增加网络表示中的冗余，从而增加我们可以用现成的压缩算法预期的压缩率(<em class="kf">添加win: gzip在大多数服务器和浏览器中默认实现；)</em>)；所有这些都没有太大的准确性损失。</p><p id="3b84" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">附:在整个项目中，Apple Finder对</em><a class="ae kg" href="https://en.wikipedia.org/wiki/Megabyte" rel="noopener ugc nofollow" target="_blank"><em class="kf">base-10 MB</em></a><em class="kf">vs Terminal对</em><a class="ae kg" href="https://en.wikipedia.org/wiki/Mebibyte" rel="noopener ugc nofollow" target="_blank"><em class="kf">base-2 MiB</em></a><em class="kf">的理解有时可能会令人困惑-_- </em> <a class="ae kg" href="https://xkcd.com/394/" rel="noopener ugc nofollow" target="_blank"> <em class="kf">相关XKCD </em> </a> <em class="kf">。</em></p><p id="cbc1" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jl hv">更新:</strong>你现在也可以使用TensorFlow的<a class="ae kg" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_weights" rel="noopener ugc nofollow" target="_blank"> quantize_weights图形变换</a>做类似的事情。</p><figure class="kp kq kr ks fq kt"><div class="bz el l di"><div class="ne nf l"/></div></figure></div></div>    
</body>
</html>