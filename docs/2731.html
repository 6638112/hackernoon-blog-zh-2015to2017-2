<html>
<head>
<title>GPUs &amp; Kubernetes for Deep Learning — Part 1/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的GPU和Kubernetes第1/3部分</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/gpus-kubernetes-for-deep-learning-part-1-3-d8eebe0dd6fe?source=collection_archive---------2-----------------------#2017-02-15">https://medium.com/hackernoon/gpus-kubernetes-for-deep-learning-part-1-3-d8eebe0dd6fe?source=collection_archive---------2-----------------------#2017-02-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="d3e2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">几周前，我分享了一个关于建立一个<a class="ae jp" href="https://hackernoon.com/installing-a-diy-bare-metal-gpu-cluster-for-kubernetes-364200254187" rel="noopener ugc nofollow" target="_blank">DYI GPU集群的附带项目，让k8s </a>与<a class="ae jp" href="https://hackernoon.com/tagged/kubernetes" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>一起玩，相对于AWS g2实例有一个适当的ROI。</p><p id="6db3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当AWS落后于旧的nVidia K20s卡(最新的驱动程序不再支持这些卡)时，这非常有趣。但是随着<a class="ae jp" href="https://aws.amazon.com/ec2/instance-types/#p2" rel="noopener ugc nofollow" target="_blank"> P系列</a> (p2.xlarge，8xlarge和16xlarge)的加入，新卡是k80，具有12GB RAM，比以前的更强大。</p><p id="c5e9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">百度刚刚在Kubernetes博客上发布了一篇关于PaddlePaddle设置的<a class="ae jp" href="http://blog.kubernetes.io/2017/02/run-deep-learning-with-paddlepaddle-on-kubernetes.html" rel="noopener ugc nofollow" target="_blank">帖子，但他们只关注了CPU。我认为在AWS上观察Kubernetes添加一些</a><a class="ae jp" href="https://hackernoon.com/tagged/gpu" rel="noopener ugc nofollow" target="_blank"> GPU </a>节点的设置会很有趣，然后在其上练习深度学习框架。医生说有可能…</p><p id="e8ed" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这篇文章是3个系列的第一篇:设置GPU集群(这篇博客)，向Kubernetes集群添加存储(紧接着)，最后在集群上运行深度学习培训(正在进行，在MWC之后……)。</p><h1 id="8a66" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">这个计划</h1><p id="b81d" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在这篇博客中，我们将:</p><ol class=""><li id="97f7" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">以开发模式在AWS上部署k8s(无HA、共置etcd、控制平面和PKI)</li><li id="dd41" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">使用GPU部署2个节点(p2.xlarge和p2.8xlarge实例)</li><li id="dcb9" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">仅使用CPU部署3个节点(m4.xlarge)</li><li id="8af9" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">验证GPU可用性</li></ol><h1 id="85af" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">要求</h1><p id="7935" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">接下来，重要的是:</p><ul class=""><li id="3176" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo lh kz la lb dt translated">你了解Kubernetes 101</li><li id="8ddb" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo lh kz la lb dt translated">您拥有AWS的管理员凭据</li><li id="b4b4" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo lh kz la lb dt translated">如果你关注了其他帖子，你就会知道我们将使用Kubernetes 的<a class="ae jp" href="https://www.ubuntu.com/cloud/kubernetes" rel="noopener ugc nofollow" target="_blank"> Canonical发行版，因此一些关于Ubuntu、Juju和Canonical生态系统的知识会有所帮助。</a></li></ul><h1 id="8c47" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">前戏</h1><ul class=""><li id="123a" class="kt ku hu it b iu ko iy kp jc li jg lj jk lk jo lh kz la lb dt translated">确保你已经安装了Juju。</li></ul><p id="a260" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在Ubuntu上，</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="9660" class="lu jr hu lq b fv lv lw l lx ly">sudo apt-add-repository ppa:juju/stable<br/>sudo apt update<br/>sudo apt install -yqq juju </span></pre><p id="ca74" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">对于其他操作系统，查找<a class="ae jp" href="https://jujucharms.com/docs/2.0/getting-started-general" rel="noopener ugc nofollow" target="_blank">正式文档</a></p><p id="1ae3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后使用您的凭证连接到AWS云，请阅读<a class="ae jp" href="https://jujucharms.com/docs/2.0/help-aws" rel="noopener ugc nofollow" target="_blank">此页面</a></p><ul class=""><li id="53ad" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo lh kz la lb dt translated">最后复制这个回购协议，以获得所有的来源</li></ul><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="bb71" class="lu jr hu lq b fv lv lw l lx ly">git clone <a class="ae jp" href="https://github.com/madeden/blogposts" rel="noopener ugc nofollow" target="_blank">https://github.com/madeden/blogposts</a> ./<br/>cd blogposts/k8s-gpu-cloud</span></pre><p id="c7d5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">好吧！让我们开始GPU化世界吧！</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="d6c4" class="jq jr hu bd js jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn dt translated">部署集群</h1><h2 id="0ecf" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">助推器</h2><p id="6c02" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">像往常一样，从自举序列开始。请注意，p2实例仅在美国西部-2、美国东部-1和欧盟西部-2以及美国政府地区可用。我在欧盟运行p2实例时遇到了问题，因此我建议使用美国地区。</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="13c9" class="lu jr hu lq b fv lv lw l lx ly">juju bootstrap aws/us-east-1 — credential canonical — constraints “cores=4 mem=16G root-disk=64G” <br/># Creating Juju controller “aws-us-east-1” on aws/us-east-1<br/># Looking for packaged Juju agent version 2.1-rc1 for amd64<br/># Launching controller instance(s) on aws/us-east-1…<br/># — i-0d48b2c872d579818 (arch=amd64 mem=16G cores=4)<br/># Fetching Juju GUI 2.3.0<br/># Waiting for address<br/># Attempting to connect to 54.174.129.155:22<br/># Attempting to connect to 172.31.15.3:22<br/># Logging to /var/log/cloud-init-output.log on the bootstrap machine<br/># Running apt-get update<br/># Running apt-get upgrade<br/># Installing curl, cpu-checker, bridge-utils, cloud-utils, tmux<br/># Fetching Juju agent version 2.1-rc1 for amd64<br/># Installing Juju machine agent<br/># Starting Juju machine agent (service jujud-machine-0)<br/># Bootstrap agent now started<br/># Contacting Juju controller at 172.31.15.3 to verify accessibility…<br/># Bootstrap complete, “aws-us-east-1” controller now available.<br/># Controller machines are in the “controller” model.<br/># Initial model “default” added.</span></pre><h2 id="8b00" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">部署实例</h2><p id="93d3" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一旦控制器准备就绪，我们就可以开始部署服务了。在我之前的帖子中，我使用了<em class="my">包</em>，这是部署复杂应用程序的快捷方式。</p><p id="15b5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果你已经熟悉Juju，你可以运行<code class="eh mz na nb lq b">juju deploy src/k8s-gpu.yaml</code>并跳到这一节的末尾。对于其他对细节感兴趣的人，这次我们将手动部署，并检查部署的逻辑。</p><p id="d9b7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Kubernetes由5个独立的应用程序组成:Master、Worker、法兰绒(网络)、etcd(集群状态存储数据库)和easyRSA(加密通信并提供x509证书的PKI)。<br/>在Juju中，每个应用都由一个<em class="my">符咒</em>建模，这是一个如何部署它的配方。</p><p id="0f29" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在部署时，您可以对Juju进行约束，可以是非常具体的(实例类型)，也可以是宽松的(内核数量)。对于后者，Juju将选择最便宜的实例来匹配您在目标云上的约束。</p><p id="d7b0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先是部署应用程序:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="4375" class="lu jr hu lq b fv lv lw l lx ly">juju deploy cs:~containers/kubernetes-master-11 --constraints "cores=4 mem=8G root-disk=32G"<br/># Located charm "cs:~containers/kubernetes-master-11".<br/># Deploying charm "cs:~containers/kubernetes-master-11".<br/>juju deploy cs:~containers/etcd-23 --to 0<br/># Located charm "cs:~containers/etcd-23".<br/># Deploying charm "cs:~containers/etcd-23".<br/>juju deploy cs:~containers/easyrsa-6 --to lxd:0<br/># Located charm "cs:~containers/easyrsa-6".<br/># Deploying charm "cs:~containers/easyrsa-6".<br/>juju deploy cs:~containers/flannel-10<br/># Located charm "cs:~containers/flannel-10".<br/># Deploying charm "cs:~containers/flannel-10".<br/>juju deploy cs:~containers/kubernetes-worker-13 --constraints "instance-type=p2.xlarge" kubernetes-worker-gpu<br/># Located charm "cs:~containers/kubernetes-worker-13".<br/># Deploying charm "cs:~containers/kubernetes-worker-13".<br/>juju deploy cs:~containers/kubernetes-worker-13 --constraints "instance-type=p2.8xlarge" kubernetes-worker-gpu8<br/># Located charm "cs:~containers/kubernetes-worker-13".<br/># Deploying charm "cs:~containers/kubernetes-worker-13".<br/>juju deploy cs:~containers/kubernetes-worker-13 --constraints "instance-type=m4.2xlarge" -n3 kubernetes-worker-cpu<br/># Located charm "cs:~containers/kubernetes-worker-13".<br/># Deploying charm "cs:~containers/kubernetes-worker-13".</span></pre><p id="b7ba" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里您可以看到Juju中一个有趣的属性，我们以前从未接触过:命名您部署的服务。我们两次部署了相同的kubernetes-worker charm，但两次使用了GPU，另一次没有。这为我们提供了一种对某一类型的实例进行分组的方法，代价是要复制一些命令。</p><p id="1f5e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">还要注意我们部署的charms中的修订号。修订并不直接与他们部署的软件版本紧密相关。如果你忽略了它们，Juju会选择最新的版本，就像Docker对图片做的那样。</p><h2 id="3c25" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">添加关系和公开软件</h2><p id="62b6" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">现在应用程序已经部署好了，我们需要告诉Juju它们是如何联系在一起的。例如，Kubernetes master需要证书来保护其API。因此，<strong class="it hv">kubernetes-master:certificates</strong>和<strong class="it hv"> easyrsa:client </strong>之间存在关联。</p><p id="ff37" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这种关系意味着一旦两个应用程序连接起来，一些脚本将运行来查询EasyRSA API以创建所需的证书，然后将它们复制到k8s主机上的正确位置。</p><p id="5cb7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后，这些关系在集群中创建状态，charms可以对此做出反应。</p><p id="c0a6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">本质上，非常高的层次，把Juju看作应用程序部署的发布-订阅实现。集群内部或外部的每个动作都向公共总线发送一条消息，charms可以对这些消息做出反应并执行附加动作，修改整体状态……等等，直到达到平衡。</p><p id="9785" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们添加关系:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="fff4" class="lu jr hu lq b fv lv lw l lx ly">juju add-relation kubernetes-master:certificates easyrsa:client<br/>juju add-relation etcd:certificates easyrsa:client<br/>juju add-relation kubernetes-master:etcd etcd:db<br/>juju add-relation flannel:etcd etcd:db<br/>juju add-relation flannel:cni kubernetes-master:cni</span><span id="b196" class="lu jr hu lq b fv nc lw l lx ly">for TYPE in cpu gpu gpu8<br/>do <br/> juju add-relation kubernetes-worker-${TYPE}:kube-api-endpoint kubernetes-master:kube-api-endpoint<br/> juju add-relation kubernetes-master:cluster-dns kubernetes-worker-${TYPE}:kube-dns<br/> juju add-relation kubernetes-worker-${TYPE}:certificates easyrsa:client<br/> juju add-relation flannel:cni kubernetes-worker-${TYPE}:cni<br/> juju expose kubernetes-worker-${TYPE}<br/>done</span><span id="aef9" class="lu jr hu lq b fv nc lw l lx ly">juju expose kubernetes-master</span></pre><p id="2fc1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">注意最后的<strong class="it hv">曝光</strong>命令。<br/>这些是Juju在云中为实例的特定端口打开防火墙的指令。有些是在charms中预定义的(Kubernetes主API是6443，Workers为ingresses打开80和443 ),但是如果需要，您也可以强制它们(例如，当您在部署后的实例中手动添加内容时)</p><h2 id="cfff" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">添加CUDA</h2><p id="c629" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">CUDA还没有正式的魅力(即将推出！！)，但是你可以在<a class="ae jp" href="https://github.com/SaMnCo/layer-nvidia-cuda" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到我的demoware实现。针对本文，它已更新到CUDA 8.0.61和驱动程序375.26。</p><p id="73c3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">确保您有可用的魅力工具，克隆并构建CUDA魅力:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="bbde" class="lu jr hu lq b fv lv lw l lx ly">sudo apt install charm charm-tools</span><span id="c3c5" class="lu jr hu lq b fv nc lw l lx ly"># Exporting the ENV<br/>mkdir -p ~/charms ~/charms/layers ~/charms/interfaces<br/>export JUJU_REPOSITORY=${HOME}/charms<br/>export LAYER_PATH=${JUJU_REPOSITORY}/layers<br/>export INTERFACE_PATH=${JUJU_REPOSITORY}/interfaces</span><span id="1408" class="lu jr hu lq b fv nc lw l lx ly"># Build the charm<br/>cd ${LAYER_PATH}<br/>git clone <a class="ae jp" href="https://github.com/SaMnCo/layer-nvidia-cuda" rel="noopener ugc nofollow" target="_blank">https://github.com/SaMnCo/layer-nvidia-cuda</a> cuda<br/>charm build cuda</span></pre><p id="d137" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这将在JUJU_REPOSITORY中创建一个名为<strong class="it hv"> builds </strong>的新文件夹，并在那里创建另一个名为<strong class="it hv"> cuda </strong>的文件夹。</p><p id="1124" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在你可以施展魅力了</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="6d54" class="lu jr hu lq b fv lv lw l lx ly">juju deploy --series xenial $HOME/charms/builds/cuda<br/>juju add-relation cuda kubernetes-worker-gpu<br/>juju add-relation cuda kubernetes-worker-gpu8</span></pre><p id="4de0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这将需要相当长的时间，因为CUDA的安装时间很长(CDK大约需要10分钟，而CUDA可能需要15分钟)。</p><p id="8341" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然而，最后状态应该显示:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="668d" class="lu jr hu lq b fv lv lw l lx ly">juju status<br/>Model    Controller     Cloud/Region   Version<br/>default  aws-us-east-1  aws/us-east-1  2.1-rc1</span><span id="3393" class="lu jr hu lq b fv nc lw l lx ly">App                     Version  Status       Scale  Charm              Store       Rev  OS      Notes<br/>cuda                             active           2  cuda               local         2  ubuntu  <br/>easyrsa                 3.0.1    active           1  easyrsa            jujucharms    6  ubuntu  <br/>etcd                    2.2.5    active           1  etcd               jujucharms   23  ubuntu  <br/>flannel                 0.7.0    active           6  flannel            jujucharms   10  ubuntu  <br/>kubernetes-master       1.5.2    active           1  kubernetes-master  jujucharms   11  ubuntu  exposed<br/>kubernetes-worker-cpu   1.5.2    active           3  kubernetes-worker  jujucharms   13  ubuntu  exposed<br/>kubernetes-worker-gpu   1.5.2    active           1  kubernetes-worker  jujucharms   13  ubuntu  exposed<br/>kubernetes-worker-gpu8  1.5.2    active           1  kubernetes-worker  jujucharms   13  ubuntu  exposed</span><span id="5423" class="lu jr hu lq b fv nc lw l lx ly">Unit                       Workload     Agent      Machine  Public address  Ports           Message<br/>easyrsa/0*                 active       idle       0/lxd/0  10.0.0.122                      Certificate Authority connected.<br/>etcd/0*                    active       idle       0        54.242.44.224   2379/tcp        Healthy with 1 known peers.<br/>kubernetes-master/0*       active       idle       0        54.242.44.224   6443/tcp        Kubernetes master running.<br/>  flannel/0*               active       idle                54.242.44.224                   Flannel subnet 10.1.76.1/24<br/>kubernetes-worker-cpu/0    active       idle       4        52.86.161.22    80/tcp,443/tcp  Kubernetes worker running.<br/>  flannel/4                active       idle                52.86.161.22                    Flannel subnet 10.1.79.1/24<br/>kubernetes-worker-cpu/1*   active       idle       5        52.70.5.49      80/tcp,443/tcp  Kubernetes worker running.<br/>  flannel/2                active       idle                52.70.5.49                      Flannel subnet 10.1.63.1/24<br/>kubernetes-worker-cpu/2    active       idle       6        174.129.164.95  80/tcp,443/tcp  Kubernetes worker running.<br/>  flannel/3                active       idle                174.129.164.95                  Flannel subnet 10.1.22.1/24<br/>kubernetes-worker-gpu8/0*  active       idle       3        52.90.163.167   80/tcp,443/tcp  Kubernetes worker running.<br/>  cuda/1                   active       idle                52.90.163.167                   CUDA installed and available<br/>  flannel/5                active       idle                52.90.163.167                   Flannel subnet 10.1.35.1/24<br/>kubernetes-worker-gpu/0*   active       idle       1        52.90.29.98     80/tcp,443/tcp  Kubernetes worker running.<br/>  cuda/0*                  active       idle                52.90.29.98                     CUDA installed and available<br/>  flannel/1                active       idle                52.90.29.98                     Flannel subnet 10.1.58.1/24</span><span id="9c62" class="lu jr hu lq b fv nc lw l lx ly">Machine  State    DNS             Inst id              Series  AZ<br/>0        started  54.242.44.224   i-09ea4f951f651687f  xenial  us-east-1a<br/>0/lxd/0  started  10.0.0.122      juju-65a910-0-lxd-0  xenial  <br/>1        started  52.90.29.98     i-03c3e35c2e8595491  xenial  us-east-1c<br/>3        started  52.90.163.167   i-0ca0716985645d3f2  xenial  us-east-1d<br/>4        started  52.86.161.22    i-02de3aa8efcd52366  xenial  us-east-1e<br/>5        started  52.70.5.49      i-092ac5367e31188bb  xenial  us-east-1a<br/>6        started  174.129.164.95  i-0a0718343068a5c94  xenial  us-east-1c</span><span id="82b6" class="lu jr hu lq b fv nc lw l lx ly">Relation      Provides                Consumes                Type<br/>juju-info     cuda                    kubernetes-worker-gpu   regular<br/>juju-info     cuda                    kubernetes-worker-gpu8  regular<br/>certificates  easyrsa                 etcd                    regular<br/>certificates  easyrsa                 kubernetes-master       regular<br/>certificates  easyrsa                 kubernetes-worker-cpu   regular<br/>certificates  easyrsa                 kubernetes-worker-gpu   regular<br/>certificates  easyrsa                 kubernetes-worker-gpu8  regular<br/>cluster       etcd                    etcd                    peer<br/>etcd          etcd                    flannel                 regular<br/>etcd          etcd                    kubernetes-master       regular<br/>cni           flannel                 kubernetes-master       regular<br/>cni           flannel                 kubernetes-worker-cpu   regular<br/>cni           flannel                 kubernetes-worker-gpu   regular<br/>cni           flannel                 kubernetes-worker-gpu8  regular<br/>cni           kubernetes-master       flannel                 subordinate<br/>kube-dns      kubernetes-master       kubernetes-worker-cpu   regular<br/>kube-dns      kubernetes-master       kubernetes-worker-gpu   regular<br/>kube-dns      kubernetes-master       kubernetes-worker-gpu8  regular<br/>cni           kubernetes-worker-cpu   flannel                 subordinate<br/>juju-info     kubernetes-worker-gpu   cuda                    subordinate<br/>cni           kubernetes-worker-gpu   flannel                 subordinate<br/>juju-info     kubernetes-worker-gpu8  cuda                    subordinate<br/>cni           kubernetes-worker-gpu8  flannel                 subordinate</span></pre><p id="9dcb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们看看nvidia-smi给了我们什么:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="015a" class="lu jr hu lq b fv lv lw l lx ly">juju ssh kubernetes-worker-gpu/0 sudo nvidia-smi<br/>Tue Feb 14 13:28:42 2017       <br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|===============================+======================+======================|<br/>|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |<br/>| N/A   33C    P0    81W / 149W |      0MiB / 11439MiB |     95%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>                                                                               <br/>+-----------------------------------------------------------------------------+<br/>| Processes:                                                       GPU Memory |<br/>|  GPU       PID  Type  Process name                               Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+</span></pre><p id="9164" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在更强大的8xlarge上，</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="ad1a" class="lu jr hu lq b fv lv lw l lx ly">juju ssh kubernetes-worker-gpu8/0 sudo nvidia-smi<br/>Tue Feb 14 13:59:24 2017       <br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|===============================+======================+======================|<br/>|   0  Tesla K80           On   | 0000:00:17.0     Off |                    0 |<br/>| N/A   41C    P8    31W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   1  Tesla K80           On   | 0000:00:18.0     Off |                    0 |<br/>| N/A   36C    P0    70W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   2  Tesla K80           On   | 0000:00:19.0     Off |                    0 |<br/>| N/A   44C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   3  Tesla K80           On   | 0000:00:1A.0     Off |                    0 |<br/>| N/A   38C    P0    70W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   4  Tesla K80           On   | 0000:00:1B.0     Off |                    0 |<br/>| N/A   43C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   5  Tesla K80           On   | 0000:00:1C.0     Off |                    0 |<br/>| N/A   38C    P0    69W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   6  Tesla K80           On   | 0000:00:1D.0     Off |                    0 |<br/>| N/A   44C    P0    58W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   7  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |<br/>| N/A   38C    P0    71W / 149W |      0MiB / 11439MiB |     39%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>                                                                               <br/>+-----------------------------------------------------------------------------+<br/>| Processes:                                                       GPU Memory |<br/>|  GPU       PID  Type  Process name                               Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+</span></pre><p id="0ae0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">是的。！我们有我们预期的8个GPU，因此8x 12GB = 96GB视频内存！</p><p id="b3c1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在此阶段，我们只在主机上启用了它们。现在让我们在Kubernetes中添加GPU支持。</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="eec3" class="jq jr hu bd js jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn dt translated">在Kubernetes中添加GPU支持</h1><p id="884e" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">默认情况下，CDK在启动API服务器和Kubelets时不会激活GPU。我们需要手动操作(目前)。</p><h2 id="c5c1" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">主更新</h2><p id="e267" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在主节点上，更新<strong class="it hv">/etc/default/kube-API server</strong>以添加:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="4478" class="lu jr hu lq b fv lv lw l lx ly"># Security Context<br/>KUBE_ALLOW_PRIV="--allow-privileged=true"</span></pre><p id="7723" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在重新启动API服务器之前。这可以通过编程实现:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="5ecc" class="lu jr hu lq b fv lv lw l lx ly">juju show-status kubernetes-master --format json | \<br/>    jq --raw-output '.applications."kubernetes-master".units | keys[]' | \<br/>    xargs -I UNIT juju ssh UNIT "echo -e '\n# Security Context \nKUBE_ALLOW_PRIV=\"--allow-privileged=true\"' | sudo tee -a /etc/default/kube-apiserver &amp;&amp; sudo systemctl restart kube-apiserver.service"</span></pre><p id="d7aa" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以现在Kube API将接受运行特权容器的请求，这是GPU工作负载所需要的。</p><h2 id="9a8e" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">工作节点</h2><p id="90ee" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在每个worker上，<strong class="it hv"> /etc/default/kubelet </strong>来添加GPU标签，所以看起来像:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="b734" class="lu jr hu lq b fv lv lw l lx ly"># Security Context<br/>KUBE_ALLOW_PRIV="--allow-privileged=true"</span><span id="dbb9" class="lu jr hu lq b fv nc lw l lx ly"># Add your own!<br/>KUBELET_ARGS="--experimental-nvidia-gpus=1 --require-kubeconfig --kubeconfig=/srv/kubernetes/config --cluster-dns=10.1.0.10 --cluster-domain=cluster.local"</span></pre><p id="c888" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在重新启动服务之前。</p><p id="6278" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这可以通过</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="4e89" class="lu jr hu lq b fv lv lw l lx ly">for WORKER_TYPE in gpu gpu8<br/>do<br/>    juju show-status kubernetes-worker-${WORKER_TYPE} --format json | \<br/>        jq --raw-output '.applications."kubernetes-worker-'${WORKER_TYPE}'".units | keys[]' | \<br/>        xargs -I UNIT juju ssh UNIT "echo -e '\n# Security Context \nKUBE_ALLOW_PRIV=\"--allow-privileged=true\"' | sudo tee -a /etc/default/kubelet"</span><span id="a8a6" class="lu jr hu lq b fv nc lw l lx ly">juju show-status kubernetes-worker-${WORKER_TYPE} --format json | \<br/>    jq --raw-output '.applications."kubernetes-worker-'${WORKER_TYPE}'".units | keys[]' | \<br/>    xargs -I UNIT juju ssh UNIT "sudo sed -i 's/KUBELET_ARGS=\"/KUBELET_ARGS=\"--experimental-nvidia-gpus=1\ /' /etc/default/kubelet &amp;&amp; sudo systemctl restart kubelet.service"</span><span id="f4bf" class="lu jr hu lq b fv nc lw l lx ly">done</span></pre><h2 id="2f18" class="lu jr hu bd js ml mm mn jw mo mp mq ka jc mr ms ke jg mt mu ki jk mv mw km mx dt translated">测试我们的设置</h2><p id="d5b1" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">现在我们想知道集群是否真的启用了GPU。要进行验证，请使用nvidia-smi pod运行作业:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="1ae9" class="lu jr hu lq b fv lv lw l lx ly">kubectl create -f src/nvidia-smi.yaml</span></pre><p id="b53f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后稍等片刻，运行log命令:</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="44f9" class="lu jr hu lq b fv lv lw l lx ly">kubectl logs $(kubectl get pods -l name=nvidia-smi -o=name -a)<br/>Tue Feb 14 14:14:57 2017       <br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|===============================+======================+======================|<br/>|   0  Tesla K80           Off  | 0000:00:17.0     Off |                    0 |<br/>| N/A   47C    P0    56W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   1  Tesla K80           Off  | 0000:00:18.0     Off |                    0 |<br/>| N/A   39C    P0    70W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   2  Tesla K80           Off  | 0000:00:19.0     Off |                    0 |<br/>| N/A   48C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   3  Tesla K80           Off  | 0000:00:1A.0     Off |                    0 |<br/>| N/A   41C    P0    70W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   4  Tesla K80           Off  | 0000:00:1B.0     Off |                    0 |<br/>| N/A   47C    P0    58W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   5  Tesla K80           Off  | 0000:00:1C.0     Off |                    0 |<br/>| N/A   40C    P0    69W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   6  Tesla K80           Off  | 0000:00:1D.0     Off |                    0 |<br/>| N/A   48C    P0    59W / 149W |      0MiB / 11439MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>|   7  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |<br/>| N/A   41C    P0    72W / 149W |      0MiB / 11439MiB |    100%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>                                                                               <br/>+-----------------------------------------------------------------------------+<br/>| Processes:                                                       GPU Memory |<br/>|  GPU       PID  Type  Process name                               Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+</span></pre><p id="50f3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">that在这里感兴趣的是，pod可以看到所有的卡，即使我们只共享了<strong class="it hv"> /dev/nvidia0 </strong> char设备。在运行时，我们会遇到问题。<br/>如果你想运行多个GPU容器，你需要像我们在第二个yaml文件(nvidia-smi-8.yaml)中所做的那样共享所有的char设备</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="c49f" class="jq jr hu bd js jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn dt translated">结论</h1><p id="5c57" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">我们到达了我们3部分旅程的第一个里程碑:集群启动并运行，GPU被激活，Kubernetes现在将欢迎GPU工作负载。</p><p id="311f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果您是一名数据科学家或运行Kubernetes工作负载，可以受益于GPU，这已经为您提供了一种优雅而快速的管理设置的方式。但是通常在这种情况下，您还需要在实例之间有可用的存储，无论是为了共享数据集还是为了交换结果。</p><p id="eaed" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Kubernetes提供了许多连接储物的选项。在博客的第二部分，我们将看到如何自动添加EFS存储到我们的实例，然后把它很好地用于一些数据集！</p><p id="6612" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">同时，如果您在云中有具体的使用案例，请随时联系我，讨论操作细节。我很乐意帮助你建立自己的GPU集群，让你开始学习科学！</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="92f4" class="jq jr hu bd js jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn dt translated">拆卸</h1><p id="626c" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">只要你愿意，你可以拆掉这个集群。这些实例可能很昂贵，因此在不使用它们时关闭它们并不是一个坏主意。</p><pre class="ll lm ln lo fq lp lq lr ls aw lt dt"><span id="df81" class="lu jr hu lq b fv lv lw l lx ly">juju kill-controller aws/us-east-1</span></pre><p id="af8c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这将要求确认，然后摧毁一切…但现在，你只是几个命令和咖啡远离重建它，所以这不是一个问题。</p><blockquote class="nd ne nf"><p id="effc" class="ir is my it b iu iv iw ix iy iz ja jb ng jd je jf nh jh ji jj ni jl jm jn jo hn dt translated"><a class="ae jp" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae jp" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae jp" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae jp" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="dca4" class="ir is my it b iu iv iw ix iy iz ja jb ng jd je jf nh jh ji jj ni jl jm jn jo hn dt translated">要了解更多信息，请<a class="ae jp" href="https://goo.gl/4ofytp" rel="noopener ugc nofollow" target="_blank">阅读我们的“关于”页面</a>、<a class="ae jp" href="http://bit.ly/HackernoonFB" rel="noopener ugc nofollow" target="_blank">在脸书上点赞/给我们发消息</a>，或者简单地说，<a class="ae jp" href="https://goo.gl/k7XYbx" rel="noopener ugc nofollow" target="_blank"> tweet/DM @HackerNoon。</a></p><p id="708a" class="ir is my it b iu iv iw ix iy iz ja jb ng jd je jf nh jh ji jj ni jl jm jn jo hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jp" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jp" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="ll lm ln lo fq nj"><div class="bz el l di"><div class="nk nl l"/></div></figure></div></div>    
</body>
</html>