<html>
<head>
<title>Neural Networks Without a PhD: Topologies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">没有博士学位的神经网络:拓扑</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/neural-networks-without-a-phd-topologies-2e7a199bf18f?source=collection_archive---------16-----------------------#2017-04-18">https://medium.com/hackernoon/neural-networks-without-a-phd-topologies-2e7a199bf18f?source=collection_archive---------16-----------------------#2017-04-18</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="a6f2" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">神经网络的拓扑是指神经元连接的方式，它是网络功能和学习的重要因素。无监督学习中的常见拓扑是将输入直接映射到表示类别的单元集合(例如，自组织映射)。— <a class="ae jt" href="http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_837" rel="noopener ugc nofollow" target="_blank">斯普林格</a></p></blockquote><p id="9775" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在我们之前的帖子中，我们了解了构成神经网络的组件:</p><ul class=""><li id="8a1b" class="jx jy hu ix b iy iz jc jd ju jz jv ka jw kb js kc kd ke kf dt translated">输入节点</li><li id="a69f" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">神经元</li><li id="6eff" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">输出节点</li></ul><p id="86c7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">并且这些元素将被组织在以下层中:</p><ul class=""><li id="ba15" class="jx jy hu ix b iy iz jc jd ju jz jv ka jw kb js kc kd ke kf dt translated">输入层</li><li id="7356" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">隐藏层</li><li id="b55f" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">输出层</li></ul><p id="eeb8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">然而，仍然不太清楚组织和连接这些神经网络的过程是什么；我们如何确定需要创建多少隐藏层？我们如何将这些神经元连接到输入、输出以及彼此之间？我们如何知道我们需要创建多少输出节点？</p><h2 id="7c82" class="kl km hu bd kn ko kp kq kr ks kt ku kv ju kw kx ky jv kz la lb jw lc ld le lf dt translated">如何组织我们的神经网络</h2><p id="ec41" class="pw-post-body-paragraph iu iv hu ix b iy lg ja jb jc lh je jf ju li ji jj jv lj jm jn jw lk jq jr js hn dt translated">我们需要的是一种方法来表示神经元如何连接以形成网络，一种<strong class="ix hv">神经网络拓扑</strong>。神经网络的拓扑在它的功能和性能中起着重要的作用。</p><p id="662e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">需要注意的重要一点是，<strong class="ix hv">多种拓扑</strong>可以用来学习同一组数据，它们甚至可能产生相似的结果；因此，对于单个神经网络来说，不存在“最佳”拓扑；也就是说，拓扑结构会极大地影响神经网络学习数据所需的时间以及对新数据进行分类时的准确性。</p><p id="020c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">拓扑选择的一种方法是简单地使其成为一个试错过程，其中我们通过修改诸如<strong class="ix hv">隐藏层</strong>、<strong class="ix hv">节点之间的连接、</strong>等的数量来手动调整神经网络。采用这种方法会极大地限制我们，因为即使对于一个简单的神经网络来说也有许多可能的排列。更复杂的是，神经网络并不局限于我们之前见过的相对简单的<strong class="ix hv">前馈</strong>布局，仅举几个例子:</p><ul class=""><li id="96c6" class="jx jy hu ix b iy iz jc jd ju jz jv ka jw kb js kc kd ke kf dt translated">前馈</li><li id="5e16" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">复发的</li><li id="d53e" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">长短期记忆</li><li id="e853" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">约旦</li><li id="89e9" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">埃尔曼</li><li id="b06f" class="jx jy hu ix b iy kg jc kh ju ki jv kj jw kk js kc kd ke kf dt translated">霍普菲尔德</li></ul><figure class="lm ln lo lp fq lq fe ff paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="fe ff ll"><img src="../Images/60753e1671553f1295a322ec185cc15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*COe-N5ejUgQdC0Ze.png"/></div></div></figure><p id="de08" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">因此，除了非常简单的应用、已知问题或学习练习<strong class="ix hv">之外，手动拓扑选择</strong>并不可行，为了找到理想的拓扑，我们必须让我们的神经网络也从训练数据中学习拓扑。目标是为我们的神经网络找到一种拓扑结构，使新数据的误差最小化。* </p><p id="0162" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">实现这一目标的最新颖的方法之一是使用遗传编程。</p><h2 id="2ee9" class="kl km hu bd kn ko kp kq kr ks kt ku kv ju kw kx ky jv kz la lb jw lc ld le lf dt translated">遗传编程和神经进化</h2><p id="61e3" class="pw-post-body-paragraph iu iv hu ix b iy lg ja jb jc lh je jf ju li ji jj jv lj jm jn jw lk jq jr js hn dt translated">遗传算法基于<strong class="ix hv">自然选择</strong>、<strong class="ix hv">变异</strong>、<strong class="ix hv">适者生存</strong>的进化原理；遗传算法将通过生成大量潜在解决方案并找到具有最佳<strong class="ix hv">适应值</strong>的解决方案来解决特定问题。</p><p id="ca8e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">无需深入研究，一个<strong class="ix hv">适应值</strong>是一种测量解决方案与已知结果或预期指标接近程度的方法；在神经网络的情况下，我们有训练数据可以用作参考点。</p><p id="2c6f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">遗传算法将一代又一代地调整和变异我们的神经网络，直到达到期望的适应度分数，或者直到我们达到最大迭代次数。</p><p id="ab2d" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">免责声明:以上是对遗传算法如何工作的过于简化的描述</strong></p><p id="10b2" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我强烈推荐,<a class="ae jt" href="https://link.springer.com/book/10.1007/978-1-4614-4463-3" rel="noopener ugc nofollow" target="_blank">Erlang的《神经进化手册》</a>,即使你对Erlang不感兴趣，该书作者在分解概念和介绍遗传算法方面做得非常好。</p><h1 id="d32d" class="lx km hu bd kn ly lz ma kr mb mc md kv me mf mg ky mh mi mj lb mk ml mm le mn dt translated">摘要</h1><p id="2c3c" class="pw-post-body-paragraph iu iv hu ix b iy lg ja jb jc lh je jf ju li ji jj jv lj jm jn jw lk jq jr js hn dt translated">重要的是，我们要理解拓扑结构，也就是我们神经网络的<strong class="ix hv">层数以及它们是如何连接的</strong>如何影响它们的有效性；有许多现有类型的人工神经网络，其中一些最适合特定的应用任务，熟悉最常见的类型是很重要的。</p><p id="6741" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">此外，为了找到最佳拓扑，可以使用<strong class="ix hv">遗传算法</strong>来尝试为给定的应用开发<strong class="ix hv">“完美的”</strong>拓扑。</p><p id="3608" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">还有很多要学的和要做的，但是如果你发现任何事实错误或者你认为有些事情需要更多的澄清，请随意留下评论。</p><p id="d367" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><em class="iw">本文原帖</em> <a class="ae jt" href="http://coderoncode.com/machine-learning/2017/04/16/neural-networks-without-a-phd-part3.html" rel="noopener ugc nofollow" target="_blank"> <em class="iw">在我自己的网站</em> </a> <em class="iw">。</em></p><div class="lm ln lo lp fq ab cb"><figure class="mo lq mp mq mr ms mt paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mo lq mp mq mr ms mt paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mo lq mp mq mr ms mt paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="ir is it"><p id="f922" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae jt" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae jt" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae jt" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae jt" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae jt" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="lm ln lo lp fq lq fe ff paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="fe ff mu"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure></div></div>    
</body>
</html>