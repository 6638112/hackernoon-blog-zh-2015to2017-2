<html>
<head>
<title>Deep Learning isn’t the brain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习不是大脑</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deep-learning-isnt-the-brain-e1d800ebb5a9?source=collection_archive---------4-----------------------#2016-11-17">https://medium.com/hackernoon/deep-learning-isnt-the-brain-e1d800ebb5a9?source=collection_archive---------4-----------------------#2016-11-17</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="3998" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">(但有时结果看起来像一个)</h2></div><p id="bb94" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">认知状态:我在一个致力于生物学上合理的神经回路的实验室工作，所以我了解这个问题，但可能还是有偏见。一旦我收到一堆比我聪明的人的反驳，可能会有后续的帖子。] </p><p id="32a5" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jl hv"> <em class="kf">更新:</em> </strong> <em class="kf">不出所料，我得到了一堆反驳，已经</em> <a class="ae kg" rel="noopener" href="/@seanaubin/deep-learning-is-almost-the-brain-3aaecd924f3d"> <em class="kf">相应调整了我的立场</em> </a></p><p id="1c08" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">一直看到<a class="ae kg" href="http://biorxiv.org/content/biorxiv/early/2016/08/23/071076.full.pdf" rel="noopener ugc nofollow" target="_blank">学术</a>和<a class="ae kg" href="http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/" rel="noopener ugc nofollow" target="_blank">非学术文章</a>比较深度<a class="ae kg" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a> (DL)和大脑。这有点冒犯我的感受，因为尽管有许多来自DL的<a class="ae kg" href="http://compneuro.uwaterloo.ca/publications/sharma2016b.html" rel="noopener ugc nofollow" target="_blank"/><a class="ae kg" href="http://www.pnas.org/content/111/23/8619.abstract" rel="noopener ugc nofollow" target="_blank">结果</a>类似于大脑的某些区域，但是DL并不是大脑的一个很好的整体描述。DL明确地将责任推给了生物合理性(就像几乎所有其他认知建模方法一样)，并暗示它的“神经元”可以在生物学上实现，只是还没有人为此烦恼。我认为问题更深层次，DL缺少大脑的许多关键特征，这使它成为一个糟糕的类比目标。</p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><h2 id="03db" class="ko kp hu bd kq kr ks kt ku kv kw kx ky js kz la lb jw lc ld le ka lf lg lh li dt translated">大脑是低能量的</h2><p id="4bcc" class="pw-post-body-paragraph jj jk hu jl b jm lj iv jo jp lk iy jr js ll ju jv jw lm jy jz ka ln kc kd ke hn dt translated">DL很耗电。Alpha GO消耗了<a class="ae kg" href="https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/" rel="noopener ugc nofollow" target="_blank"> 1202个CPU和</a>176个GPU的电量，不是为了训练，而是<em class="kf">只是为了运行</em>。<a class="ae kg" href="https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html" rel="noopener ugc nofollow" target="_blank"> TenserFlow处理单元</a>试图满足这种渴望，但它甚至还没有接近<a class="ae kg" href="http://cogsci.stackexchange.com/q/12750/4397" rel="noopener ugc nofollow" target="_blank">大脑20W </a>的功耗。IBM的TrueNorth芯片是另一个试图实现低功耗计算的例子，但与其他神经形态硬件相比，它的能力非常有限。具体来说，真北只实现前馈网络，没有片上学习。</p><h2 id="295c" class="ko kp hu bd kq kr ks kt ku kv kw kx ky js kz la lb jw lc ld le ka lf lg lh li dt translated">大脑不会背撑</h2><p id="2da8" class="pw-post-body-paragraph jj jk hu jl b jm lj iv jo jp lk iy jr js ll ju jv jw lm jy jz ka ln kc kd ke hn dt translated">反向传播是所有DL的基础。尽管有证据表明错误通过多层传播正在大脑中发生，但没有人提出一种不依赖于通过单向突触反向传播的信息的反向传播(back-prop)方法。我个人认为，发现一种生物学上看似合理的方法只是时间问题，但在此之前，忽视实施细节和它可能对学习内容的限制是不明智的。</p><h2 id="f8e6" class="ko kp hu bd kq kr ks kt ku kv kw kx ky js kz la lb jw lc ld le ka lf lg lh li dt translated">大脑使用尖峰信号来交流</h2><p id="7897" class="pw-post-body-paragraph jj jk hu jl b jm lj iv jo jp lk iy jr js ll ju jv jw lm jy jz ka ln kc kd ke hn dt translated">虽然有可能<a class="ae kg" href="https://arxiv.org/abs/1510.08829" rel="noopener ugc nofollow" target="_blank">将DL网络转换成脉冲神经元以用于神经形态硬件</a>，但是这些脉冲并没有被用于特定的计算优势。据我所知(我还有一些阅读要做)，尖峰计算还没有在任何地方用于DL的学习。</p><h2 id="d2bd" class="ko kp hu bd kq kr ks kt ku kv kw kx ky js kz la lb jw lc ld le ka lf lg lh li dt translated">神经递质不仅仅是尖峰信号运输者，神经元也不仅仅是尖峰信号机器</h2><p id="0cdf" class="pw-post-body-paragraph jj jk hu jl b jm lj iv jo jp lk iy jr js ll ju jv jw lm jy jz ka ln kc kd ke hn dt translated">DL完全忽略了神经递质的作用。然而，神经递质已经被证明在动态适应接受特征网络方面具有重要的计算意义，这是DL很难做到的。</p><h2 id="c999" class="ko kp hu bd kq kr ks kt ku kv kw kx ky js kz la lb jw lc ld le ka lf lg lh li dt translated">大脑是嘈杂的</h2><p id="85d7" class="pw-post-body-paragraph jj jk hu jl b jm lj iv jo jp lk iy jr js ll ju jv jw lm jy jz ka ln kc kd ke hn dt translated">鉴于在神经元冗余和神经元性能之间的选择，进化选择了让大脑冗余。神经元是嘈杂的，当你考虑到它们所处的温暖、生物可变的环境时，这并不奇怪。虽然某些DL网络可以处理节点丢失，但DL并不以其对噪声输入或噪声训练数据的鲁棒性而闻名。</p></div><div class="ab cl kh ki hc kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hn ho hp hq hr"><p id="1882" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">总之，DL忽略了大脑的许多特征，因此用DL来类比神经回路并不理想。与DL进行比较的替代方法是使用考虑到这些挑战的建模范例。在撰写本文时，我所知道的唯一方法是我所属实验室的神经工程框架(NEF ),但我确信随着研究的进展，其他框架将会出现。</p><p id="44b5" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">“但是肖恩，”你眼里闪着调皮的光芒喊道，“你所属的实验室的大多数NEF模型不都有和DL一样的问题吗？模型通常停留在LIF神经元，这根本不是真实的神经元！为什么不使用更复杂的神经元模型，比如像<a class="ae kg" href="http://pub.ist.ac.at/Pubs/courses/AY1314/MolandCellNeuro_S14/files/Stuart%20et%20al_Dendrites_Chapter16.pdf" rel="noopener ugc nofollow" target="_blank">树突计算</a>，多室神经元、<a class="ae kg" href="http://www.cell.com/trends/immunology/abstract/S1471-4906%2815%2900200-8" rel="noopener ugc nofollow" target="_blank">胶质细胞</a>和神经发生这样的东西？”</p><p id="8cc0" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这是一项正在进行的工作。我所在的<a class="ae kg" href="http://compneuro.uwaterloo.ca/index.html" rel="noopener ugc nofollow" target="_blank">计算神经科学研究小组</a> (CNRG)的16人中有2人(<a class="ae kg" href="http://compneuro.uwaterloo.ca/people/aaron-russell-voelker.html" rel="noopener ugc nofollow" target="_blank"> Aaron Voelker </a>和<a class="ae kg" href="http://compneuro.uwaterloo.ca/people/peter-duggins.html" rel="noopener ugc nofollow" target="_blank"> Peter Duggins </a>)正在研究更复杂的神经元模型问题。Eric Hunsberger正在研究一种生物上合理的支撑物，一旦他有了突破，你可以肯定我会把它推到每个人的脸上。</p><p id="8d12" class="pw-post-body-paragraph jj jk hu jl b jm jn iv jo jp jq iy jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">至于神经发生，没有一个好的计算模型来解释神经发生做了什么，CNRG也缺乏资源来做这种基础研究。树突和神经胶质细胞的计算没有人在做，因为我们只有16个人。如果你觉得不舒服，也许你想加入我们？</p><blockquote class="lo lp lq"><p id="464a" class="jj jk kf jl b jm jn iv jo jp jq iy jr lr jt ju jv ls jx jy jz lt kb kc kd ke hn dt translated"><a class="ae kg" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae kg" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae kg" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae kg" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="dca4" class="jj jk kf jl b jm jn iv jo jp jq iy jr lr jt ju jv ls jx jy jz lt kb kc kd ke hn dt translated">要了解更多信息，请<a class="ae kg" href="https://goo.gl/4ofytp" rel="noopener ugc nofollow" target="_blank">阅读我们的“关于”页面</a>、<a class="ae kg" href="http://bit.ly/HackernoonFB" rel="noopener ugc nofollow" target="_blank">在脸书上点赞/给我们发消息</a>，或者简单地说，<a class="ae kg" href="https://goo.gl/k7XYbx" rel="noopener ugc nofollow" target="_blank"> tweet/DM @HackerNoon。</a></p><p id="708a" class="jj jk kf jl b jm jn iv jo jp jq iy jr lr jt ju jv ls jx jy jz lt kb kc kd ke hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae kg" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae kg" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="lu lv lw lx fq ly"><div class="bz el l di"><div class="lz ma l"/></div></figure></div></div>    
</body>
</html>