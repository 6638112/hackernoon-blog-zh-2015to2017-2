<html>
<head>
<title>What Killed the Curse of Dimensionality?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是什么杀死了维度的诅咒？</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/what-killed-the-curse-of-dimensionality-8dbfad265bbe?source=collection_archive---------4-----------------------#2017-09-06">https://medium.com/hackernoon/what-killed-the-curse-of-dimensionality-8dbfad265bbe?source=collection_archive---------4-----------------------#2017-09-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="4209" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">深度<a class="ae jp" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>如何克服机器学习中的这道坎，为什么？</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/0635fb8574c07c357bf27eaf3fb870b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9-utjjtjQRrLrXm69zRHw.png"/></div></div></figure><p id="3d3c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，我们应该清楚地定义维数灾难:</p><blockquote class="kc kd ke"><p id="801e" class="ir is kf it b iu iv iw ix iy iz ja jb kg jd je jf kh jh ji jj ki jl jm jn jo hn dt translated">当数据的维度增加时，数据的稀疏性增加。</p></blockquote><p id="b196" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">数据是有维度的。添加到数据中的维度越多，就越难找到模式。把维度想象成你在玩捉人游戏的动物的活动范围。如果你在追逐一只只能在地面上移动的动物，它们只能在二维空间内移动，向左或向右(x)，向前或向后(y)。抓住一只鸟更难，因为这只鸟可以在三维空间里向左或向右(x)，向前或向后(y)，向上或向下(z)移动。我们可以想象一些神话中的时间旅行兽，它们可以在4个维度上移动，向左或向右(x)，向前或向后(y)，向上或向下(z)，过去或未来(t)。你可以看到，随着维度的增加，这变得更加困难。</p><p id="e452" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">同样的问题也适用于数据和机器学习。随着数据维度的增加，数据的稀疏性增加，从而更难确定模式。在传统的ML中，有一些绕过维数灾难的方法，这些方法需要某些技术，如函数平滑和近似。深度学习凭借其固有的特性克服了这一“诅咒”，可能是受欢迎程度增加的贡献之一。</p><h1 id="a6b8" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">深度学习</h1><p id="1b99" class="pw-post-body-paragraph ir is hu it b iu lh iw ix iy li ja jb jc lj je jf jg lk ji jj jk ll jm jn jo hn dt translated">在高维度应用中，深度学习不会遭受与线性回归等其他机器学习算法相同的后果。这一事实是这种用神经网络建模的方法如此有效的魔力的一部分。在当今的大数据世界中，神经网络对维数灾难的不渗透性是一个有用的特征。</p><p id="3025" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">关于为什么会发生这种情况，有多种理论。现在，我们将快速回顾一下每个主题的主要内容:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff lm"><img src="../Images/434718dde37466195cbd495626eda7ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*WYp07s3adFKy7Jpbvw2ggA.png"/></div><figcaption class="ln lo fg fe ff lp lq bd b be z ek">Linear Manifold</figcaption></figure><h2 id="02b7" class="lr kk hu bd kl ls lt lu kp lv lw lx kt jc ly lz kx jg ma mb lb jk mc md lf me dt translated">流形假设[1]</h2><p id="0e71" class="pw-post-body-paragraph ir is hu it b iu lh iw ix iy li ja jb jc lj je jf jg lk ji jj jk ll jm jn jo hn dt translated">在高层次上，流形假设表明高维数据实际上位于嵌入在高维空间中的低维流形上。</p><p id="ae53" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">从某种意义上来说，这几乎意味着在这种高维数据中，在较低水平的维度中存在一些深度学习方法擅长利用的潜在模式。因此，给定一个表示图像的高维矩阵，神经网络擅长发现在高维表示中不明显的低维特征。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mf"><img src="../Images/78a3fdba2115dfbb28c06b434bbc6777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4SksBV0SDivBhX9lLmiYg.png"/></div></div><figcaption class="ln lo fg fe ff lp lq bd b be z ek">Image from deep learning book. Manifold over high dimensional data</figcaption></figure><p id="a510" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">上图代表了高维空间中的一组数据点。在这篇博文中，它是二维的，以便于表达。点与点之间有一条线的表示在这个高维空间中有一些流形，大部分数据都在那里。那个流形是连接所有点的线。神经网络和深度学习方法利用了这一点，并在理论上找到了这种多样性。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/949fe350ed30204c8f8fa0054914602a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*hdYvpxzMOb9jV_QBVIiGLg.png"/></div><figcaption class="ln lo fg fe ff lp lq bd b be z ek">Visualization of activations from different neurons of a Convolution Neural Network</figcaption></figure><h2 id="0b71" class="lr kk hu bd kl ls lt lu kp lv lw lx kt jc ly lz kx jg ma mb lb jk mc md lf me dt translated">稀疏编码[2]</h2><p id="cbf1" class="pw-post-body-paragraph ir is hu it b iu lh iw ix iy li ja jb jc lj je jf jg lk ji jj jk ll jm jn jo hn dt translated">当数据流经网络中不同的神经元时，就会发生这种情况。神经网络中的每个神经元都有自己的激活功能。当每个神经元被激活时，它会导致稀疏编码。</p><p id="7fb6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">上图描绘了神经网络激活的可视化。每一个单独的神经元都从一幅图像中获得不同的特征。上面的图示显示了不同的神经元如何获得不同的特征，并且每个特征都对网络的最终输出做出贡献。</p><p id="ea32" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">例如，英语由26个字母组成。这些字母可以组成这种语言的所有单词。就神经网络而言，有一些神经元在被激发时可以与其他神经元的激发相结合，以输出正确的答案，而不管输入的维度如何。</p><h2 id="d3e7" class="lr kk hu bd kl ls lt lu kp lv lw lx kt jc ly lz kx jg ma mb lb jk mc md lf me dt translated">结论</h2><p id="3dac" class="pw-post-body-paragraph ir is hu it b iu lh iw ix iy li ja jb jc lj je jf jg lk ji jj jk ll jm jn jo hn dt translated">没有唯一被证实的理论表明为什么神经网络克服了维数灾难。有很多研究正在进行，以了解允许深度学习和神经网络在实践中如此有效的不同方面和潜在特征。在那之前，我们对引擎盖下正在进行的概念类型有一个很好的想法，这有助于推动深度学习和神经网络在实践中使用。</p><blockquote class="mh"><p id="02f7" class="mi mj hu bd mk ml mm mn mo mp mq jo ek translated">更多深度学习文章，请查看<a class="ae jp" href="http://camron.xyz" rel="noopener ugc nofollow" target="_blank"> camron.xyz </a></p></blockquote><h2 id="7d20" class="lr kk hu bd kl ls mr lu kp lv ms lx kt jc mt lz kx jg mu mb lb jk mv md lf me dt translated">延伸阅读:</h2><p id="206b" class="pw-post-body-paragraph ir is hu it b iu lh iw ix iy li ja jb jc lj je jf jg lk ji jj jk ll jm jn jo hn dt translated">流形假设[1]:【https://www.ima.umn.edu/2008-2009/SW10.27-30.08/6687】T2</p><p id="d7ff" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">深度学习<a class="ae jp" href="https://hackernoon.com/tagged/book" rel="noopener ugc nofollow" target="_blank">书</a>【1】:<a class="ae jp" href="http://www.deeplearningbook.org/version-2015-10-03/contents/manifolds.html" rel="noopener ugc nofollow" target="_blank">http://www . deep Learning Book . org/version-2015-10-03/contents/manifolds . html</a></p><p id="318c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">[1] &amp; [2]帕萨·尼代木的演讲:<a class="ae jp" href="https://www.ima.umn.edu/2008-2009/SW10.27-30.08/6687" rel="noopener ugc nofollow" target="_blank">https://www.ima.umn.edu/2008-2009/SW10.27-30.08/6687</a></p><p id="a0aa" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">维基百科[2]:<a class="ae jp" href="https://en.wikipedia.org/wiki/Neural_coding#Sparse_coding" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Neural_coding#Sparse_coding</a></p><p id="4ebb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">关于稀疏编码的文章[2]:<a class="ae jp" href="http://redwood.berkeley.edu/vs265/handout-sparse-08.pdf" rel="noopener ugc nofollow" target="_blank">http://redwood.berkeley.edu/vs265/handout-sparse-08.pdf</a></p><p id="2174" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="kf">受此reddit帖子启发:</em><a class="ae jp" href="https://goo.gl/jJFtP1" rel="noopener ugc nofollow" target="_blank"><em class="kf">https://goo.gl/jJFtP1</em></a></p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="mw mx l"/></div></figure></div></div>    
</body>
</html>