<html>
<head>
<title>TensorFlow in a Nutshell — Part Three: All the Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流概述——第三部分:所有模型</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930?source=collection_archive---------2-----------------------#2016-10-03">https://medium.com/hackernoon/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930?source=collection_archive---------2-----------------------#2016-10-03</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/057a6633cbeaafd2515e8e44108d8bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUd-POvCacwkb4QfztoMdQ.png"/></div></div></figure><h2 id="de3e" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">世界上最流行的深度学习框架的快速简单指南。</h2><p id="5659" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated">请务必在此查看其他文章<a class="ae kv" href="http://camron.xyz" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="6a97" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">概观</h2><p id="48c5" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated">在这一期中，我们将回顾TensorFlow中当前可用的所有抽象模型，并描述该特定模型的用例以及简单的示例代码。工作示例的完整来源在<a class="ae kv" href="https://github.com/c0cky/TensorFlow-in-a-Nutshell" rel="noopener ugc nofollow" target="_blank">tensor flow In a null repo</a>中。</p></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff ld"><img src="../Images/5d9de87b3b2b3da5910c689d49ac6f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*lQ4izz9ZbhKYD8NClZpsmQ.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">A recurrent neural network</figcaption></figure><h2 id="9b2e" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">递归神经网络</h2><p id="178e" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>语言建模、机器翻译、单词嵌入、文本处理。</p><p id="aa2c" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">自从长短期记忆和门控递归单元出现以来，递归神经网络在自然语言处理中取得了超越其他模型的飞跃。它们可以被输入代表字符的向量，并被训练以基于训练集生成新的句子。这个模型的优点是它保留了句子的上下文，并推导出“猫坐在垫子上”意味着猫在垫子上的意思。自从张量流写作的出现，这些网络变得越来越简单。Denny Britz <a class="ae kv" href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/" rel="noopener ugc nofollow" target="_blank">在这里</a>甚至提到了一些隐藏的特性，这些特性使得编写RNN的代码更加简单。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="ba4b" class="jc jd hu lt b fv lx ly l lz ma">import tensorflow as tf<br/>import numpy as np</span><span id="3238" class="jc jd hu lt b fv mb ly l lz ma"><em class="lm"># Create input data</em><br/>X = np.random.randn(2, 10, 8)<br/><br/><em class="lm"># The second example is of length 6 </em><br/>X[1,6,:] = 0<br/>X_lengths = [10, 6]<br/><br/>cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=<strong class="lt hv">True</strong>)<br/>cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)<br/>cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=<strong class="lt hv">True</strong>)<br/><br/>outputs, last_states = tf.nn.dynamic_rnn(<br/>    cell=cell,<br/>    dtype=tf.float64,<br/>    sequence_length=X_lengths,<br/>    inputs=X)<br/><br/>result = tf.contrib.learn.run_n(<br/>    {"outputs": outputs, "last_states": last_states},<br/>    n=1,<br/>    feed_dict=<strong class="lt hv">None</strong>)</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mc"><img src="../Images/3ca4813594c9fb7532f82d7b81d8abb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4h1SgwbWNmtrRhszM9EJg.png"/></div></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Convolution Neural Network</figcaption></figure><h2 id="e266" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">卷积神经网络</h2><p id="1f7f" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">使用案例:</em> </strong>图像处理、面部识别、计算机视觉</p><p id="13d8" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">卷积神经网络是独特的，因为它们在创建时就考虑到了输入将是图像。CNN对矩阵执行滑动窗口功能。该窗口被称为内核，它在图像上滑动，创建一个卷积特征。</p><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff md"><img src="../Images/0e7de8a67e0c197106c87d417e45b499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*ZCjPUFrB6eHPRi4eyP6aaA.gif"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">from <a class="ae kv" href="http://deeplearning.standford.edu/wiki/index.php/Feature_extraction_using_convolution" rel="noopener ugc nofollow" target="_blank">http://deeplearning.standford.edu/wiki/index.php/Feature_extraction_using_convolution</a></figcaption></figure><p id="1423" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">创建一个卷积特征允许边缘检测，然后允许一个<a class="ae kv" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>从图片中描绘物体。</p><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff me"><img src="../Images/9888a548a44c4ab77dbe0977deb55625.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*3H4Ho1lX_saXzqK243Ic9w.jpeg"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">edge detection from GIMP manual</figcaption></figure><p id="ba35" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">创建该矩阵的卷积特征如下图所示:</p><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/bbc233f1b74cf39a0eea7627c8e54cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*h5XnUMUF7XcmTCFrU5pTeQ.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Convolved feature from GIMP manual</figcaption></figure><p id="deea" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">下面是从MNIST数据集中识别手写数字的代码示例。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="a1f0" class="jc jd hu lt b fv lx ly l lz ma">### Convolutional network<br/>def max_pool_2x2(tensor_in):<br/>  return tf.nn.max_pool(<br/>      tensor_in, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')</span><span id="147f" class="jc jd hu lt b fv mb ly l lz ma">def conv_model(X, y):<br/>  # reshape X to 4d tensor with 2nd and 3rd dimensions being image width and<br/>  # height final dimension being the number of color channels.<br/>  X = tf.reshape(X, [-1, 28, 28, 1])</span><span id="93c3" class="jc jd hu lt b fv mb ly l lz ma">  # first conv layer will compute 32 features for each 5x5 patch<br/>  with tf.variable_scope('conv_layer1'):<br/>    h_conv1 = learn.ops.conv2d(X, n_filters=32, filter_shape=[5, 5],<br/>                               bias=True, activation=tf.nn.relu)<br/>    h_pool1 = max_pool_2x2(h_conv1)</span><span id="f120" class="jc jd hu lt b fv mb ly l lz ma">  # second conv layer will compute 64 features for each 5x5 patch.<br/>  with tf.variable_scope('conv_layer2'):<br/>    h_conv2 = learn.ops.conv2d(h_pool1, n_filters=64, filter_shape=[5, 5],<br/>                               bias=True, activation=tf.nn.relu)<br/>    h_pool2 = max_pool_2x2(h_conv2)</span><span id="64fb" class="jc jd hu lt b fv mb ly l lz ma">    # reshape tensor into a batch of vectors<br/>    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])</span><span id="367c" class="jc jd hu lt b fv mb ly l lz ma">  # densely connected layer with 1024 neurons.<br/>  h_fc1 = learn.ops.dnn(<br/>      h_pool2_flat, [1024], activation=tf.nn.relu, dropout=0.5)<br/>  return learn.models.logistic_regression(h_fc1, y)</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mg"><img src="../Images/50f718c093b4299c9bc75cf4e62b71dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toBL6XleRkwABSwTAFaY_g.png"/></div></div></figure><h2 id="f2cc" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">前馈神经网络</h2><p id="d164" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>分类和回归</p><p id="ae8e" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">这些网络由多层感知器组成，接受输入，将信息传递给下一层。网络的最后一层产生输出。给定层中的每个节点之间没有连接。没有原始输入和最终输出的层称为隐藏层。</p><p id="6558" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">该网络的目标类似于使用反向传播的其他监督神经网络，以使输入具有期望的训练输出。这些是用于分类和回归问题的一些最简单有效的神经网络。我们将展示创建一个前馈网络来分类手写数字是多么容易:</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="cb46" class="jc jd hu lt b fv lx ly l lz ma">def init_weights(shape):<br/>    return tf.Variable(tf.random_normal(shape, stddev=0.01))</span><span id="5192" class="jc jd hu lt b fv mb ly l lz ma">def model(X, w_h, w_o):<br/>    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions<br/>    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us</span><span id="3a3e" class="jc jd hu lt b fv mb ly l lz ma">mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)<br/>trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels</span><span id="7257" class="jc jd hu lt b fv mb ly l lz ma">X = tf.placeholder("float", [None, 784])<br/>Y = tf.placeholder("float", [None, 10])</span><span id="cbb0" class="jc jd hu lt b fv mb ly l lz ma">w_h = init_weights([784, 625]) # create symbolic variables<br/>w_o = init_weights([625, 10])</span><span id="e1fd" class="jc jd hu lt b fv mb ly l lz ma">py_x = model(X, w_h, w_o)</span><span id="358a" class="jc jd hu lt b fv mb ly l lz ma">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) # compute costs<br/>train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer<br/>predict_op = tf.argmax(py_x, 1)</span><span id="65e1" class="jc jd hu lt b fv mb ly l lz ma"># Launch the graph in a session<br/>with tf.Session() as sess:<br/>    # you need to initialize all variables<br/>    tf.initialize_all_variables().run()</span><span id="46ed" class="jc jd hu lt b fv mb ly l lz ma">for i in range(100):<br/>        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):<br/>            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})<br/>        print(i, np.mean(np.argmax(teY, axis=1) ==<br/>                         sess.run(predict_op, feed_dict={X: teX, Y: teY})))</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mh"><img src="../Images/b4a8f5ecb221317e762494c70dfa0466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sy_6ipmBh_21KD0_dds1HQ.png"/></div></div></figure><h2 id="e783" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">线性模型</h2><p id="20f3" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>分类和回归</p><p id="72b1" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">线性模型采用X值并生成一条最佳拟合线，用于Y值的分类和回归。例如，如果您有一个住宅区的房子大小及其价格的列表，您可以使用线性模型预测给定大小的房子的价格。</p><p id="d053" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">需要注意的一点是，线性模型可以用于多个X特征。例如，在住房示例中，我们可以创建一个线性模型，给出房屋大小、房间数量、浴室数量和价格，并预测给定房屋大小、房间数量和浴室数量的价格。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="2716" class="jc jd hu lt b fv lx ly l lz ma">import numpy as np<br/>import tensorflow as tf</span><span id="c560" class="jc jd hu lt b fv mb ly l lz ma">def weight_variable(shape):<br/>    initial = tf.truncated_normal(shape, stddev=1)<br/>    return tf.Variable(initial)</span><span id="9bb2" class="jc jd hu lt b fv mb ly l lz ma"># dataset<br/>xx = np.random.randint(0,1000,[1000,3])/1000.<br/>yy = xx[:,0] * 2 + xx[:,1] * 1.4 + xx[:,2] * 3</span><span id="ce4d" class="jc jd hu lt b fv mb ly l lz ma"># model<br/>x = tf.placeholder(tf.float32, shape=[None, 3])<br/>y_ = tf.placeholder(tf.float32, shape=[None])<br/>W1 = weight_variable([3, 1])<br/>y = tf.matmul(x, W1)</span><span id="ed3e" class="jc jd hu lt b fv mb ly l lz ma"># training and cost function<br/>cost_function = tf.reduce_mean(tf.square(tf.squeeze(y) - y_))<br/>train_function = tf.train.AdamOptimizer(1e-2).minimize(cost_function)</span><span id="f116" class="jc jd hu lt b fv mb ly l lz ma"># create a session<br/>sess = tf.Session()</span><span id="654a" class="jc jd hu lt b fv mb ly l lz ma"># train<br/>sess.run(tf.initialize_all_variables())<br/>for i in range(10000):<br/>    sess.run(train_function, feed_dict={x:xx, y_:yy})<br/>    if i % 1000 == 0:<br/>        print(sess.run(cost_function, feed_dict={x:xx, y_:yy}))</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mi"><img src="../Images/e50e894eb70be27842c4a8d01e938739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwNZplJ1p-xnUKRQPMS6Aw.png"/></div></div></figure><h2 id="2f96" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">支持向量机</h2><p id="934d" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>目前仅二进制分类</p><p id="176b" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">SVM背后的一般思想是，对于线性可分模式，存在一个最优超平面。对于不可线性分离的数据，我们可以使用核函数将原始数据转换到一个新的空间。支持向量机最大化分离超平面的边界。它们在高维空间中非常有效，如果维数大于样本数，它们仍然有效。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="831f" class="jc jd hu lt b fv lx ly l lz ma">def input_fn():<br/>      return {<br/>          'example_id': tf.constant(['1', '2', '3']),<br/>          'price': tf.constant([[0.6], [0.8], [0.3]]),<br/>          'sq_footage': tf.constant([[900.0], [700.0], [600.0]]),<br/>          'country': tf.SparseTensor(<br/>              values=['IT', 'US', 'GB'],<br/>              indices=[[0, 0], [1, 3], [2, 1]],<br/>              shape=[3, 5]),<br/>          'weights': tf.constant([[3.0], [1.0], [1.0]])<br/>      }, tf.constant([[1], [0], [1]])</span><span id="1ed1" class="jc jd hu lt b fv mb ly l lz ma">price = tf.contrib.layers.real_valued_column('price')<br/>    sq_footage_bucket = tf.contrib.layers.bucketized_column(<br/>        tf.contrib.layers.real_valued_column('sq_footage'),<br/>        boundaries=[650.0, 800.0])<br/>    country = tf.contrib.layers.sparse_column_with_hash_bucket(<br/>        'country', hash_bucket_size=5)<br/>    sq_footage_country = tf.contrib.layers.crossed_column(<br/>        [sq_footage_bucket, country], hash_bucket_size=10)<br/>    svm_classifier = tf.contrib.learn.SVM(<br/>        feature_columns=[price, sq_footage_bucket, country, sq_footage_country],<br/>        example_id_column='example_id',<br/>        weight_column_name='weights',<br/>        l1_regularization=0.1,<br/>        l2_regularization=1.0)</span><span id="a6c8" class="jc jd hu lt b fv mb ly l lz ma">svm_classifier.fit(input_fn=input_fn, steps=30)<br/>    accuracy = svm_classifier.evaluate(input_fn=input_fn, steps=1)['accuracy']</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/3376726e9fcd9940b542bbdae49e5d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*EaDupDnQB1QYL6MPvr3w_Q.png"/></div></figure><h2 id="fc81" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">深度和宽度模型</h2><p id="5b58" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>推荐系统，分类和回归</p><p id="a7bb" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">深度和宽度模型在第二部分的<a class="ae kv" rel="noopener" href="/@camrongodbout/tensorflow-in-a-nutshell-part-two-hybrid-learning-98c121d35392#.oubizxp18">中有更详细的介绍，所以我们在这里不会讲太多。宽而深的网络将线性模型与前馈神经网络相结合，这样我们的预测将具有记忆性和泛化能力。这种类型的模型可用于分类和回归问题。这允许具有相对准确预测的较少特征工程。因此，两全其美。下面是来自第二部分github </a>的代码片段。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="b9b8" class="jc jd hu lt b fv lx ly l lz ma">def input_fn(df, train=False):<br/>  """Input builder function."""<br/>  # Creates a dictionary mapping from each continuous feature column name (k) to<br/>  # the values of that column stored in a constant Tensor.<br/>  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}<br/>  # Creates a dictionary mapping from each categorical feature column name (k)<br/>  # to the values of that column stored in a tf.SparseTensor.<br/>  categorical_cols = {k: tf.SparseTensor(<br/>    indices=[[i, 0] for i in range(df[k].size)],<br/>    values=df[k].values,<br/>    shape=[df[k].size, 1])<br/>                      for k in CATEGORICAL_COLUMNS}<br/>  # Merges the two dictionaries into one.<br/>  feature_cols = dict(continuous_cols)<br/>  feature_cols.update(categorical_cols)<br/>  # Converts the label column into a constant Tensor.<br/>  if train:<br/>    label = tf.constant(df[SURVIVED_COLUMN].values)<br/>      # Returns the feature columns and the label.<br/>    return feature_cols, label<br/>  else:<br/>    return feature_cols</span><span id="44a4" class="jc jd hu lt b fv mb ly l lz ma">m = build_estimator(model_dir)<br/>m.fit(input_fn=lambda: input_fn(df_train, True), steps=200)<br/>print m.predict(input_fn=lambda: input_fn(df_test))<br/>results = m.evaluate(input_fn=lambda: input_fn(df_train, True), steps=1)<br/>for key in sorted(results):<br/>  print("%s: %s" % (key, results[key]))</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/952e262b5c8d7b9dc61bf431c8d0e6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*breo9La3b-US5oG4KUuZqw.png"/></div></figure><h2 id="f0e6" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">随机森林</h2><p id="7599" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>分类和回归</p><p id="f20e" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">随机森林模型采用许多不同的分类树，每棵树为该类别投票。森林选择投票最多的分类。</p><p id="d2f5" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">随机森林不会过度适应，你可以跑尽可能多的树，这是相对较快的。用下面的片段试试虹膜数据:</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="bcc6" class="jc jd hu lt b fv lx ly l lz ma">hparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(<br/>        num_trees=3, max_nodes=1000, num_classes=3, num_features=4)<br/>classifier = tf.contrib.learn.TensorForestEstimator(hparams)</span><span id="8ee0" class="jc jd hu lt b fv mb ly l lz ma">iris = tf.contrib.learn.datasets.load_iris()<br/>data = iris.data.astype(np.float32)<br/>target = iris.target.astype(np.float32)</span><span id="aeab" class="jc jd hu lt b fv mb ly l lz ma">monitors = [tf.contrib.learn.TensorForestLossMonitor(10, 10)]<br/>classifier.fit(x=data, y=target, steps=100, monitors=monitors)<br/>classifier.evaluate(x=data, y=target, steps=10)</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ml"><img src="../Images/52f05dae615ca2e48c89c55ef22d58b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yu7chokfJ6Ufut79peUNlw.png"/></div></div></figure><h2 id="7b09" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">贝叶斯强化学习</h2><p id="19de" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>分类和回归</p><p id="85ac" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">在TensorFlow的contrib文件夹中有一个名为BayesFlow的库。除了一个增强算法的例子，BayesFlow没有任何文档。这个算法是由罗纳德·威廉姆斯在一篇论文中提出的。</p><blockquote class="mm mn mo"><p id="a45f" class="ka kb lm kc b kd ln kf kg kh lo kj kk mp lp km kn mq lq kp kq mr lr ks kt ku hn dt translated"><strong class="kc hv">RE</strong>ward<strong class="kc hv">I</strong>increment =<strong class="kc hv">N</strong>on negative<strong class="kc hv">F</strong>actor *<strong class="kc hv">O</strong>ffset<strong class="kc hv">R</strong>enforcement *<strong class="kc hv">C</strong>character istic<strong class="kc hv">E</strong>责任</p></blockquote><p id="fd9e" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">这个网络试图解决一个即时强化<a class="ae kv" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>任务，在每次试验获得强化值后调整权重。在每次试验结束时，每个权重增加一个学习率因子乘以强化值减去基线乘以特征合格性。Williams的论文还讨论了使用反向传播来训练增强网络。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="f1d2" class="jc jd hu lt b fv lx ly l lz ma">"""Build the Split-Apply-Merge Model.<br/>  Route each value of input [-1, -1, 1, 1] through one of the<br/>  functions, plus_1, minus_1.  The decision for routing is made by<br/>  4 Bernoulli R.V.s whose parameters are determined by a neural network<br/>  applied to the input.  REINFORCE is used to update the NN parameters.<br/>  Returns:<br/>    The 3-tuple (route_selection, routing_loss, final_loss), where:<br/>      - route_selection is an int 4-vector<br/>      - routing_loss is a float 4-vector<br/>      - final_loss is a float scalar.<br/>  """<br/>  inputs = tf.constant([[-1.0], [-1.0], [1.0], [1.0]])<br/>  targets = tf.constant([[0.0], [0.0], [0.0], [0.0]])<br/>  paths = [plus_1, minus_1]<br/>  weights = tf.get_variable("w", [1, 2])<br/>  bias = tf.get_variable("b", [1, 1])<br/>  logits = tf.matmul(inputs, weights) + bias</span><span id="6c16" class="jc jd hu lt b fv mb ly l lz ma"># REINFORCE forward step<br/>  route_selection = st.StochasticTensor(<br/>      distributions.Categorical, logits=logits)</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><figure class="le lf lg lh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ms"><img src="../Images/01c36d8f7c61f8bf386bdd9e0b8b9c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1MHiieXwdKo75p-_4VkFnQ.png"/></div></div></figure><h2 id="c89b" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">线性链条件随机场</h2><p id="48c6" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated"><strong class="kc hv"> <em class="lm">用例:</em> </strong>时序数据</p><p id="553a" class="pw-post-body-paragraph ka kb hu kc b kd ln kf kg kh lo kj kk jn lp km kn jr lq kp kq jv lr ks kt ku hn dt translated">CRF是根据一个无向模型进行因子化的条件概率分布。它们预测单个样本的标签，保持来自相邻样本的上下文。CRF类似于隐马尔可夫模型。CRF通常用于图像分割和对象识别，以及浅层解析、命名实体识别和基因查找。</p><pre class="le lf lg lh fq ls lt lu lv aw lw dt"><span id="e0f3" class="jc jd hu lt b fv lx ly l lz ma"># Train for a fixed number of iterations.<br/>session.run(tf.initialize_all_variables())<br/>  for i in range(1000):<br/>    tf_unary_scores, tf_transition_params, _ = session.run(<br/>       [unary_scores, transition_params, train_op])<br/>    if i % 100 == 0:<br/>      correct_labels = 0<br/>      total_labels = 0<br/>      for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y, sequence_lengths):<br/>        # Remove padding from the scores and tag sequence.<br/>        tf_unary_scores_ = tf_unary_scores_[:sequence_length_]<br/>        y_ = y_[:sequence_length_]<br/><br/>        # Compute the highest scoring sequence.<br/>        viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(<br/>            tf_unary_scores_, tf_transition_params)<br/><br/>        # Evaluate word-level accuracy.<br/>        correct_labels += np.sum(np.equal(viterbi_sequence, y_))<br/>        total_labels += sequence_length_<br/>      accuracy = 100.0 * correct_labels / float(total_labels)<br/>      print("Accuracy: %.2f%%" % accuracy)</span></pre></div><div class="ab cl kw kx hc ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hn ho hp hq hr"><h2 id="e6fa" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">结论</h2><p id="95ff" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku hn dt translated">自从TensorFlow发布以来，围绕该项目的社区一直在添加更多的包、示例和案例来使用这个令人惊叹的库。甚至在撰写本文时，还有更多的模型和示例代码正在编写中。看到张量流在过去的几个月里增长了这么多是令人惊讶的。随着时间的推移，软件包的易用性和多样性正在增加，而且似乎不会很快放缓。</p><blockquote class="mt"><p id="4d4d" class="mu mv hu bd mw mx my mz na nb nc ku ek translated">一如既往——如有任何问题或咨询，请随时发送电子邮件至camron@camron.xyz</p></blockquote><blockquote class="mm mn mo"><p id="8a50" class="ka kb lm kc b kd nd kf kg kh ne kj kk mp nf km kn mq ng kp kq mr nh ks kt ku hn dt translated">最初发布于<a class="ae kv" href="http://camron.xyz" rel="noopener ugc nofollow" target="_blank"> Camron.xyz </a></p></blockquote><div class="le lf lg lh fq ab cb"><figure class="ni iv nj nk nl nm nn paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="ni iv nj nk nl nm nn paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="ni iv nj nk nl nm nn paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="mm mn mo"><p id="f922" class="ka kb lm kc b kd ln kf kg kh lo kj kk mp lp km kn mq lq kp kq mr lr ks kt ku hn dt translated"><a class="ae kv" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是这个家庭的一员。我们现在<a class="ae kv" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>并乐意<a class="ae kv" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="ka kb lm kc b kd ln kf kg kh lo kj kk mp lp km kn mq lq kp kq mr lr ks kt ku hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae kv" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae kv" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="le lf lg lh fq iv fe ff paragraph-image"><a href="https://goo.gl/Ahtev1"><div class="fe ff no"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></a></figure><figure class="le lf lg lh fq iv"><div class="bz el l di"><div class="np nq l"/></div></figure></div></div>    
</body>
</html>