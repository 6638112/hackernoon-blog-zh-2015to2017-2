<html>
<head>
<title>Word2Vec (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/word2vec-part-1-fe2ec6514d70?source=collection_archive---------0-----------------------#2016-10-15">https://medium.com/hackernoon/word2vec-part-1-fe2ec6514d70?source=collection_archive---------0-----------------------#2016-10-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/5b5ecf6591f599afa1605b8bb9a86f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0roG7_LinNzvVm7Bfb8tkA.jpeg"/></div></div></figure><blockquote class="jc"><p id="bf37" class="jd je hu bd jf jg jh ji jj jk jl jm ek translated">Word2Vec自然语言处理的类固醇</p></blockquote><p id="62cb" class="pw-post-body-paragraph jn jo hu jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj jm hn dt translated">让我们从基础开始。</p><h1 id="d35a" class="kk kl hu bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">词向量</h1><p id="07ec" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated"><strong class="jp hv">问)什么是词向量？</strong></p><p id="8dd8" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"> Ans) </strong>用数字表示文字。</p><p id="c0de" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">问)为什么用词向量？</strong></p><p id="3e4f" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"> Ans) </strong>我总结一下三个主要原因:</p><p id="2fce" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">1.计算机不能对字符串进行计算。</p><p id="605b" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">2.字符串本身并不包含太多明确的信息。</p><p id="c9a4" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">3.单词向量通常是密集向量表示。</p><p id="c36d" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">问)那么什么是显性信息呢？</strong></p><p id="0e10" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"> Ans) </strong>是的，这个词本身并没有说太多它在现实生活中代表了什么。示例:</p><p id="dc8f" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">字符串“cat”只是告诉我们它有三个字母“c”、“a”和“t”。</p><p id="b05d" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">它没有关于它所代表的动物、数量或使用它的上下文的信息。</p><p id="cb6f" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">问)密集矢量表示？</strong></p><p id="7184" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"> Ans) </strong>简而言之(就目前而言)，相对于它们的大小，这些向量可以容纳巨大的信息。</p><p id="38a1" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">问)词向量的类型？</strong></p><p id="9131" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">a)有两个主要类别:</p><ul class=""><li id="32e6" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated"><strong class="jp hv">基于频率的</strong>:使用stats来计算一个单词与其相邻单词共现的概率。</li><li id="be4b" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated"><strong class="jp hv">基于预测的</strong>:使用预测分析对与其相邻单词共现的单词进行加权猜测。</li></ul><p id="de43" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">预测有两种类型:</p><ul class=""><li id="4a3c" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated"><strong class="jp hv">语义</strong>:试着根据上下文猜测一个单词</li><li id="6323" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated"><strong class="jp hv">句法</strong>:根据文本的句法猜一个单词</li></ul><p id="f648" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">问)基于语法和上下文的向量之间的区别？</strong></p><p id="b900" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"> Ans) </strong>我们来看一个例子:</p><p id="16ad" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">考虑下面的句子“牛顿不喜欢苹果”</p><ul class=""><li id="bd36" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated">语义向量关心的是‘<strong class="jp hv">一个文本是关于谁或什么的实体？</strong>’。在这个例子中是“牛顿”和“苹果”</li><li id="f099" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">句法向量关注的是'<strong class="jp hv">那些实体是怎么说的？</strong>’。在这种情况下“不”和“喜欢”</li></ul></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="fd80" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">Word2Vec</h1><p id="ed91" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated"><em class="mt">是最广泛使用的词向量表示形式之一。最早由谷歌杜撰于</em> <a class="ae ls" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mt">米科洛夫等人</em> </a> <em class="mt">。</em></p><p id="572b" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">它有两种变体:</p><ol class=""><li id="ec07" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm mu lz ma mb dt translated"><strong class="jp hv"> CBOW(连续单词包)</strong>:这个模型试图根据一个单词的邻居来预测这个单词。</li></ol><p id="4b29" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">2.<strong class="jp hv"> SkipGram </strong> : <strong class="jp hv"> </strong>这个模型试图预测一个单词的邻居。</p><blockquote class="mv mw mx"><p id="8b8a" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated">从统计上看，CBOW平滑了大量分布信息(通过将整个上下文视为一个观察)。在很大程度上，这对于较小的数据集来说是很有用的。然而，skip-gram将每个上下文-目标对视为一个新的观察，当我们有更大的数据集时，这往往会做得更好。</p><p id="24b4" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated">-张量流</p></blockquote><p id="37f7" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">用更简单的话来说，<strong class="jp hv"> CBOW </strong>倾向于找到一个单词在一个邻域(上下文)中出现的概率。所以<em class="mt">概括了一个词可以被使用的所有不同的语境</em>。</p><p id="2608" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">而<strong class="jp hv">skip program</strong>倾向于<em class="mt">分别学习不同的上下文</em>。所以SkipGram需要足够的数据。因此<a class="ae ls" href="https://hackernoon.com/tagged/skipgram" rel="noopener ugc nofollow" target="_blank"> SkipGram </a>需要更多的数据来训练，并且SkipGram(给定足够的数据)包含更多关于上下文的知识。</p><p id="4917" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">注意</strong>:这些技术不需要带标签的数据集(尽管带标签的数据集可以用来包含额外的信息，我们将在后面看到)。因此，任何大型文本语料库实际上都是一个数据集。因为要预测的标签是文本中已经存在的单词。</p><p id="ac41" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">我们将关注SkipGram作为足够大的数据集(维基百科，Reddit，Stackoverflow等。)可供下载。</p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="f3ce" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">SkipGram</h1><p id="d122" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">首先，我们根据什么将是我们的<strong class="jp hv">目标单词</strong>(待预测)、什么将是<strong class="jp hv">源单词</strong>(在此基础上我们进行预测)以及<strong class="jp hv">我们寻找上下文有多远</strong>(窗口的大小)来决定我们要寻找什么上下文。</p><p id="01e5" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">示例:</p><p id="d407" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><em class="mt">考虑窗口大小为3 </em></p><h2 id="3de0" class="nb kl hu bd km nc nd ne kq nf ng nh ku jy ni nj ky kc nk nl lc kg nm nn lg no dt translated">类型1</h2><p id="6f0d" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">将中间的单词视为源单词。下一个和上一个单词作为目标单词。</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff np"><img src="../Images/2518b31d817b1cac8bbe03ec0d51e5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IqB2yIU3GymmkFwoG-I23Q.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 1</figcaption></figure><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ny"><img src="../Images/5706f35a4eb705a46a6b347d845fc4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJXCWnLjWrYz6m45EBxtYw.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 2</figcaption></figure><h2 id="a43c" class="nb kl hu bd km nc nd ne kq nf ng nh ku jy ni nj ky kc nk nl lc kg nm nn lg no dt translated">类型2</h2><p id="016e" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">将第一个单词视为源单词。下面两个词作为目标词。</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nz"><img src="../Images/380dac1bef716782eeebccd2e8957c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F25MNJGsg3MeYOAbhMWXgw.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 3</figcaption></figure><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oa"><img src="../Images/0f922ef7c761e1388de6b3723dcda850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwZrTEzE_T_mp88qSZutAw.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 4</figcaption></figure><p id="063c" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">在这两种类型中，源单词被与该源单词的上下文相关的单词所包围。像‘梅西’一般会被与‘足球’相关的词语包围。因此，在看到一些例子后，“梅西”的词向量将开始结合与“足球”、“进球”、“比赛”等相关的上下文。</p><p id="6732" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">在“苹果”的情况下，它的词向量会做同样的事情，但是对于公司和水果(见图6)。</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ob"><img src="../Images/1cca7ee101e982518c0a236d5d2c6829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-86sti2FBnAs895WpXzsw.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 5 Word2Vec’s Neural Network</figcaption></figure><p id="4d8b" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">W1(s)和W2(s)包含关于单词的信息。W1和W2中的信息被组合/平均以获得Word2Vec表示。</p><p id="2322" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">假设W(s)的大小是400，那么“苹果”的Word2Vec表示看起来就像这样</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="8f32" class="nb kl hu od b fv oh oi l oj ok">array([<br/>-2.56660223e-01,  -7.96796158e-02,  -2.04517767e-02,</span><span id="0bd3" class="nb kl hu od b fv ol oi l oj ok">-7.34366626e-02,   3.26843783e-02,  -1.90244913e-02,</span><span id="87e2" class="nb kl hu od b fv ol oi l oj ok">7.93217495e-02,    4.07200940e-02,  -1.74737453e-01,</span><span id="dd5b" class="nb kl hu od b fv ol oi l oj ok">.....</span><span id="d55a" class="nb kl hu od b fv ol oi l oj ok">1.86899990e-01,    -4.33036387e-02,  -2.66942739e-01,</span><span id="334b" class="nb kl hu od b fv ol oi l oj ok">-1.00671440e-01],   dtype=float32)</span></pre><p id="1964" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">现在像“苹果落在牛顿身上”这样包含4个单词的简单句，借助Word2Vec，可以转换成4*400 (1600)个数字；<em class="mt">每个</em><strong class="jp hv"><em class="mt">【1】</em></strong><em class="mt">包含明确的信息</em>。所以现在我们也知道这篇课文在谈论一个人，科学，水果等等。</p><p id="97c0" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv"><em class="mt">【1】</em></strong><em class="mt">:因此是密集矢量表示</em></p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="310f" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">(英)可视化(= visualization)</h1><p id="f6a8" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">直接可视化Word2Vec目前对人类来说是不可能的(因为像400这样的高维度)。相反，我们使用<em class="mt">降维</em>技术，如<a class="ae ls" href="http://en.wikipedia.org/wiki/Multidimensional_scaling" rel="noopener ugc nofollow" target="_blank">多维标度</a>、<a class="ae ls" href="https://en.wikipedia.org/wiki/Sammon_mapping" rel="noopener ugc nofollow" target="_blank"> sammon映射</a>、<a class="ae ls" href="http://en.wikipedia.org/wiki/Nearest_neighbor_graph" rel="noopener ugc nofollow" target="_blank">最近邻图</a>等。</p><p id="08c1" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">最广泛的算法是<a class="ae ls" href="http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank"> t分布随机邻居嵌入</a> (t-SNE)。克里斯托弗·奥拉有一个关于降维的惊人博客。</p><p id="ace6" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">t-SNE在Word2Vec上的最终结果类似于</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff om"><img src="../Images/c74fa5eb9792f4972bf827372c788b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gb46k5aVUvaiUjLE_Zrj3Q.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 6 Multiple contexts of Apple</figcaption></figure><p id="6e41" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">这个图说明苹果介于公司(IBM，微软)和水果(芒果)之间。</p><p id="b69d" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">这是因为苹果的Word2Vec表示包含了苹果公司和苹果公司的信息。</p><p id="489a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">间距</p><ul class=""><li id="a4fa" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated">苹果和芒果:0.505</li><li id="2465" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">苹果和IBM : 0.554</li><li id="d355" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">芒果和IBM : 0.902</li></ul><p id="f1f5" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">和</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff on"><img src="../Images/2e138d1c754e34f84696b280cfe5063f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCyt-jsDj6hD-OKjI2e5iA.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 7 Combining contexts of multiple words</figcaption></figure><p id="c09a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">该图显示，通过组合两个向量“State”和“America”的方向，合成向量“Dakota”与原始向量相关。</p><p id="fc0a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">如此有效</p><ul class=""><li id="297f" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated">州+美国=达科他州</li><li id="7475" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">州+德国=巴伐利亚</li></ul><p id="bc80" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">其他例子有:</p><ul class=""><li id="cae6" class="lt lu hu jp b jq ln ju lo jy lv kc lw kg lx jm ly lz ma mb dt translated">德国+航空公司=汉莎航空</li><li id="fe6d" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">国王+女人——男人=王后</li></ul></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="a54e" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">履行</h1><p id="adbf" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">Gensim和Tensorflow都有令人印象深刻的Word2Vec实现。</p><p id="e3f2" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">这是<a class="ae ls" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>实现的优秀<a class="ae ls" href="http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim" rel="noopener ugc nofollow" target="_blank">博客</a>和<a class="ae ls" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>有<a class="ae ls" href="https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html" rel="noopener ugc nofollow" target="_blank">教程</a>。</p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="440d" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">问题</h1><p id="3641" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">默认情况下，Word2Vec模型每个单词有一个表示。向量可以尝试累积所有的上下文，但这最终至少在某种程度上概括了所有的上下文，因此每个上下文的精度都受到损害。这对于具有非常不同的上下文的单词来说尤其是一个问题。这可能会导致一个情境，过度驱动其他情境。</p><p id="8d79" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">比如:“苹果”公司和“苹果”水果只有一个单词2Vec表示。</p><p id="90b2" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">例一:</strong></p><p id="5dee" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">“少女”可用于女子、乐队(铁娘子)、体育等。</p><p id="9822" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">当你试图找到与“少女”最相似的词时</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="228b" class="nb kl hu od b fv oh oi l oj ok">[(u'odi_debut', 0.43079674243927),<br/> (u'racecourse_debut', 0.42960068583488464),<br/> .....<br/> (u'marathon_debut', 0.40903717279434204),<br/> (u'one_day_debut', 0.40729495882987976),<br/> (u'test_match_debut', 0.4013477563858032)]</span></pre><p id="5860" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><em class="mt">显而易见，与“运动”相关的语境已经压倒了其他。</em></p><p id="bb1a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">即使把“铁”和“少女”结合在一起，也不能解决问题，因为现在“铁”的语境压倒了一切。</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="2606" class="nb kl hu od b fv oh oi l oj ok">[(u'steel', 0.5581518411636353),<br/> (u'copper', 0.5266575217247009),<br/> .....<br/> (u'bar_iron', 0.49549400806427)]</span></pre><p id="51cc" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">例2 </strong></p><p id="e4c9" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">一个词可以用作动词和名词，但意思完全不同。比如‘铁’这个词。作为动词，它通常用于用电熨斗使东西光滑，但在名词中，它主要用于表示金属。</p><p id="544f" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">当我们找到“铁”的最近邻居时</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="43e0" class="nb kl hu od b fv oh oi l oj ok">[(u'steel', 0.5581518411636353),<br/> (u'copper', 0.5266575217247009),<br/> .....<br/> (u'bar_iron', 0.49549400806427)]</span></pre><p id="2a4a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><em class="mt">动词对应物的引用可以忽略不计。</em></p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="70e6" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">变体</h1><h2 id="3913" class="nb kl hu bd km nc nd ne kq nf ng nh ku jy ni nj ky kc nk nl lc kg nm nn lg no dt translated">变体1:基于复合名词的Word2Vec</h2><p id="1777" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated">通过在训练集中用复合名词(如‘iron _ maiden’)替换名词(如‘iron _ maiden’)。</p><blockquote class="mv mw mx"><p id="9ade" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated"><em class="hu">《铁娘子是一支了不起的乐队》</em></p></blockquote><p id="9c14" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">成为</p><blockquote class="mv mw mx"><p id="f520" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated"><em class="hu"/><strong class="jp hv"><em class="hu">铁娘子</em> </strong> <em class="hu">是一支令人惊叹的乐队</em></p></blockquote><p id="b385" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">所以复合名词的上下文非常突出，而且非常准确！</p><p id="3505" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">结果:</strong></p><p id="1f65" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">与“铁娘子”最相关的词是:</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="601d" class="nb kl hu od b fv oh oi l oj ok">[(u'judas_priest', 0.8176089525222778),<br/> (u'black_sabbath', 0.7859792709350586),<br/> (u'megadeth', 0.7748109102249146),<br/> (u'metallica', 0.7701393961906433),<br/> .....</span></pre><p id="66c0" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">那是硬核，字面意思！</strong></p><p id="a76e" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">下面是一个Python代码，用于将名词转换为复合名词(<em class="mt">以及形容词-名词配对</em>)，以便为训练Word2Vec创建训练集。</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oo"><img src="../Images/010dbb732b0301bd347cb75b43f1762d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FL0OFZVK7R-HNECEwQ9R0A.jpeg"/></div></div></figure><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff op"><img src="../Images/09b37e96971f6bec25facd89919328e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_ugkn1HL9oZPfcsxYbZyQ.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 8</figcaption></figure><p id="1aa5" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">这个Python代码用于将名词转换成复合词(<em class="mt">仅名词-名词配对</em>)。</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oq"><img src="../Images/62eea11a5bdad8b710446f02793f85de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VazAGAUNuE9TJBgaHhrWA.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 9</figcaption></figure><h2 id="e840" class="nb kl hu bd km nc nd ne kq nf ng nh ku jy ni nj ky kc nk nl lc kg nm nn lg no dt translated">变体2 : Sense2Vec</h2><p id="ff13" class="pw-post-body-paragraph jn jo hu jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj jm hn dt translated"><em class="mt">(注:Sense2Vec的非NER实现)</em></p><p id="119a" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">通过将词性(P.O.S)标签添加到训练集中，将上述变体更进一步。</p><p id="c7b9" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">示例:</p><blockquote class="mv mw mx"><p id="4bf5" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated"><em class="hu">“我上课熨衬衫”</em></p></blockquote><p id="85aa" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">成为</p><blockquote class="mv mw mx"><p id="156f" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated"><em class="hu">“我</em><strong class="jp hv"><em class="hu">/PRP</em></strong><em class="hu">铁</em><strong class="jp hv"><em class="hu">/VBP</em></strong><em class="hu">我</em><strong class="jp hv"><em class="hu">/PRP</em></strong><em class="hu">衬衫</em><strong class="jp hv"><em class="hu">/NN</em></strong><em class="hu">同</em><strong class="jp hv"><em class="hu">/在</em> </strong> <em class="hu">班</em>T72<em class="hu">/。</em><em class="hu"/></p></blockquote><p id="7623" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">或者</p><blockquote class="mv mw mx"><p id="36b3" class="jn jo mt jp b jq ln js jt ju lo jw jx my lp ka kb mz lq ke kf na lr ki kj jm hn dt translated">“我<strong class="jp hv">/名词</strong>铁<strong class="jp hv">/动词</strong>我的<strong class="jp hv"> /ADJ </strong>衬衫<strong class="jp hv">/名词</strong>带<strong class="jp hv"> /ADP </strong>类<strong class="jp hv">/名词</strong>。<strong class="jp hv"> /PUNCT </strong></p></blockquote><p id="a574" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">结果:</p><p id="149f" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">现在最相关的单词是:</p><pre class="nq nr ns nt fq oc od oe of aw og dt"><span id="e5ac" class="nb kl hu od b fv oh oi l oj ok">[(u'ironing/VERB', 0.818801760673523),<br/> (u'polish/VERB', 0.794084906578064),<br/> (u'smooth/VERB', 0.7590495347976685),<br/> .....</span></pre><p id="36c5" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><em class="mt">(参见“问题”部分的“示例2”进行比较)</em></p><p id="6256" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">下面是<em class="mt">sense 2 vec</em>的可视化</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff or"><img src="../Images/863c671bba53060c502d455d7c531010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQudp89tM0cRHNh74EIlOg.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 10</figcaption></figure><p id="ea89" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">下面是为Sense2Vec准备训练数据集的Python代码</p><figure class="nq nr ns nt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff os"><img src="../Images/e87885da9df5fa839890d0cea77d7c93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*62tr97bfBAo5J4j0xMt82Q.jpeg"/></div></div><figcaption class="nu nv fg fe ff nw nx bd b be z ek">fig no. 11</figcaption></figure><p id="2a66" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated">这些代码可从<a class="ae ls" href="https://github.com/mukulmalik18/preprocessing" rel="noopener ugc nofollow" target="_blank"> github </a>获得。</p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="4d45" class="kk kl hu bd km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh dt translated">结论</h1><ul class=""><li id="cb75" class="lt lu hu jp b jq li ju lj jy ot kc ou kg ov jm ly lz ma mb dt translated">与它们的大小相比，Word2Vec可以容纳大量的信息！</li><li id="918f" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">他们可以学习语义和句法</li><li id="49b9" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">一个问题是对多个上下文的概括，但这也可以通过对训练文本的额外修改来解决</li><li id="0a9e" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">它们是计算友好的，因为它们都是数字数组</li><li id="5021" class="lt lu hu jp b jq mc ju md jy me kc mf kg mg jm ly lz ma mb dt translated">向量之间的关系可以用线性代数来发现</li></ul></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><p id="612b" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">接下来:</strong> Word2Vec(第二部分)用例</p><p id="8b34" class="pw-post-body-paragraph jn jo hu jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj jm hn dt translated"><strong class="jp hv">上一页:</strong> <a class="ae ls" rel="noopener" href="/@mukulmalik/natural-language-processing-nlp-933cb7162932#.280sgl482">自然语言处理</a></p><figure class="nq nr ns nt fq iv"><div class="bz el l di"><div class="ow ox l"/></div></figure></div></div>    
</body>
</html>