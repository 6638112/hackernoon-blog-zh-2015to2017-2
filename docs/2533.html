<html>
<head>
<title>Predict Stock Market with Daily Top News</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用每日头条新闻预测股市</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/predict-stock-market-with-daily-top-news-8c8db25bef8d?source=collection_archive---------1-----------------------#2017-02-02">https://medium.com/hackernoon/predict-stock-market-with-daily-top-news-8c8db25bef8d?source=collection_archive---------1-----------------------#2017-02-02</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="6ce3" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">R语言中自然语言处理的分类方法</h2></div><blockquote class="jj jk jl"><p id="638c" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">观众</strong></p></blockquote><p id="762e" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">这篇简短的博客文章是为那些对自然语言处理算法<em class="jo"> word2vec，</em>有基本了解，并且熟悉机器<a class="ae km" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>基础知识，特别是分类技术，并且热衷于学习两者在r中的实际集成的人写的。我们将详细讨论数据准备以及模型优化。还建议回顾一下<a class="ae km" rel="noopener" href="/@jameschen_78678/social-behavior-analytics-201-523199badba0#.k8nhyigon">这篇</a>关于<em class="jo"> word2vec的简要介绍。</em></p><blockquote class="jj jk jl"><p id="2ac8" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">目标</strong></p></blockquote><p id="df11" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">目标是了解道琼斯工业平均指数将如何受到Reddit上每日热门新闻的影响。</p><blockquote class="jj jk jl"><p id="ec38" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">数据集</strong></p></blockquote><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/1f758e3c99d241e6e3530887032efee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vlL0kIqIe5EuNyFMQgTLQQ.png"/></div></div></figure><p id="e5f8" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">数据集可以在<a class="ae km" href="https://www.kaggle.com/aaron7sun/stocknews" rel="noopener ugc nofollow" target="_blank">这里</a>找到。简化的数据集由道琼斯每日数据和Reddit世界新闻频道过去8年的每日头条新闻组成。目标变量被标记为(0表示向下，1表示向上)。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kz"><img src="../Images/0b54ce8ee9e9d92fb8e2f31cc1fa02c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaPuRXfgCiSaAp39Budd5Q.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Screenshot of the stock news data set</figcaption></figure><blockquote class="jj jk jl"><p id="da64" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">接近</strong></p></blockquote><p id="5e02" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">我们将使用<em class="jo"> word2vec </em>算法首先在所有标题上创建100维的词向量。然后，我们将通过平均组成每个句子的单词向量，将标题转换成句子向量。最后，我们将使用各种分类算法来预测道琼斯工业平均指数是上涨还是下跌。</p><blockquote class="jj jk jl"><p id="f495" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">数据准备</strong></p></blockquote><p id="d8d9" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">1…删除不需要的字符，包括标点符号和大写字母</p><p id="9ea8" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">2…删除英语停用词，如“我”、“我的”、“我自己”</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kz"><img src="../Images/3452ac122373167ad676958e5ed5937d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfjQdFGsbQnAfJjx6kWXAg.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Screenshot of the data set with unwanted features removed</figcaption></figure><p id="cc2b" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">3…将清洗后的标题输入到<em class="jo"> word2vec </em>模型中，获得单词向量</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff le"><img src="../Images/be63358f69414e2f6c1fcac9eaed3db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EA15K-yIGgoz44mMNGinMw.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kz"><img src="../Images/1bded9774e45ce17b452a48d34db857d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K18xpo7to-_Wlx1uoOfkfQ.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Screenshot of the word vectors generated from cleaned headlines</figcaption></figure><p id="ffd3" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">4…对所有标题应用平均词向量</p><p id="2b53" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">5…使用t-SNE返回标题上的二维向量表示</p><p id="b308" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">6…用目标变量和二维向量创建数据集</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lf"><img src="../Images/37ecfa650bca587ce859fd8b2e9b92c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Z7QRoLWcoj44KBD5mQxqQ.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Screenshot of the finished data set</figcaption></figure><blockquote class="jj jk jl"><p id="df04" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">模型评估</strong></p></blockquote><p id="3c59" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在这里，我们将使用R中的<em class="jo"> caret </em>包来评估各种算法的预测精度，包括CART (rpart)、逻辑回归(glm)、线性判别分析(LDA)、k-最近邻(kNN)、支持向量机与径向基函数(svmRadial)和极端梯度推进(xgbLinear)。</p><p id="1a3e" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">观察到上述所有算法都具有不令人满意的性能，具有接近50%的准确度和差的Kappa值。然而，具有径向基函数核的SVM往往比其他算法执行得更好，精确度的95%置信区间在51%和55%之间。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lg"><img src="../Images/753731696902a54a931c96388c4dac40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vH8Bjy7AC4HotoslpmsLg.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/4b042ccf076a0d5a4a09103f915b6343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYkkoDCJ8plN60kOb3nA0A.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Performance on various classification algorithms</figcaption></figure><p id="638b" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">因此，我们将探索一些优化选项。</p><blockquote class="jj jk jl"><p id="fbdf" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv">性能优化</strong></p></blockquote><p id="796d" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv"><em class="jo">1…堆叠多个型号</em> </strong></p><p id="2905" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">我们将首先尝试堆叠所有模型，看看性能是否会提高。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lh"><img src="../Images/a121b000189aa864cd14593688c9a89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XK1rcWrmmFPkslnQIVnhqA.png"/></div></div></figure><p id="e482" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">不幸的是，没有观察到多少改善。</p><p id="0ce9" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv"><em class="jo">2…平均所有标题</em> </strong></p><p id="8638" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">我们将把所有的标题组合起来，创建一个二维向量集，而不是把标题当作单独的变量。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff li"><img src="../Images/681ebbfbcafe6fdb6bc8b14e9008e64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjmhLbOMy7hjPrLxZYj8tA.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/a887d437a07055a2b2e498fd7d3bfdaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yn1dooIg0_HWO8SzBs3ajw.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Performance on the algorithms with averaged headlines</figcaption></figure><p id="decb" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在SVM算法中观察到平均准确度有0.1%的轻微增加。</p><p id="98cb" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv"><em class="jo">3…移动目标变量</em> </strong></p><p id="e95a" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">我们将尝试将目标变量后移1-7天，因为每日新闻可能会对<a class="ae km" href="https://goo.gl/1nZ8JP" rel="noopener ugc nofollow" target="_blank">股票表现</a>产生滞后效应。或者，我们也可以将目标变量前移几天，但是，这种方法将与使用每日头条新闻预测<em class="jo">股票营销业绩的目标相矛盾。</em></p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lj"><img src="../Images/8059b0819de7207063729e6277c715f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3ngr_gNcRHYUx42XFecJA.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/fd312761000152686f83e9a159c16e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lkOrpkoei-WpG0SW_EWaIQ.png"/></div></div><figcaption class="la lb fg fe ff lc ld bd b be z ek">Performance on the algorithms with 3 day lag on target variable</figcaption></figure><p id="4d83" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">我们可以观察到CART (rpart)现在在平均准确率方面具有最好的性能，为53.66%。在SVM算法中还观察到平均精度的0.1%的另一个轻微增加。</p><p id="2444" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv"><em class="jo">4…调谐模型参数</em> </strong></p><p id="48bb" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在这里，我们将演示如何调整购物车和SVM算法的参数，看看我们是否可以提高预测的准确性。</p><p id="d9b1" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv">推车</strong></p><p id="4b2a" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在rpart中，调谐参数是<em class="jo"> cp </em>，或者是<em class="jo">复杂度参数</em>。我们将检查<em class="jo"> cp </em>的不同值的准确性。最佳cp值为0.15，如下所示。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lg"><img src="../Images/648e1ccd7d1f2fc42b0ea534469e8a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJWeFfxcerbClV895twOPQ.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/eafedb632b12c058759c60e1b20d1be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S50E3tjW-iP03kVniNT_9A.png"/></div></div></figure><p id="eef7" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jp hv"> SVM </strong></p><p id="3625" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在径向基函数的SVM中，调优参数为<em class="jo"> sigma </em>和<em class="jo"> C </em> ( <em class="jo"> cost </em>)。我们将检查<em class="jo">σ</em>和<em class="jo"> C </em>的不同值的准确性。最佳值如下所示，其中<em class="jo">σ= 0.05</em>和<em class="jo"> C = 3 </em>。</p><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff lg"><img src="../Images/3fa314f75034695d397dccb3ab2afc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-4ukDVXZTGnyYwzO3AvFw.png"/></div></div></figure><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff kn"><img src="../Images/dde660871fc98ac48e73a4aa5eae527d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2H4sVYCDVZ_jG89uNk2tQ.png"/></div></div></figure><blockquote class="jj jk jl"><p id="e7a0" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><strong class="jp hv"> R代码</strong></p></blockquote><pre class="ko kp kq kr fq lk ll lm ln aw lo dt"><span id="f0e5" class="lp lq hu ll b fv lr ls l lt lu">library(devtools)<br/>library(tm)<br/>library(ggplot2)<br/>library(text2vec)<br/>library(rword2vec)<br/>library(wordVectors)<br/>library(tidyverse)<br/>library(Rtsne)<br/>library(caret)<br/>library(mlbench)<br/>library(caretEnsemble)</span><span id="5411" class="lp lq hu ll b fv lv ls l lt lu">setwd("~/Desktop/stock")<br/>#Simple Visualization on DJIA data<br/>stock &lt;- read.csv("DJIA_table.csv")<br/>stock$Date &lt;- as.Date(stock$Date)</span><span id="ef60" class="lp lq hu ll b fv lv ls l lt lu">ggplot(data=stock,aes(x=Date,y=Open))+geom_line()+labs(title="Dow Jones Industrial Average")</span><span id="2186" class="lp lq hu ll b fv lv ls l lt lu">#Read the simplified data set<br/>data &lt;- read.csv("stock.csv")</span><span id="5a44" class="lp lq hu ll b fv lv ls l lt lu">#Remove unwanted columns for natural language processing<br/>text &lt;- data</span><span id="142e" class="lp lq hu ll b fv lv ls l lt lu">text$Date &lt;- NULL<br/>text$Label &lt;- NULL</span><span id="6609" class="lp lq hu ll b fv lv ls l lt lu">#Remove unwanted characters/punctuations/upper-cases<br/>for (i in 1:25){<br/>  text[,i] &lt;- gsub('b"',"",text[,i])<br/>  text[,i] &lt;- gsub("b'","",text[,i])<br/>  text[,i] &lt;- gsub("[[:punct:]]", "", text[,i])<br/>  text[,i] &lt;- tolower(text[,i])<br/>}</span><span id="a31e" class="lp lq hu ll b fv lv ls l lt lu">write.table(text,"text.txt")</span><span id="1f91" class="lp lq hu ll b fv lv ls l lt lu">#Rename column names<br/>new_text &lt;- subset(text,select=paste("Top",1,sep=""))<br/>new_text &lt;- setNames(new_text,"x")</span><span id="ac8f" class="lp lq hu ll b fv lv ls l lt lu">for (i in 2:25){<br/>new_text2 &lt;- subset(text,select=paste("Top",i,sep=""))<br/>new_text2 &lt;- setNames(new_text2,"x")<br/>new_text &lt;- rbind(new_text,new_text2)<br/>}</span><span id="4b11" class="lp lq hu ll b fv lv ls l lt lu">write.table(new_text,"text.txt")</span><span id="552f" class="lp lq hu ll b fv lv ls l lt lu">final_text &lt;- new_text</span><span id="2f42" class="lp lq hu ll b fv lv ls l lt lu">#Remove stopwords<br/>stopWords &lt;- stopwords("en")<br/>'%nin%' &lt;- Negate('%in%')</span><span id="cd66" class="lp lq hu ll b fv lv ls l lt lu">train &lt;- lapply(final_text$x, function(x) {<br/>  t &lt;- unlist(strsplit(x, " "))<br/>  t[t %nin% stopWords]<br/>})</span><span id="2a87" class="lp lq hu ll b fv lv ls l lt lu">train.df = as.data.frame(do.call(rbind, train))<br/>train.df$x &lt;- do.call(paste, c(train.df[,1:45], sep=" "))</span><span id="5274" class="lp lq hu ll b fv lv ls l lt lu">finaltext &lt;- subset(train.df,select="x")</span><span id="04df" class="lp lq hu ll b fv lv ls l lt lu">write.table(finaltext,"text_data.txt")</span><span id="9b64" class="lp lq hu ll b fv lv ls l lt lu">#Input words to word2vec<br/>model = train_word2vec("text_data.txt", output="vec.bin", <br/>                       threads = 4, vectors = 100, window = 12, min_count = 5, iter=10, force=TRUE)</span><span id="e28c" class="lp lq hu ll b fv lv ls l lt lu">#Plot the word vectors<br/>plot(model)<br/>#Convert binary to text format<br/>bin_to_txt("vec.bin","model1text.txt")</span><span id="6b81" class="lp lq hu ll b fv lv ls l lt lu">#Remove first rows manually in the .txt file</span><span id="9dd0" class="lp lq hu ll b fv lv ls l lt lu">#Convert .txt to .cvs<br/>m1 &lt;- read.table("model1text.txt",<br/>                     header = F,quote = "", row.names = NULL, stringsAsFactors = FALSE)</span><span id="4b90" class="lp lq hu ll b fv lv ls l lt lu">colnames(m1)[1] &lt;- "word"</span><span id="f97c" class="lp lq hu ll b fv lv ls l lt lu">#Apply word vectors to headlines and return 2-dimension representations with t-SNE<br/>label &lt;- subset(data,select="Label")<br/>labeltext &lt;- cbind(label,text)<br/>labeltext$id &lt;- 1:nrow(labeltext)<br/>labeltext$id &lt;- as.factor(labeltext$id)</span><span id="8673" class="lp lq hu ll b fv lv ls l lt lu">for (i in 2:24){<br/>  s &lt;- strsplit(labeltext[,i], split= " ")<br/>  new &lt;- data.frame(V1 = rep(labeltext$id, sapply(s, length)), V2 = unlist(s))<br/>  colnames(new) &lt;- c("id","word")<br/>  new &lt;- merge(new,m1,by="word")<br/>  new2 &lt;- aggregate(V2 ~ id, new, mean)<br/>  for (j in 4:102){<br/>    new3 &lt;- aggregate(new[,j] ~ id, new, mean)<br/>    new2 &lt;- merge (new2,new3,by="id")<br/>  }<br/>  colnames(new2)[2:101] &lt;- paste("V", 1:100, sep="")<br/>  tsne &lt;- Rtsne(new2, dims = 2, perplexity=100, verbose=TRUE, max_iter = 500)<br/>  t = as.data.frame(tsne$Y)<br/>  colnames(t) &lt;- c(paste("top",i,"x",sep=""),paste("top",i,"y",sep=""))<br/>  labeltext &lt;- cbind(labeltext,t)<br/>}</span><span id="c759" class="lp lq hu ll b fv lv ls l lt lu">write.csv(labeltext,"labeltext.csv")</span><span id="96e9" class="lp lq hu ll b fv lv ls l lt lu">#Add back Label information<br/>mydata &lt;- labeltext<br/>mydata[,2:27] &lt;- NULL</span><span id="20f7" class="lp lq hu ll b fv lv ls l lt lu">mydata$Label[mydata$Label==1]&lt;- "Yes"<br/>mydata$Label[mydata$Label==0]&lt;- "No"</span><span id="0d40" class="lp lq hu ll b fv lv ls l lt lu">#Classfication Models<br/>control &lt;- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)<br/>algorithmList &lt;- c('rpart','glm','lda','knn','svmRadial','xgbLinear')<br/>set.seed(1)<br/>models &lt;- caretList(Label~., data=mydata, trControl=control, methodList=algorithmList)<br/>results &lt;- resamples(models)<br/>summary(results)<br/>dotplot(results)</span><span id="0cb7" class="lp lq hu ll b fv lv ls l lt lu">#-----Performance Optimization #1: Stacking Multiple Models-----<br/>#Model Correlations<br/>modelCor(results)<br/>splom(results)<br/>#Model Stacking with GLM<br/>stackControl &lt;- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)<br/>set.seed(2)<br/>stack.glm &lt;- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)<br/>print(stack.glm)</span><span id="dca6" class="lp lq hu ll b fv lv ls l lt lu">#-----Performance Optimization #2: Averaging All Headlines-----<br/>mydata2 &lt;- mydata<br/>mydata2$headline &lt;- rowMeans(mydata2[,2:45], na.rm = TRUE)<br/>mydata2[,2:45] &lt;- NULL</span><span id="b460" class="lp lq hu ll b fv lv ls l lt lu">control &lt;- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)<br/>algorithmList &lt;- c('rpart','glm','lda','knn','svmRadial','xgbLinear')<br/>set.seed(1)<br/>models &lt;- caretList(Label~., data=mydata2, trControl=control, methodList=algorithmList)<br/>results &lt;- resamples(models)<br/>summary(results)<br/>dotplot(results)</span><span id="d19f" class="lp lq hu ll b fv lv ls l lt lu">#-----Performance Optimization #3: Shifting Target Variable-----<br/>shift &lt;-subset(mydata,select="Label")<br/>shift1 &lt;- as.data.frame(shift[-1,])<br/>shift2 &lt;- as.data.frame(shift[-c(1,2),])<br/>shift3 &lt;- as.data.frame(shift[-c(1,2,3),])<br/>shift4 &lt;- as.data.frame(shift[-c(1,2,3,4),])<br/>shift5 &lt;- as.data.frame(shift[-c(1,2,3,4,5),])<br/>shift6 &lt;- as.data.frame(shift[-c(1,2,3,4,5,6),])<br/>shift7 &lt;- as.data.frame(shift[-c(1,2,3,4,5,6,7),])</span><span id="8960" class="lp lq hu ll b fv lv ls l lt lu">mydata1 &lt;- mydata[-1989,]<br/>mydata2 &lt;- mydata[-c(1988,1989),]<br/>mydata3 &lt;- mydata[-c(1987,1988,1989),]<br/>mydata4 &lt;- mydata[-c(1986,1987,1988,1989),]<br/>mydata5 &lt;- mydata[-c(1985,1986,1987,1988,1989),]<br/>mydata6 &lt;- mydata[-c(1984,1985,1986,1987,1988,1989),]<br/>mydata7 &lt;- mydata[-c(1983,1984,1985,1986,1987,1988,1989),]</span><span id="ba64" class="lp lq hu ll b fv lv ls l lt lu">mydata1$Label &lt;- NULL<br/>mydata2$Label &lt;- NULL<br/>mydata3$Label &lt;- NULL<br/>mydata4$Label &lt;- NULL<br/>mydata5$Label &lt;- NULL<br/>mydata6$Label &lt;- NULL<br/>mydata7$Label &lt;- NULL</span><span id="457d" class="lp lq hu ll b fv lv ls l lt lu">mydata1 &lt;- cbind(shift1,mydata1)<br/>colnames(mydata1)[1] &lt;- "label1day"<br/>mydata2 &lt;- cbind(shift2,mydata2)<br/>colnames(mydata2)[1] &lt;- "label2day"<br/>mydata3 &lt;- cbind(shift3,mydata3)<br/>colnames(mydata3)[1] &lt;- "label3day"<br/>mydata4 &lt;- cbind(shift4,mydata4)<br/>colnames(mydata4)[1] &lt;- "label4day"<br/>mydata5 &lt;- cbind(shift5,mydata5)<br/>colnames(mydata5)[1] &lt;- "label5day"<br/>mydata6 &lt;- cbind(shift6,mydata6)<br/>colnames(mydata6)[1] &lt;- "label6day"<br/>mydata7 &lt;- cbind(shift7,mydata7)<br/>colnames(mydata7)[1] &lt;- "label7day"</span><span id="c4f4" class="lp lq hu ll b fv lv ls l lt lu">control &lt;- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)<br/>algorithmList &lt;- c('rpart','glm','lda','knn','svmRadial','xgbLinear')<br/>set.seed(1)<br/>models &lt;- caretList(label3day~., data=mydata3, trControl=control, methodList=algorithmList)<br/>results &lt;- resamples(models)<br/>summary(results)<br/>dotplot(results)</span><span id="7466" class="lp lq hu ll b fv lv ls l lt lu">#-----Performance Optimization #4: Tuning Model Parameters-----<br/>set.seed(3)<br/>control &lt;- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)</span><span id="3aa5" class="lp lq hu ll b fv lv ls l lt lu">#Tuning cp in CART<br/>rpartGrid &lt;-  expand.grid(cp = c(0.01,0.05,0.1,0.15))</span><span id="97fb" class="lp lq hu ll b fv lv ls l lt lu">rpartFit &lt;- train(label3day ~ .,<br/>                  data = mydata3,<br/>                  method = "rpart",<br/>                  tuneLength = 10,<br/>                  tuneGrid=rpartGrid)</span><span id="5cea" class="lp lq hu ll b fv lv ls l lt lu">rpartFit<br/>ggplot(rpartFit)</span><span id="abdb" class="lp lq hu ll b fv lv ls l lt lu">#Tuning sigma and C in SVM<br/>svmGrid &lt;-  expand.grid(sigma = c(0.01,0.05,0.1,0.15),<br/>                        C = c(1,3,5,7,9))</span><span id="75ce" class="lp lq hu ll b fv lv ls l lt lu">svmFit &lt;- train(label3day ~ ., data = mydata3, <br/>                 method = "svmRadial", <br/>                 trControl = control, <br/>                 verbose = FALSE,<br/>                 tuneGrid = svmGrid)</span><span id="9272" class="lp lq hu ll b fv lv ls l lt lu">svmFit<br/>ggplot(svmFit)</span></pre></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><p id="23d2" class="pw-post-body-paragraph jm jn hu jp b jq jr iv js jt ju iy jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt md translated">问题、评论或顾虑？<br/>jchen6912@gmail.com</p><div class="ko kp kq kr fq ab cb"><figure class="mm ks mn mo mp mq mr paragraph-image"><a href="http://bit.ly/HackernoonFB"><img src="../Images/50ef4044ecd4e250b5d50f368b775d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0hqOaABQ7XGPT-OYNgiUBg.png"/></a></figure><figure class="mm ks mn mo mp mq mr paragraph-image"><a href="https://goo.gl/k7XYbx"><img src="../Images/979d9a46439d5aebbdcdca574e21dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vgw1jkA6hgnvwzTsfMlnpg.png"/></a></figure><figure class="mm ks mn mo mp mq mr paragraph-image"><a href="https://goo.gl/4ofytp"><img src="../Images/2930ba6bd2c12218fdbbf7e02c8746ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gKBpq1ruUi0FVK2UM_I4tQ.png"/></a></figure></div><blockquote class="jj jk jl"><p id="f922" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated"><a class="ae km" href="http://bit.ly/Hackernoon" rel="noopener ugc nofollow" target="_blank">黑客中午</a>是黑客如何开始他们的下午。我们是<a class="ae km" href="http://bit.ly/atAMIatAMI" rel="noopener ugc nofollow" target="_blank"> @AMI </a>家庭的一员。我们现在<a class="ae km" href="http://bit.ly/hackernoonsubmission" rel="noopener ugc nofollow" target="_blank">接受投稿</a>，并乐意<a class="ae km" href="mailto:partners@amipublications.com" rel="noopener ugc nofollow" target="_blank">讨论广告&amp;赞助</a>机会。</p><p id="708a" class="jm jn jo jp b jq jr iv js jt ju iy jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated">如果你喜欢这个故事，我们推荐你阅读我们的<a class="ae km" href="http://bit.ly/hackernoonlatestt" rel="noopener ugc nofollow" target="_blank">最新科技故事</a>和<a class="ae km" href="https://hackernoon.com/trending" rel="noopener ugc nofollow" target="_blank">趋势科技故事</a>。直到下一次，不要把世界的现实想当然！</p></blockquote><figure class="ko kp kq kr fq ks fe ff paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="fe ff ms"><img src="../Images/be0ca55ba73a573dce11effb2ee80d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35tCjoPcvq6LbB3I6Wegqw.jpeg"/></div></div></figure><figure class="ko kp kq kr fq ks"><div class="bz el l di"><div class="mt mu l"/></div></figure></div></div>    
</body>
</html>