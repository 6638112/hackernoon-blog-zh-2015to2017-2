<html>
<head>
<title>A Guide to Scaling Machine Learning Models in Production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生产中扩展机器学习模型的指南</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/a-guide-to-scaling-machine-learning-models-in-production-aa8831163846?source=collection_archive---------2-----------------------#2017-12-25">https://medium.com/hackernoon/a-guide-to-scaling-machine-learning-models-in-production-aa8831163846?source=collection_archive---------2-----------------------#2017-12-25</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/785fbc6efd20ef859aef0010806e71d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGKUyc1WsvNbQKIIBjL7OA.png"/></div></div></figure><div class=""/><p id="8444" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">构建机器学习模型的工作流程往往在评估阶段就结束了:你已经达到了一个可以接受的精度，并且"<em class="ka"> ta-da！</em> <em class="ka">使命完成。</em></p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="kf kg l"/></div></figure><p id="d358" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">除此之外，为您的论文或内部文档获取那些好看的图表可能就足够了。</p><p id="bb16" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">事实上，并不总是需要付出额外的努力将模型投入生产。即使是这样，这项任务也会委托给系统管理员。</p><figure class="kb kc kd ke fq hw fe ff paragraph-image"><div class="fe ff kh"><img src="../Images/77c2b7ab05481e69ed5df197666d1bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0mDxCiP5D6Io-Cy0PcUovw.png"/></div></figure><p id="7f89" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然而，如今，许多研究人员/工程师发现他们自己负责处理从构思模型到向外界提供服务的完整流程。无论是大学项目还是个人实验，展示我们的工作通常是让更多观众感兴趣的好方法。很少有人会花额外的精力去使用一个价值不能立即被感知的系统。</p><figure class="kb kc kd ke fq hw fe ff paragraph-image"><div class="fe ff ki"><img src="../Images/5c3f982659b94566e1f191f3bd8141db.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*iwTQIip1MpsyWBTN-yRgXg.png"/></div></figure><p id="67f8" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在本文中，我们将一起经历这个工作流；一个我不得不自己反复做的过程。假设你已经建立了一个机器学习或者深度学习模型，使用你喜欢的框架(scikit-learn，Keras，Tensorflow，PyTorch等。).现在你想通过一个API向全世界大规模提供服务。</p><p id="d470" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je ig">通过<em class="ka">【大规模】</em>，我们不是在谈论一家大公司的工业规模。我们的目标是充分利用那台拥有大量CPU和大容量RAM的服务器，</strong>闲置在您的机构或云中。这需要一次处理多个请求，随着负载的增加产生新的进程，随着负载的减少减少进程的数量。您还需要额外的保证，即服务器在意外的系统故障后会重新启动。</p><p id="1a84" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果这是你的想法，让我们一起经历它。</p></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><p id="5923" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将考虑Linux服务器上基于Python的框架的上下文。我们的设置将包括:</p><ul class=""><li id="03fc" class="kq kr if je b jf jg jj jk jn ks jr kt jv ku jz kv kw kx ky dt translated"><a class="ae kz" href="https://anaconda.org" rel="noopener ugc nofollow" target="_blank"> <strong class="je ig"> Anaconda </strong> </a>:用于管理包安装，创建一个隔离的Python 3环境。</li><li id="ea1b" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><a class="ae kz" href="http://keras.io" rel="noopener ugc nofollow" target="_blank"><strong class="je ig">Keras</strong></a><strong class="je ig">:</strong>一种高级神经网络API，能够在<a class="ae kz" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>、<a class="ae kz" href="https://github.com/Microsoft/cntk" rel="noopener ugc nofollow" target="_blank"> CNTK </a>或<a class="ae kz" href="https://github.com/Theano/Theano" rel="noopener ugc nofollow" target="_blank"> Theano </a>之上运行。</li><li id="2b5e" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><a class="ae kz" href="http://flask.pocoo.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="je ig"> Flask </strong> </a>:构建RESTful APIs的极简python框架。尽管很容易使用，但Flask的内置服务器默认情况下一次只服务一个请求；因此<a class="ae kz" href="http://flask.pocoo.org/docs/deploying/" rel="noopener ugc nofollow" target="_blank">它本身不适合</a>在生产中部署。</li><li id="a37a" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><a class="ae kz" href="https://nginx.org/en/" rel="noopener ugc nofollow" target="_blank"> <strong class="je ig"> nginx </strong> </a>:高度稳定的<strong class="je ig"> web服务器</strong>，提供负载均衡、SSL配置等好处。</li><li id="9b91" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><a class="ae kz" href="https://uwsgi-docs.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><strong class="je ig">uWSGI</strong></a><strong class="je ig">:</strong>一个高度可配置的<strong class="je ig"> WSGI服务器</strong> (Web服务器网关接口)，允许分叉多个工作器一次服务多个请求。</li><li id="a7b4" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><a class="ae kz" href="https://en.wikipedia.org/wiki/Systemd" rel="noopener ugc nofollow" target="_blank"><strong class="je ig">systemd</strong></a><strong class="je ig">:</strong>在多个Linux发行版中使用的一个<em class="ka"> init </em>系统，用于管理引导后的系统进程。</li></ul></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><p id="9879" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Nginx将是我们与互联网的接口，它将是处理客户请求的接口。Nginx拥有对二进制uWSGI协议的本地支持，它们通过Unix套接字进行通信。反过来，uWSGI服务器将直接调用Flask应用程序中的可调用对象。这是满足请求的方式。</p><figure class="kb kc kd ke fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lf"><img src="../Images/e068ebb6c8c5e80b88dbc13de8f5dae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L4_SuPkWy1wZlGe-NQn59w.png"/></div></div><figcaption class="lg lh fg fe ff li lj bd b be z ek">The complete workflow illustrated</figcaption></figure><p id="eff0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程开头的一些注意事项:</p><ul class=""><li id="5ebd" class="kq kr if je b jf jg jj jk jn ks jr kt jv ku jz kv kw kx ky dt translated"><strong class="je ig">上述大部分组件可以很容易地用等效组件替换，其余步骤几乎没有变化</strong>。比如Keras可以轻松换成PyTorch，Flask可以轻松换成<a class="ae kz" href="https://bottlepy.org/" rel="noopener ugc nofollow" target="_blank">瓶</a>等等。</li><li id="bcba" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><strong class="je ig">我们将只考虑服务模型超过CPU的情况。</strong>典型案例是访问具有大量CPU内核的服务器，并尝试利用这些内核为模型提供服务。另一方面，大量购买GPU的成本更高。此外，根据您的应用，在预测时使用GPU获得的速度增益可能并不显著(特别是在NLP应用中)。</li></ul><h1 id="3b4e" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">设置环境</h1><p id="d547" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">首先，我们需要安装<code class="eh mn mo mp mq b">systemd</code>和<code class="eh mn mo mp mq b">nginx</code>包:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="025d" class="mv ll if mq b fv mw mx l my mz">sudo apt-get install systemd <!-- -->nginx</span></pre><p id="0431" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">接下来，我们必须按照官方网站上的说明<a class="ae kz" href="https://www.anaconda.com/download/#linux" rel="noopener ugc nofollow" target="_blank">安装Anaconda，包括下载可执行文件，运行它，并将Anaconda添加到您的系统路径中。下面，我们将假设Anaconda安装在主目录下。</a></p><p id="7af0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本文中的所有代码和配置文件都可以从附带的<a class="ae kz" href="https://github.com/harkous/production_ml" rel="noopener ugc nofollow" target="_blank"> Github资源库</a>中获得。但是请确保您按照下面的步骤来获得完整的工作流程。</p><div class="ht hu fm fo hv na"><a href="https://github.com/harkous/production_ml" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd ig fv z el nf eo ep ng er et ie dt translated">harkous/production_ml</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">production_ml —在生产中扩展机器学习模型</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">github.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ib na"/></div></div></a></div><p id="d897" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">接下来，让我们从<code class="eh mn mo mp mq b">environment.yml</code>文件创建隔离的Anaconda环境。下面是这个文件的样子(它已经包含了我们将要使用的几个框架):</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="6e4b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们运行以下命令来创建环境:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="4b7e" class="mv ll if mq b fv mw mx l my mz">conda env create --file environment.yml</span></pre><p id="75b6" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当我们想要激活这个环境时，我们运行:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="e4ad" class="mv ll if mq b fv mw mx l my mz">source activate production_ml_env</span></pre><p id="3af4" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">到目前为止，我们已经安装了Keras，还有flask、uwsgi、uwsgitop等。所以我们准备好开始了。</p><h1 id="e435" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">构建Flask Web应用程序</h1><p id="d4e5" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">出于本教程的目的，我们不会深入研究如何构建您的ML模型。相反，我们将使用Keras中捆绑的<a class="ae kz" href="https://keras.io/datasets/#reuters-newswire-topics-classification" rel="noopener ugc nofollow" target="_blank">路透社新闻专线数据集</a>来修改主题分类的示例。这是构建分类器的代码:</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="16fa" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要复制我们在这里使用的设置，只需运行以下命令来训练一个没有GPU的模型:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="426a" class="mv ll if mq b fv mw mx l my mz">export CUDA_VISIBLE_DEVICES=-1<br/>KERAS_BACKEND=theano python build_classifier.py</span></pre><p id="2b60" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这将在文件夹<code class="eh mn mo mp mq b">models</code>中创建一个模型文件<code class="eh mn mo mp mq b">reuters_model.hdf5</code>。现在，我们已经准备好通过<code class="eh mn mo mp mq b">4444</code>端口上的烧瓶为模型提供服务。在下面的代码中，我们提供了一个支持<code class="eh mn mo mp mq b">GET</code>请求的REST端点<code class="eh mn mo mp mq b">/predict</code>，其中要分类的文本作为参数提供。返回的JSON的形式是<code class="eh mn mo mp mq b">{"prediction": "N"}</code>，其中<code class="eh mn mo mp mq b">N</code>是表示预测类的整数。</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="f6ce" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了启动Flask应用服务器，我们运行:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="a133" class="mv ll if mq b fv mw mx l my mz">python app.py</span></pre><p id="3e30" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">瞧啊。现在我们已经有了简单、轻量级的服务器。</p><p id="c368" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">您可以使用您最喜欢的REST客户端(例如<a class="ae kz" href="https://www.getpostman.com/" rel="noopener ugc nofollow" target="_blank"> Postman </a>)来测试服务器，或者只需在您的web浏览器中访问这个URL(用您的服务器的URL替换<code class="eh mn mo mp mq b"><strong class="je ig">your_server_url</strong></code>):<code class="eh mn mo mp mq b"><a class="ae kz" href="http://your_server_url" rel="noopener ugc nofollow" target="_blank">http://<strong class="je ig">your_server_url</strong></a>:4444/predict?text=this is a news sample text about sports and football in specific</code></p><p id="1573" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">您应该得到这样的响应</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="7723" class="mv ll if mq b fv mw mx l my mz">{<br/>  "class": "11"<br/>}</span></pre><h1 id="8547" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">配置uWSGI服务器</h1><p id="f6d0" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">现在，我们开始扩展我们简单的应用服务器。uWSGI将是这里的关键。它通过调用文件<code class="eh mn mo mp mq b">app.py</code>中的可调用对象<code class="eh mn mo mp mq b">app</code>与我们的Flask应用程序通信。uWSGI包含了我们所追求的大部分并行化特性。其配置文件如下所示:</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="89de" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je ig">在你这边，你要把</strong> <code class="eh mn mo mp mq b"><strong class="je ig">my_app_folder</strong></code> <strong class="je ig">选项修改为自己app目录的文件夹，把</strong> <code class="eh mn mo mp mq b"><strong class="je ig">my_user</strong></code> <strong class="je ig">选项修改为自己的用户名。</strong>根据您的需求和文件位置，您可能还需要修改/添加<a class="ae kz" href="http://uwsgi-docs.readthedocs.io/en/latest/Options.html" rel="noopener ugc nofollow" target="_blank">其他选项</a>。</p><p id="bb64" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh mn mo mp mq b">uwsgi.ini</code>中的一个重要部分是我们使用uWSGI中更便宜的<a class="ae kz" href="http://uwsgi-docs.readthedocs.io/en/latest/Cheaper.html" rel="noopener ugc nofollow" target="_blank"> <strong class="je ig">子系统</strong></a>的部分，它允许我们并行运行多个workers来服务多个并发请求。这是uWSGI的一个很酷的特性，在这里，只需几个参数就可以实现动态缩放。采用上述配置，我们将始终拥有至少5名工作人员。随着负载的增加，priest将一次分配3个额外的工作线程，直到所有的请求都找到可用的工作线程。以上工作人员的最大数量设置为50人。</p><p id="8bbf" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在您的情况下，最佳配置选项取决于服务器中的内核数量、可用的总内存以及应用程序的内存消耗。看看<a class="ae kz" href="http://uwsgi-docs.readthedocs.io/en/latest/Cheaper.html" rel="noopener ugc nofollow" target="_blank">官方文件</a>中的高级部署选项。</p><h1 id="cf0b" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">用nginx连接uWSGI</h1><p id="d821" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">我们快到了。如果我们现在启动uWSGI(我们稍后会启动)，它将负责从文件<code class="eh mn mo mp mq b">app.py</code>中调用应用程序，我们将受益于它提供的所有缩放功能。然而，为了从互联网获取REST请求并通过uWSGI将它们传递给Flask应用程序，我们将配置nginx。</p><p id="61f6" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里是nginx的一个准系统配置文件，只有我们在这个应用程序中所依赖的部分。当然，nginx还可以用于配置SSL或服务静态文件，但这超出了本文的范围。</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="25d0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将这个文件放在<code class="eh mn mo mp mq b">/etc/nginx/sites-available/nginx_production_ml</code>中(为此您需要sudo访问)。然后，为了启用这个nginx配置，我们将它链接到<code class="eh mn mo mp mq b">sites-enabled</code>目录:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="6765" class="mv ll if mq b fv mw mx l my mz">sudo ln -s /etc/nginx/sites-available/nginx_production_ml /etc/nginx/sites-enabled</span></pre><p id="3b70" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们重启nginx:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="3eef" class="mv ll if mq b fv mw mx l my mz">sudo service nginx restart</span></pre><h1 id="f86b" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">配置systemd服务</h1><p id="8e3a" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">最后，我们将启动我们之前配置的uWSGI服务器。然而，为了确保我们的服务器在系统重启或意外故障后不会永远死亡，我们将把它作为<code class="eh mn mo mp mq b">systemd</code>服务推出。下面是我们的服务配置文件，我们使用以下命令将它放在<code class="eh mn mo mp mq b">/etc/systemd/system</code>目录中:</p><p id="3dea" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh mn mo mp mq b">sudo vi /etc/systemd/system/production_ml.service</code></p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="f37b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们从以下方面开始服务:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="7b99" class="mv ll if mq b fv mw mx l my mz">sudo systemctl start <!-- -->production_ml<!-- -->.service</span></pre><p id="16a3" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要允许此服务在机器重新启动时启动:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="e6fa" class="mv ll if mq b fv mw mx l my mz">sudo systemctl enable <!-- -->production_ml<!-- -->.service</span></pre><p id="8435" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这个阶段，我们的服务应该成功启动。如果我们稍后更新服务，我们只需重新启动它:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="b104" class="mv ll if mq b fv mw mx l my mz">sudo systemctl restart <!-- -->production_ml<!-- -->.service</span></pre><h1 id="3c9d" class="lk ll if bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh dt translated">监控服务</h1><p id="e6e9" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">为了监视服务并查看每个工作人员的负载，我们可以使用<code class="eh mn mo mp mq b"><a class="ae kz" href="https://github.com/xrmx/uwsgitop]" rel="noopener ugc nofollow" target="_blank">uwsgitop</a></code>。在<code class="eh mn mo mp mq b">uwsgi.ini</code>中，我们已经在应用程序文件夹中配置了一个stats套接字。要查看统计信息，请在该文件夹中执行以下命令:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="b533" class="mv ll if mq b fv mw mx l my mz">uwsgitop stats.production_ml.sock</span></pre><p id="3e46" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是一个正在运行的workers的例子，其中已经产生了额外的workers。要模拟你这边这么重的负载，即使是简单的任务，也可以人为的在预测代码中加一个<code class="eh mn mo mp mq b">time.sleep(3)</code>。</p><figure class="kb kc kd ke fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff nq"><img src="../Images/b644b8e376d7005f8f270cf877aae94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNC9-2QEWODIYSf-iMvm2A.png"/></div></div></figure><p id="1c7c" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">向服务器发送并发请求的一种方法是使用curl(记住用服务器的url或IP地址替换<code class="eh mn mo mp mq b">YOUR_SERVER_NAME_OR_IP</code>)。</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="np kg l"/></div></figure><p id="f805" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了监控应用程序本身的日志，我们可以使用<code class="eh mn mo mp mq b">journalctl</code>:</p><pre class="kb kc kd ke fq mr mq ms mt aw mu dt"><span id="b456" class="mv ll if mq b fv mw mx l my mz">sudo journalctl -u production_ml.service -f</span></pre><p id="bc36" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">您的输出应该如下所示:</p><figure class="kb kc kd ke fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff nr"><img src="../Images/3b44d8a502906c318708b50b6118940c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ktGmxGPMMTUHaKgyIP_qA.png"/></div></div></figure></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><h1 id="621c" class="lk ll if bd lm ln ns lp lq lr nt lt lu lv nu lx ly lz nv mb mc md nw mf mg mh dt translated">最终注释</h1><p id="35e0" class="pw-post-body-paragraph jc jd if je b jf mi jh ji jj mj jl jm jn mk jp jq jr ml jt ju jv mm jx jy jz hn dt translated">如果您已经达到了这个阶段，并且您的应用程序已经成功运行，那么本文就达到了它的目的。在此阶段，一些额外的注意事项值得一提:</p><ul class=""><li id="cdfe" class="kq kr if je b jf jg jj jk jn ks jr kt jv ku jz kv kw kx ky dt translated">uwsgi中的<code class="eh mn mo mp mq b"><strong class="je ig">lazy-apps</strong></code> <strong class="je ig">模式:</strong>为了让本文足够通用，我们在uwsgi中使用了<code class="eh mn mo mp mq b">lazy-apps</code>模式，它为每个工作者加载一次应用程序。<a class="ae kz" href="http://uwsgi-docs.readthedocs.io/en/latest/articles/TheArtOfGracefulReloading.html" rel="noopener ugc nofollow" target="_blank">根据文档</a>，这将需要O(n)时间来加载(其中n是工人的数量)。这也可能需要更多的内存，但会给每个工作人员带来一个干净的环境。默认情况下，uWSGI以不同的方式加载整个应用程序。它始于一个过程；然后，它会多次分叉以获取额外的工作线程。这导致更多的内存节省。然而，这并不适用于所有的ML框架。例如，Keras中的TensorFlow后端在没有<code class="eh mn mo mp mq b">lazy-apps</code>模式的情况下会失败(例如，check <a class="ae kz" href="https://github.com/keras-team/keras/issues/5640" rel="noopener ugc nofollow" target="_blank"> this </a>、<a class="ae kz" href="https://github.com/keras-team/keras/issues/2397" rel="noopener ugc nofollow" target="_blank"> this </a>和<a class="ae kz" href="https://stackoverflow.com/questions/40154320/replicating-models-in-keras-and-tensorflow-for-a-multi-threaded-setting" rel="noopener ugc nofollow" target="_blank"> this </a>)。最好的办法是先不使用<code class="eh mn mo mp mq b">lazy-apps = true</code>进行尝试，如果遇到类似的问题，再使用它。</li><li id="75aa" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><strong class="je ig">Flask App的参数:</strong>因为uWSGI调用<code class="eh mn mo mp mq b">app</code>作为可调用的，所以App本身的参数不应该通过命令行传递。您最好使用类似于<a class="ae kz" href="https://docs.python.org/3/library/configparser.html" rel="noopener ugc nofollow" target="_blank"> configparser </a>的配置文件来读取这些参数。</li><li id="846b" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><strong class="je ig">跨多个服务器扩展:</strong>上面的指南没有讨论多个服务器的情况。幸运的是，这可以在不显著改变我们的设置的情况下实现。受益于nginx  中的<a class="ae kz" href="https://www.nginx.com/resources/admin-guide/load-balancer/" rel="noopener ugc nofollow" target="_blank"> <strong class="je ig">负载平衡特性，您可以设置多台机器，每台机器都有我们上面描述的uWSGI设置。然后，您可以配置nginx将请求路由到不同的服务器。nginx提供了多种方法来分配负载，从简单的循环到计算连接数量或平均延迟。</strong></a></li><li id="d5fc" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><strong class="je ig">端口选择:</strong>以上指南使用端口4444进行说明。请随意适应您自己的端口。并确保您在防火墙中打开这些端口，或者要求您所在机构的管理员这样做。</li><li id="b26f" class="kq kr if je b jf la jj lb jn lc jr ld jv le jz kv kw kx ky dt translated"><strong class="je ig">套接字权限:</strong>我们已经通过向所有用户授予写访问权限来许可套接字权限。您也可以根据自己的目的随意调整这些权限，并使用不同的用户/组运行服务。确保您的nginx和uWSGI在更改后仍然可以成功地相互通信。</li></ul></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><p id="7be8" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">原来如此！我希望这个指南和<a class="ae kz" href="https://github.com/harkous/production_ml" rel="noopener ugc nofollow" target="_blank">相关的资源库</a>能够对那些试图将他们的模型作为web应用程序的一部分或者API部署到产品中的人有所帮助。如果您有任何反馈，请在下面留言。</p><figure class="kb kc kd ke fq hw"><div class="bz el l di"><div class="nx kg l"/></div></figure></div><div class="ab cl kj kk hc kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hn ho hp hq hr"><p id="4d45" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">感谢您的阅读！您可能也有兴趣在我的媒体页面上查看我的其他文章:</p><div class="ht hu fm fo hv na"><a rel="noopener follow" target="_blank" href="/@hamzaharkous"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd ig fv z el nf eo ep ng er et ie dt translated">Hamza Harkous —中等</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">阅读哈姆扎·哈库斯在媒介上的作品。瑞士EPFL博士后；在隐私、NLP的交叉点上工作…</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">medium.com</p></div></div><div class="nj l"><div class="ny l nl nm nn nj no ib na"/></div></div></a></div><p id="0fb4" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">…或者看到我发的推文:</p><div class="ht hu fm fo hv na"><a href="https://twitter.com/hamzaharkous" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd ig fv z el nf eo ep ng er et ie dt translated">Hamza Harkous (@hamzaharkous) |推特</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">Hamza Harkous的最新推文(@hamzaharkous)。博士后@ EPFL；在隐私、自然语言处理和…</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">twitter.com</p></div></div><div class="nj l"><div class="nz l nl nm nn nj no ib na"/></div></div></a></div></div></div>    
</body>
</html>