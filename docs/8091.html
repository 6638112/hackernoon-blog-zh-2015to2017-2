<html>
<head>
<title>nSupervised Machine Learning — Linear Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督机器学习Python中的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/supervised-machine-learning-linear-regression-in-python-541a5d8141ce?source=collection_archive---------2-----------------------#2017-11-19">https://medium.com/hackernoon/supervised-machine-learning-linear-regression-in-python-541a5d8141ce?source=collection_archive---------2-----------------------#2017-11-19</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/138bc88c124e88a5ab301ad354e4e891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjEbLFjut-hjcBH_6lknsQ.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="https://deathtothestockphoto.com/" rel="noopener ugc nofollow" target="_blank">Source/CCo</a></figcaption></figure><pre class="jh ji jj jk fq jl jm jn jo aw jp dt"><span id="c699" class="jq jr hu jm b fv js jt l ju jv"><strong class="jm hv">Update [17/11/17]: </strong>The full implementation of Supervised Linear Regression can be <a class="ae jg" href="https://github.com/Chippasaur/Supervised-Learning-Linear-Regression" rel="noopener ugc nofollow" target="_blank">found here</a>.</span></pre><h1 id="b4ef" class="jw jr hu bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dt translated">介绍</h1><p id="36ec" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated">最近，<a class="ae jg" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的概念在某种程度上已经成为一种时尚，从小型初创企业到大型企业的公司都在大声疾呼，希望通过引用、集成复杂的自动化和预测分析来实现技术支持。</p><p id="4256" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">这导致了一种印象，即机器学习非常模糊，集成系统超出了普通公众的理解范围。这与事实相去甚远。相反，我认为机器学习更倾向于计算统计领域，而不是一些神秘的黑盒，在黑盒中，有人挥动魔杖，咒语，就能够变戏法般地做出一些神奇的预测。</p><h1 id="03be" class="jw jr hu bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dt translated">动机</h1><p id="7171" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated"><em class="lw">这个系列的动机是让任何对机器学习领域感兴趣的人都能够</em> <a class="ae jg" href="https://hackernoon.com/tagged/develop" rel="noopener ugc nofollow" target="_blank"> <em class="lw">开发</em> </a> <em class="lw">，理解并实现自己的机器学习算法。</em></p><p id="2c13" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">这将是我计划写的一系列文章的第一篇；我希望你喜欢。</p><p id="db66" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">在我们开始之前，有几个重要的术语:</p><ul class=""><li id="0056" class="lx ly hu kv b kw lr la ls le lz li ma lm mb lq mc md me mf dt translated"><strong class="kv hv">自变量(特征):</strong>自变量是被操纵来决定因变量的值的变量。简单地说，它们是我们想要用来预测y的某个给定值的特征。它也可以被称为解释变量</li><li id="0e7e" class="lx ly hu kv b kw mg la mh le mi li mj lm mk lq mc md me mf dt translated"><strong class="kv hv">因变量(目标):</strong>因变量依赖于自变量的值。简单地说，这就是我们试图预测的特征。这通常也被称为响应变量。</li></ul></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h1 id="4616" class="jw jr hu bd jx jy ms ka kb kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks dt translated">简单和多元线性回归</h1><p id="5235" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated">你受教育的预期收入是多少？根据你以前的分数，期末考试的预期成绩是多少？你赢得梦中女孩芳心的机会有多大(我开玩笑的)</p><p id="11ee" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">尽管如此，线性回归是统计学和机器学习中可用的最强大的工具之一，可用于预测给定一组<strong class="kv hv">特性或特征(X) </strong>的某个<strong class="kv hv">值(Y) </strong>。</p><p id="4798" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">鉴于它是如此强大的工具，对于对数据科学和机器学习领域感兴趣的个人来说，这是一个很好的起点来了解，<em class="lw">“机器如何学习进行预测”。</em></p><p id="4482" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">为了说明线性回归是如何工作的，我们可以研究一下学生在大学时面临的一个普遍问题。根据我以前的成绩，我期望的期末考试分数是多少？这个问题在数学上可以定义为我们的<strong class="kv hv">自变量(X) </strong>和对应的<strong class="kv hv">期末考试分数(Y) </strong>之间的某个函数。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff mx"><img src="../Images/d7ec52070d7af4653f073ee20ddeaee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*6T7Y8lpS5K-LSoSwAwNM9A.png"/></div></figure><pre class="jh ji jj jk fq jl jm jn jo aw jp dt"><span id="284c" class="jq jr hu jm b fv js jt l ju jv"><strong class="jm hv">X (input)</strong> = Assignment Results<br/><strong class="jm hv">Y (output)</strong> = Final Exam Mark<br/><strong class="jm hv">f =</strong> function which describes the relationship between X and Y<br/><strong class="jm hv">e (epsilon) </strong>= Random error term (positive or negative) with a mean zero (there are move assumptions for our residuals, however we won't be covering them)</span></pre><p id="d786" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">根据经验，你可能会得出这样的结论，“如果我的作业分数是73%，那么我的期末考试分数通常是1.1x，大约是80.3%”。虽然这可能是真的，但这种近似方法相当不正统，并且缺乏准确性，因为我们人类在我们的近似中隐含着偏见。此外，随着我们增加更多的独立变量，这变得越来越难以预测。</p><p id="fa8d" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">另一方面，当提供一组逻辑序列时，计算机被优化为表现非常好，因为与人类相比，它们不会遭受偏见。而且计算机在精度和计算速度上都更高效；我们可以利用计算机来预测我们想要了解的特征。</p><p id="b59a" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">对于我们的例子，我们将使用一个监督的“训练和测试”集来预测学生的预期期末考试成绩。我们将通过将数据集分成训练集和测试集来实现这一点。训练集的目的是使机器能够学习学生的作业结果和他们各自的期末考试分数之间的关系。这样，我们就可以使用学习过的函数来估计学生的期末考试成绩，并将其应用于我们的未标记测试集，以预测学生的预期期末考试分数。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff my"><img src="../Images/64e80c539b51edf28fc5d06c0ed91cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hW9B8_ggbo3GTFIuEn86Eg.png"/></div></div></figure><pre class="jh ji jj jk fq jl jm jn jo aw jp dt"><span id="c981" class="jq jr hu jm b fv js jt l ju jv"><strong class="jm hv">Regression</strong></span><span id="b795" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv">Y = f(X) + e, </strong>where X = (X1, x2...Xn)</span><span id="0d91" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv">Training: </strong>Machine learns (fits) f from labelled training set</span><span id="1001" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv">Test: </strong>Machine predicts Y from unlabeled test set</span><span id="3bfa" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv"><em class="lw">Note:</em> </strong>f(x) can be derived through matrices to perform least square linear regression. However this beyond the scope of this tutorial, if you'd like to learn how to derive regression lines <a class="ae jg" href="https://www.youtube.com/watch?v=Qa_FI92_qo8" rel="noopener ugc nofollow" target="_blank">here is a good link</a> . Also, X can be a tensor with any number of dimensions. A 1D tensor is a vector (1 row, many columns), 2D tensor is a matrix (many rows, many columns), and higher dimensional tensor.</span></pre><p id="2c17" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">为了简单起见，我们将只使用一个独立变量(赋值)来预测我们估计的期末考试分数，它是一个<strong class="kv hv"> 2D张量</strong>。</p></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h1 id="a87b" class="jw jr hu bd jx jy ms ka kb kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks dt translated">线性回归(普通最小二乘法)</h1><blockquote class="na nb nc"><p id="24ec" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">如何通过画直线来预测未来？是的，这算机器学习。</p></blockquote><p id="48f9" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">普通最小二乘回归(OLS)的目标是学习一个线性模型(线)，我们可以用它来预测<strong class="kv hv"> (Y) </strong>，同时试图减少误差(误差项)。通过减少误差项，我们反过来增加了预测的准确性。从而改善我们的学习功能。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ng"><img src="../Images/07eca3deb1e12b0ea3ba8c756a552cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZb0D9Unu22fvNJP1pjlSA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Source: <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Linear_regression.svg" rel="noopener ugc nofollow" target="_blank">Wikipedia ’Linear Regression’</a></figcaption></figure><p id="a2d8" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">因为线性回归是一种参数方法:样本数据来自一个总体，该总体遵循基于一组固定参数的概率分布。因此，关于与X和Y相关的函数的形式，必须满足各种假设——参见附加注释中的进一步阅读。我们的模型将是一个给定特定x预测y-hat的函数:</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff nh"><img src="../Images/b99cc617e8ad610a1dea812d13b59c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*aSvQLKZcCvYQKaI8otCRWQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">This can be interpreted as, for a one unit increase in X, holding all else constant, Y increases <em class="ni">β1</em></figcaption></figure></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h2 id="f17d" class="jq jr hu bd jx nj nk nl kb nm nn no kf le np nq kj li nr ns kn lm nt nu kr nv dt translated">解释</h2><p id="b076" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated"><strong class="kv hv"> <em class="lw"> β0 </em> : </strong>为x = 0时的y截距。即当你的作业成绩为0时，你预测的期末考试分数是Y截距(<em class="lw"> β0 ) </em></p><p id="720a" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated"><strong class="kv hv"> <em class="lw"> β1 </em> : </strong>是我们这条线的斜率。也就是说，你的作业分数每增加一个单位，我们的期末考试分数就会增加多少。</p><p id="12c7" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">提醒一下，我们的目标是学习使我们的误差项最小化的模型参数，从而增加我们的模型预测的准确性。</p><blockquote class="na nb nc"><p id="a112" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">要找到最佳模型参数:</p><p id="d02a" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">1.定义一个成本函数，或损失函数，衡量我们的模型预测有多不准确。</p><p id="fa91" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">2.找到使损失最小化的参数，即尽可能使我们的模型精确。</p></blockquote><p id="c60e" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">从图形上看，这可以用笛卡尔平面来表示，因为我们的模型是二维的。这会变成一个三维的平面，等等…</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nw"><img src="../Images/465d54cbf8edcb49582128cf785aeb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XasZ3p6hHsTbAsK1nSclA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Where Y is our Final Exam Mark, and X is our Assignment Mark</figcaption></figure><blockquote class="na nb nc"><p id="6304" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated"><strong class="kv hv">维度注意:</strong>为了简单起见，我们的示例是二维的，但是这是不现实的，通常您的模型中会有更多的特征(x)和系数，因为通常您会有不止一个特征对解释您的因变量有重要意义。此外，线性回归深受<a class="ae jg" href="https://stats.stackexchange.com/questions/169156/explain-curse-of-dimensionality-to-a-child" rel="noopener ugc nofollow" target="_blank"> <strong class="kv hv">维数灾难</strong> </a>之苦，因为一旦我们处理高维空间，每个数据点都会变成离群值。</p></blockquote></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h2 id="9b47" class="jq jr hu bd jx nj nk nl kb nm nn no kf le np nq kj li nr ns kn lm nt nu kr nv dt translated">价值函数</h2><p id="4fe6" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated">一开始，成本函数的公式可能会让您望而生畏。但是，理解起来极其简单直观。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/dc9b11606d7608265399bd28401b4afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*dyzQshni7wqRYFb01AoxKg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Cost Function (Error Term) of our linear model</figcaption></figure><p id="84ae" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">简单地说，成本函数表示取每个真实数据点<strong class="kv hv"> (y) </strong>和我们的模型预测<strong class="kv hv">(ŷ)</strong>之间的差，求差的平方以避免负数，并惩罚较大的差。最后相加，取平均值。除了不是除以n，而是除以2*n，这是因为数学家认为这样更容易推导。你可以把这个带到数学法庭去。然而，为了简单起见，请记住我们取2*n。</p><p id="549d" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">对于二维的问题，我们可以简单地推导出使损失函数最小化的最佳β参数。然而，随着模型变得越来越复杂，计算每个变量的beta参数变得不再可行。因此，我们需要一种叫做<strong class="kv hv">梯度下降</strong>的方法来最小化我们的损失函数。</p></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h1 id="9cd0" class="jw jr hu bd jx jy ms ka kb kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks dt translated">梯度下降:学习我们的参数</h1><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ny"><img src="../Images/7fbb999e1dac287d5ce07347d512e3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lbvLg7MSu05pZkmHUAiTsQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="https://www.youtube.com/watch?v=riplXsNf_zs" rel="noopener ugc nofollow" target="_blank">Source: Youtube</a> <a class="ae jg" href="https://www.youtube.com/channel/UClR00RTNrUcjr3l36kyTzgA" rel="noopener ugc nofollow" target="_blank">Mwamba Capital</a></figcaption></figure><blockquote class="na nb nc"><p id="8dd3" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">想象你站在一座山上的某个地方<strong class="kv hv">(A点)</strong>。你想尽可能快地降到最低，所以你决定采取以下步骤:</p><p id="b8c6" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">-你检查你现在的高度，你的高度向北一步，向南一步，向东一步，向西一步。利用这一点，你弄清楚你应该朝哪个方向走，以便在这一步中尽可能降低你的高度。<br/> <br/> —重复，直到踏向任何方向都会使你再次上升<strong class="kv hv">(B点)</strong>。</p><p id="e0ad" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">这是梯度下降</p></blockquote><p id="27d2" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">目前，梯度下降是用于参数优化的最流行的方法之一。它经常与神经网络一起使用，我们将在后面介绍。尽管如此，理解<em class="lw">做什么</em>，以及<em class="lw">如何工作</em>是很重要的。</p><p id="4a20" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">梯度下降的目标是通过迭代获得越来越好的近似来找到我们的模型的成本函数的最小点。这是通过迭代实现的，在地面最陡峭的方向上移动，直到向任何方向移动都会使你再次上升。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nz"><img src="../Images/a8062d54c9817858088738fe344ba4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UA1jE11ju6__Jl5bUgcyYw.gif"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="http://songhuiming.github.io/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/" rel="noopener ugc nofollow" target="_blank">Source: PyData</a></figcaption></figure><p id="7ca1" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">看看我们在回归中看到的损失函数:</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/dc9b11606d7608265399bd28401b4afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*dyzQshni7wqRYFb01AoxKg.png"/></div></figure><p id="736e" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">我们看到这其实是两个变量的函数:<em class="lw"> β0 </em> <strong class="kv hv"> <em class="lw"> </em> </strong>和<em class="lw"> β1 </em>。所有其余的变量都是确定的，因为X、Y和N是在训练期间给定的。因此，我们想尽量减少这种功能。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nw"><img src="../Images/f92aa8271411ada94af42b11b0725e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wsBakfF2Geh1zgY4HJbwFQ.gif"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Source: <a class="ae jg" href="https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/" rel="noopener ugc nofollow" target="_blank">Github Alykhan Tejani</a></figcaption></figure><p id="e393" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">函数为(<em class="lw"> β0 </em>，<em class="lw"> β1 </em> ) = <em class="lw"> z </em>。为了开始梯度下降，我们首先猜测使函数最小化的参数B0和B1。</p><p id="6dc4" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">接下来，我们求损失函数关于每个β参数的偏导数:[ <em class="lw"> dz </em> /d <em class="lw"> β0 </em>，<em class="lw"> dz </em> /d <em class="lw"> β1 </em>。偏导数表示如果稍微增加<em class="lw"> β0 </em>或<em class="lw"> β1 </em>，总损失增加或减少了多少。如果<em class="lw"> dz </em> /d <em class="lw"> β1 </em>的偏导数是负数，那么增加<em class="lw"> β1 </em>是好的，因为这将减少我们的总损失。如果是正数，你要减少<em class="lw"> β1 </em>。<em class="lw">如果它是零，我们不改变β1，因为这意味着我们已经达到最优。</em></p><p id="7c8c" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">我们这样做，直到我们到达底部，即算法收敛并且损失已经最小化。</p></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h1 id="9f86" class="jw jr hu bd jx jy ms ka kb kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks dt translated">过度拟合</h1><blockquote class="na nb nc"><p id="44ff" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated"><strong class="kv hv">过度拟合:</strong>“夏洛克，你对刚才发生的事情的解释太针对具体情况了”。“不要把事情弄得太复杂，夏洛克。”你多说一个字我就揍你一顿。<strong class="kv hv">超参数(<em class="hu"> λ </em> ) </strong>:“这是我每多一个字就揍你一拳的力量”<a class="ae jg" rel="noopener" href="/@v_maini">归功于Vishal Maini </a></p></blockquote><p id="4fe9" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">当经过训练的模型在模型学习的训练数据上表现得太好，但在测试数据上不能很好地概括时，就会发生过度拟合。过度适应的问题不仅限于计算机，人类通常也好不到哪里去。例如，假设你有一次与XYZ航空公司的糟糕经历，可能是服务不好，或者是航班经常延误。你可能会忍不住说XYZ航空公司的所有航班都很糟糕。这被称为<strong class="kv hv">过度适应</strong>,借此我们过度概括了一些事情，否则，可能只是我们过了糟糕的一天。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/b502697482299b3ca3a22401e0a541f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*3O3Ib-4DCvEDONHau9A5ZA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Source: <a class="ae jg" href="https://www.quora.com/What-is-overfitting" rel="noopener ugc nofollow" target="_blank">Quora: Luis Argerich</a></figcaption></figure><p id="ae8a" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">当一个模型<em class="lw">从训练数据中过度学习</em>到开始挑选不代表真实世界模式的特质时，过度拟合就发生了。随着模型变得越来越复杂，这就变得尤其成问题。拟合不足是指模型不够复杂，无法捕捉数据中的潜在趋势。</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div class="fe ff ob"><img src="../Images/04f4794421a48b7de4796214eb194933.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*zA8baiaM4HIhAa5Kpn94Hw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Source: <a class="ae jg" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">Scott Fortmann-Roe</a></figcaption></figure><pre class="jh ji jj jk fq jl jm jn jo aw jp dt"><span id="a99d" class="jq jr hu jm b fv js jt l ju jv"><strong class="jm hv">Bias-Variance Tradeoff</strong></span><span id="595f" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv">Bias:</strong> is the amount of error introduced by approximating real-world phenomena with a simplified model.</span><span id="212f" class="jq jr hu jm b fv mz jt l ju jv"><strong class="jm hv">Variance: </strong>is how much your model’s test error changes based on variation in the training data. It reflects the model’s sensitivity to the idiosyncrasies of the data set it was trained on</span><span id="95e1" class="jq jr hu jm b fv mz jt l ju jv">As a model increases in complexity and it becomes flexible, its bias decreases (it does a good job of explaining the training data), but variance increases (it doesn’t generalise as well).</span></pre><blockquote class="oc"><p id="83a5" class="od oe hu bd of og oh oi oj ok ol lq ek translated">最终，为了有一个好的模型，你需要一个低偏差和低方差的模型</p></blockquote><p id="69ed" class="pw-post-body-paragraph kt ku hu kv b kw om ky kz la on lc ld le oo lg lh li op lk ll lm oq lo lp lq hn dt translated">记住，我们唯一关心的是<em class="lw">模型在测试数据上表现如何。您希望在学生的期末考试成绩被打分之前预测他们的分数，而不仅仅是建立一个基于训练集对学生分数进行100%准确分类的模型。</em></p><p id="68d2" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated"><strong class="kv hv">对抗过度拟合的两种方法:</strong></p><p id="c6c4" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">1.<strong class="kv hv">使用更多的训练数据:</strong>你拥有的数据越多，就越难通过从任何单个训练示例中学习太多来对数据进行过度拟合。</p><p id="866c" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">2.<strong class="kv hv">用户规则化:</strong>在损失函数中添加一个惩罚，用于构建一个对任何一个特征赋予过多解释能力或允许考虑许多特征的模型</p><figure class="jh ji jj jk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff or"><img src="../Images/c740a2635fac7b0df2dc787c4035cfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rFT6mtU45diT0OJhlgDcBg.png"/></div></div></figure><p id="e0be" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">上面总和的第一部分是我们的正常成本函数。第二部分是一个<strong class="kv hv">正则化术语</strong>，它增加了对大beta系数的惩罚，这些系数对任何特定特征给出了过多的解释力。</p><blockquote class="na nb nc"><p id="d522" class="kt ku lw kv b kw lr ky kz la ls lc ld nd lt lg lh ne lu lk ll nf lv lo lp lq hn dt translated">有了这两个元素，成本函数现在在两个优先级之间进行平衡:解释训练数据和防止解释变得过于具体。</p></blockquote><p id="f7e3" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">成本函数中正则化项的λ系数是一个<strong class="kv hv">超参数:</strong>您的模型的一般设置，可以增加或减少(即<strong class="kv hv">调整</strong>)以提高性能。较高的lambda值对可能导致潜在过拟合的较大beta系数不利。为了决定lambda ( <em class="lw"> λ </em>)的最佳值，您将使用一种被称为<strong class="kv hv">交叉验证</strong>的方法，该方法涉及在训练期间保留我们的一部分训练数据，然后查看您的模型如何解释保留的部分。我们将在以后的系列文章中更深入地讨论这个问题。</p><pre class="jh ji jj jk fq jl jm jn jo aw jp dt"><span id="4df1" class="jq jr hu jm b fv js jt l ju jv">For those whom are interested in the full implementation of Supervised Learning: Linear Regression. It can be <a class="ae jg" href="https://github.com/Chippasaur/Supervised-Learning-Linear-Regression" rel="noopener ugc nofollow" target="_blank">found here </a></span></pre></div><div class="ab cl ml mm hc mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hn ho hp hq hr"><h1 id="a567" class="jw jr hu bd jx jy ms ka kb kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks dt translated">哇哦。！您已经涵盖了监督学习的要点:线性回归！</h1><p id="8b98" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated">以下是我们在本节中介绍的内容:</p><ul class=""><li id="4ef5" class="lx ly hu kv b kw lr la ls le lz li ma lm mb lq mc md me mf dt translated">如何使用<strong class="kv hv">监督数据</strong>使计算机能够<strong class="kv hv">学习一个功能</strong>，而无需明确编程。</li><li id="b333" class="lx ly hu kv b kw mg la mh le mi li mj lm mk lq mc md me mf dt translated"><strong class="kv hv">线性回归</strong>，参数化算法<strong class="kv hv">的基本原理</strong></li><li id="c005" class="lx ly hu kv b kw mg la mh le mi li mj lm mk lq mc md me mf dt translated">使用<strong class="kv hv">梯度下降</strong>学习<strong class="kv hv">参数</strong></li><li id="d672" class="lx ly hu kv b kw mg la mh le mi li mj lm mk lq mc md me mf dt translated"><strong class="kv hv">过拟合</strong>和<strong class="kv hv">调整</strong></li></ul><h1 id="f9ee" class="jw jr hu bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dt translated">进一步阅读和练习</h1><p id="13a4" class="pw-post-body-paragraph kt ku hu kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq hn dt translated">为了更全面地理解线性回归，我推荐Trevor Hastie的'<strong class="kv hv">统计学习的元素'</strong>。这是一本很棒的书，涵盖了使用线性代数进行统计学习的要点。</p><p id="0b72" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">对于逻辑回归，我推荐David W. Hosmer的'<strong class="kv hv">Applied logistic regression</strong>n '。它在逻辑回归可用的不同方法和途径中进行了更详细的描述。</p><p id="1afe" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated"><strong class="kv hv"> <em class="lw">为修行:</em> </strong></p><p id="bb0b" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">对于实践，我建议使用用于预测房价的数据集，<a class="ae jg" href="http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kv hv">波士顿住房数据</strong> </a>最受欢迎。否则考虑<a class="ae jg" href="https://www.kaggle.com/c/job-salary-prediction/data" rel="noopener ugc nofollow" target="_blank"> <strong class="kv hv">薪资预测</strong> </a> <strong class="kv hv">。</strong></p><p id="a77c" class="pw-post-body-paragraph kt ku hu kv b kw lr ky kz la ls lc ld le lt lg lh li lu lk ll lm lv lo lp lq hn dt translated">对于数据集，考虑实现监督学习:线性回归。我建议继续使用<a class="ae jg" href="https://www.reddit.com/r/datasets/" rel="noopener ugc nofollow" target="_blank"><strong class="kv hv">Reddit/datasets</strong></a>或<a class="ae jg" href="https://www.kaggle.com/kernels" rel="noopener ugc nofollow" target="_blank"> <strong class="kv hv"> Kaggle </strong> </a>来练习，看看你能做出多准确的预测。</p><div class="os ot fm fo ou ov"><a href="https://hackernoon.com/supervised-machine-learning-dimensional-reduction-and-principal-component-analysis-614dec1f6b4c" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab ej"><div class="ox ab oy cl cj oz"><h2 class="bd hv fv z el pa eo ep pb er et ht dt translated">监督机器学习——降维与主成分分析</h2><div class="pc l"><h3 class="bd b fv z el pa eo ep pb er et ek translated">“使用Python进行维度缩减和主成分分析的初级课程”</h3></div><div class="pd l"><p class="bd b gc z el pa eo ep pb er et ek translated">hackernoon.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ja ov"/></div></div></a></div><figure class="jh ji jj jk fq iv"><div class="bz el l di"><div class="pk pl l"/></div></figure></div></div>    
</body>
</html>