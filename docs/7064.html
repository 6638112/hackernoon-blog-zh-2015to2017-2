<html>
<head>
<title>Attention Mechanism in Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的注意机制</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/attention-mechanism-in-neural-network-30aaf5e39512?source=collection_archive---------4-----------------------#2017-10-16">https://medium.com/hackernoon/attention-mechanism-in-neural-network-30aaf5e39512?source=collection_archive---------4-----------------------#2017-10-16</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="a72b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">一个<strong class="it hv">编码器</strong>读取源语句并编码成一个<strong class="it hv">定长向量</strong>。</p><p id="5381" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">一个<strong class="it hv">解码器</strong>然后输出来自编码矢量的翻译。</p><h2 id="c238" class="jp jq hu bd jr js jt ju jv jw jx jy jz jc ka kb kc jg kd ke kf jk kg kh ki kj dt translated"><strong class="ak">限制</strong></h2><p id="1bec" class="pw-post-body-paragraph ir is hu it b iu kk iw ix iy kl ja jb jc km je jf jg kn ji jj jk ko jm jn jo hn dt translated">这种编码器-解码器方法的一个潜在问题是，神经网络需要能够将源句子的所有必要信息压缩到一个<strong class="it hv">固定长度向量</strong>中。</p><h2 id="e00d" class="jp jq hu bd jr js jt ju jv jw jx jy jz jc ka kb kc jg kd ke kf jk kg kh ki kj dt translated"><strong class="ak">注意力如何解决问题？</strong></h2><p id="d621" class="pw-post-body-paragraph ir is hu it b iu kk iw ix iy kl ja jb jc km je jf jg kn ji jj jk ko jm jn jo hn dt translated">注意机制允许解码器在输出生成的每一步都注意源句子的不同部分。</p><p id="7a13" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们让模型<strong class="it hv"> </strong>学习<strong class="it hv">如何为每个输出时间步长生成上下文向量</strong>，而不是将输入序列编码到<strong class="it hv">单个固定上下文向量</strong>中。也就是说，我们让模型<strong class="it hv">学习</strong>根据输入的句子和它到目前为止产生的结果来处理什么。</p><h1 id="5036" class="kp jq hu bd jr kq kr ks jv kt ku kv jz kw kx ky kc kz la lb kf lc ld le ki lf dt translated">注意机制</h1><figure class="lh li lj lk fq ll fe ff paragraph-image"><div class="fe ff lg"><img src="../Images/a2868d1737a21594461a8b25aab2a4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/0*Jpp6WALMjZbjUFjP.png"/></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Attention Mechanism</figcaption></figure><p id="6cf5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里，<strong class="it hv">编码器</strong>产生<strong class="it hv"> h1、h2、h…hT </strong>来自输入<strong class="it hv"> X1，X2，X3…XT </strong></p><p id="45b0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后，我们必须为每个输出时间步长找出<strong class="it hv">上下文向量ci </strong>。</p><h2 id="4ab1" class="jp jq hu bd jr js jt ju jv jw jx jy jz jc ka kb kc jg kd ke kf jk kg kh ki kj dt translated"><strong class="ak">如何计算每个输出时间步长的上下文向量？</strong></h2><figure class="lh li lj lk fq ll fe ff paragraph-image"><div class="fe ff ls"><img src="../Images/6b7355d0b20dfe5560a7cc20b939efd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Agii69DmmkTAGBLNUCeN0g.png"/></div></figure><p id="0223" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv"> a </strong>是<strong class="it hv">校准模型</strong>，它是一个<strong class="it hv">前馈神经网络</strong>，用所提出的系统的所有其他组件进行训练</p><p id="6594" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">对准模型</strong>对每个编码输入(h)与解码器(s)的当前输出的匹配程度进行评分(e)。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div class="fe ff lt"><img src="../Images/8fb23d0d83280b13ba8352cd789dc767.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*VBalT1KkZ16WGvjWpjYo4g.png"/></div></figure><p id="7e4d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">使用<strong class="it hv"> softmax函数对比对分数进行标准化。</strong></p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/1a6b182cfc7de0e0e68a9d821d8d457f.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*uXp_uFfXbAqqrJx5g-xYzQ.png"/></div></figure><p id="0cc2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">上下文向量是<strong class="it hv">注释</strong> (hj)和<strong class="it hv">归一化比对分数的加权和。</strong></p><h1 id="e46f" class="kp jq hu bd jr kq kr ks jv kt ku kv jz kw kx ky kc kz la lb kf lc ld le ki lf dt translated">解码</h1><p id="c505" class="pw-post-body-paragraph ir is hu it b iu kk iw ix iy kl ja jb jc km je jf jg kn ji jj jk ko jm jn jo hn dt translated">解码器通过查看第I个上下文向量和先前的隐藏输出s(t-1)来生成第I个时间步长的输出。</p><h2 id="7f64" class="jp jq hu bd jr js jt ju jv jw jx jy jz jc ka kb kc jg kd ke kf jk kg kh ki kj dt translated">参考</h2><p id="3cee" class="pw-post-body-paragraph ir is hu it b iu kk iw ix iy kl ja jb jc km je jf jg kn ji jj jk ko jm jn jo hn dt translated">— <a class="ae lv" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a>，2015。</p></div></div>    
</body>
</html>